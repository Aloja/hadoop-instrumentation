diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/core/org/apache/hadoop/io/ObjectWritable.java hadoop-dist-dcarrera/src/core/org/apache/hadoop/io/ObjectWritable.java
--- hadoop-1.0.3/src/core/org/apache/hadoop/io/ObjectWritable.java	2012-05-08 22:34:52.000000000 +0200
+++ hadoop-dist-dcarrera/src/core/org/apache/hadoop/io/ObjectWritable.java	2014-08-20 11:06:33.000000000 +0200
@@ -180,6 +180,9 @@
     Class<?> declaredClass = PRIMITIVE_NAMES.get(className);
     if (declaredClass == null) {
       try {
+	//<smendoza>
+	//System.out.println("@ObjectWritable.className="+className);
+	//</smendoza>
         declaredClass = conf.getClassByName(className);
       } catch (ClassNotFoundException e) {
         throw new RuntimeException("readObject can't find class " + className, e);
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java hadoop-dist-dcarrera/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java
--- hadoop-1.0.3/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java	2012-05-08 22:34:53.000000000 +0200
+++ hadoop-dist-dcarrera/src/hdfs/org/apache/hadoop/hdfs/server/datanode/DataNode.java	2014-06-11 15:38:40.000000000 +0200
@@ -503,6 +503,9 @@
     dnRegistration.setIpcPort(ipcServer.getListenerAddress().getPort());
 
     LOG.info("dnRegistration = " + dnRegistration);
+
+    es.bsc.tools.extrae.IDManager.registerDatanode(conf);
+
   }
   
   private ObjectName mxBean = null;
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java hadoop-dist-dcarrera/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java
--- hadoop-1.0.3/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java	2012-05-08 22:34:53.000000000 +0200
+++ hadoop-dist-dcarrera/src/hdfs/org/apache/hadoop/hdfs/server/namenode/NameNode.java	2014-06-11 15:38:41.000000000 +0200
@@ -308,6 +308,8 @@
       serviceRpcServer.start();      
     }
     startTrashEmptier(conf);
+
+    es.bsc.tools.extrae.IDManager.registerNamenode(conf);
   }
 
   private void startTrashEmptier(Configuration conf) throws IOException {
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java hadoop-dist-dcarrera/src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java
--- hadoop-1.0.3/src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	2012-05-08 22:34:52.000000000 +0200
+++ hadoop-dist-dcarrera/src/hdfs/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	2014-06-11 15:38:41.000000000 +0200
@@ -234,6 +234,9 @@
              "(" + checkpointPeriod/60 + " min)");
     LOG.warn("Log Size Trigger    :" + checkpointSize + " bytes " +
              "(" + checkpointSize/1024 + " KB)");
+
+    es.bsc.tools.extrae.IDManager.registerSecondaryNamenode(conf);
+
   }
 
   /**
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/Child.java hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/Child.java
--- hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/Child.java	2012-05-08 22:34:53.000000000 +0200
+++ hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/Child.java	2014-06-11 15:38:46.000000000 +0200
@@ -243,6 +243,12 @@
         for(Token<?> token : UserGroupInformation.getCurrentUser().getTokens()) {
           childUGI.addToken(token);
         }
+	//<smendoza>
+	int myp = es.bsc.tools.extrae.Wrapper.GetPID();
+	System.out.println("Child.java > task-pid="+myp);
+	//</smendoza>
+	
+	es.bsc.tools.extrae.IDManager.registerTask(job, host+":"+port);
         
         // Create a final reference to the task for the doAs block
         final Task taskFinal = task;
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/JobTracker.java hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/JobTracker.java
--- hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/JobTracker.java	2012-05-08 22:34:52.000000000 +0200
+++ hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/JobTracker.java	2014-06-11 15:38:47.000000000 +0200
@@ -288,6 +288,7 @@
   public static JobTracker startTracker(JobConf conf
                                         ) throws IOException,
                                                  InterruptedException {
+    es.bsc.tools.extrae.IDManager.registerJobTracker(conf);
     return startTracker(conf, generateNewIdentifier());
   }
   
@@ -3325,6 +3326,10 @@
     // First check if the last heartbeat response got through
     String trackerName = status.getTrackerName();
     long now = clock.getTime();
+
+    es.bsc.tools.extrae.Events.GenerateReceiveEvent(es.bsc.tools.extrae.Events.Tags.HeartBeat, 0, es.bsc.tools.extrae.IDManager.getTaskTrackerID(status.getHost()), responseId);
+
+
     if (restarted) {
       faultyTrackers.markTrackerHealthy(status.getHost());
     } else {
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/MapTask.java hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/MapTask.java
--- hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/MapTask.java	2012-05-08 22:34:52.000000000 +0200
+++ hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/MapTask.java	2014-07-07 13:10:44.000000000 +0200
@@ -345,6 +345,7 @@
     throws IOException, ClassNotFoundException, InterruptedException {
     this.umbilical = umbilical;
 
+    //es.bsc.tools.extrae.IDManager.registerTask(job);
     // start thread that will handle communication with parent
     TaskReporter reporter = new TaskReporter(getProgress(), umbilical,
         jvmContext);
@@ -366,11 +367,13 @@
       return;
     }
 
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.RunMapper);
     if (useNewApi) {
       runNewMapper(job, splitMetaInfo, umbilical, reporter);
     } else {
       runOldMapper(job, splitMetaInfo, umbilical, reporter);
     }
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.End);
     done(umbilical, reporter);
   }
   @SuppressWarnings("unchecked")
@@ -1281,6 +1284,9 @@
 
     public synchronized void flush() throws IOException, ClassNotFoundException,
                                             InterruptedException {
+
+      es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.Flush);
+
       LOG.info("Starting flush of map output");
       spillLock.lock();
       try {
@@ -1323,6 +1329,9 @@
       mergeParts();
       Path outputPath = mapOutputFile.getOutputFile();
       fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());
+      
+      es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.End);
+
     }
 
     public void close() { }
@@ -1379,6 +1388,7 @@
 
     private void sortAndSpill() throws IOException, ClassNotFoundException,
                                        InterruptedException {
+      es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.SortAndSpill);
       //approximate the length of the output file to be the length of the
       //buffer + header lengths for the partitions
       long size = (bufend >= bufstart
@@ -1396,7 +1406,9 @@
         final int endPosition = (kvend > kvstart)
           ? kvend
           : kvoffsets.length + kvend;
+	es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.Sort);
         sorter.sort(MapOutputBuffer.this, kvstart, endPosition, reporter);
+	es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.End);
         int spindex = kvstart;
         IndexRecord rec = new IndexRecord();
         InMemValBytes value = new InMemValBytes();
@@ -1418,6 +1430,7 @@
                           (kvindices[kvoff + VALSTART] - 
                            kvindices[kvoff + KEYSTART]));
                 writer.append(key, value);
+		es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.SpillRecordDumped);
                 ++spindex;
               }
             } else {
@@ -1433,7 +1446,9 @@
                 combineCollector.setWriter(writer);
                 RawKeyValueIterator kvIter =
                   new MRResultIterator(spstart, spindex);
+		es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.Combine);
                 combinerRunner.combine(kvIter, combineCollector);
+		es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.End);
               }
             }
 
@@ -1452,12 +1467,15 @@
           }
         }
 
+	es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.TotalIndexCacheMemory);
         if (totalIndexCacheMemory >= INDEX_CACHE_MEMORY_LIMIT) {
+	  es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.CreateSpillIndexFile);
           // create spill index file
           Path indexFilename =
               mapOutputFile.getSpillIndexFileForWrite(numSpills, partitions
                   * MAP_OUTPUT_INDEX_RECORD_LENGTH);
           spillRec.writeToFile(indexFilename, job);
+	  es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.End);
         } else {
           indexCacheList.add(spillRec);
           totalIndexCacheMemory +=
@@ -1468,6 +1486,7 @@
       } finally {
         if (out != null) out.close();
       }
+      es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.MapOutputBuffer, es.bsc.tools.extrae.Events.Values.End);
     }
 
     /**
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/ReduceTask.java hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
--- hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/ReduceTask.java	2012-05-08 22:34:52.000000000 +0200
+++ hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/ReduceTask.java	2014-06-11 15:38:46.000000000 +0200
@@ -1,4 +1,4 @@
-/**
+ /**
  * Licensed to the Apache Software Foundation (ASF) under one
  * or more contributor license agreements.  See the NOTICE file
  * distributed with this work for additional information
@@ -18,16 +18,24 @@
 
 package org.apache.hadoop.mapred;
 
+//smendoza
+import java.lang.management.ManagementFactory;
+//end_smendoza
+
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.File;
 import java.io.IOException;
 import java.io.InputStream;
 import java.io.OutputStream;
+import java.io.BufferedReader;
+import java.io.FileReader;
+import java.io.IOException;
 import java.net.URI;
 import java.net.URL;
 import java.net.URLClassLoader;
 import java.net.URLConnection;
+import java.net.*;
 import java.text.DecimalFormat;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -347,6 +355,7 @@
   @SuppressWarnings("unchecked")
   public void run(JobConf job, final TaskUmbilicalProtocol umbilical)
     throws IOException, InterruptedException, ClassNotFoundException {
+    //es.bsc.tools.extrae.IDManager.registerTask(job);
     this.umbilical = umbilical;
     job.setBoolean("mapred.skip.on", isSkipping());
 
@@ -376,9 +385,11 @@
       return;
     }
     
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.RunReducer);
     // Initialize the codec
     codec = initCodec();
 
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.ReducerCopyPhase);
     boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
     if (!isLocal) {
       reduceCopier = new ReduceCopier(umbilical, job, reporter);
@@ -391,7 +402,9 @@
       }
     }
     copyPhase.complete();                         // copy is already complete
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.End);
     setPhase(TaskStatus.Phase.SORT);
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.ReducerSortPhase);
     statusUpdate(umbilical);
 
     final FileSystem rfs = FileSystem.getLocal(job).getRaw();
@@ -407,7 +420,9 @@
     mapOutputFilesOnDisk.clear();
     
     sortPhase.complete();                         // sort is complete
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.End);
     setPhase(TaskStatus.Phase.REDUCE); 
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.ReducerReducePhase);
     statusUpdate(umbilical);
     Class keyClass = job.getMapOutputKeyClass();
     Class valueClass = job.getMapOutputValueClass();
@@ -421,6 +436,8 @@
                     keyClass, valueClass);
     }
     done(umbilical, reporter);
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.End);
+    es.bsc.tools.extrae.Events.GenerateEvent(es.bsc.tools.extrae.Events.Types.TaskTracker, es.bsc.tools.extrae.Events.Values.End);
   }
 
   private class OldTrackingRecordWriter<K, V> implements RecordWriter<K, V> {
@@ -1478,8 +1495,57 @@
       throws IOException, InterruptedException {
         // Connect
         URL url = mapOutputLoc.getOutputLocation();
-        URLConnection connection = url.openConnection();
-        
+//<smendoza>
+	URLConnection connection = url.openConnection();
+	String pid = new File("/proc/self").getCanonicalFile().getName();
+	System.out.println("le_pid="+pid);
+	String[] net_fnames = {"/proc/"+pid+"/net/tcp","/proc/"+pid+"/net/udp","/proc/"+pid+"/net/raw","/proc/"+pid+"/net/unix"};
+	BufferedReader br = null;
+	//File file = new File(tcp_fname);
+	//FileInputStream fis = null;
+	System.out.println("elpid="+pid);
+
+/*
+	try {
+		String sCurrentLine;
+		for(String net_fn: net_fnames){
+			br = new BufferedReader(new FileReader(net_fn));
+			while ((sCurrentLine = br.readLine()) != null) {
+				System.out.println("4dapid["+net_fn+"]="+sCurrentLine);
+			}
+		}
+
+		fis = new FileInputStream(file);
+		System.out.println("Total file size to read (in bytes) : "+ fis.available());
+		int content;
+		while ((content = fis.read()) != -1) {
+			// convert to char and display it
+			System.out.print((char) content);
+		}
+	} catch (IOException e) {
+		e.printStackTrace();
+	} finally {
+		try {
+				if (br != null)br.close();
+			} catch (IOException ex) {
+				ex.printStackTrace();
+		}
+		try {
+			if (fis != null)
+				fis.close();
+		} catch (IOException ex) {
+			ex.printStackTrace();
+		}
+	}
+*/
+	//Proxy proxy = new Proxy(Proxy.Type.DIRECT, new InetSocketAddress(InetAddress.getByName("172.20.0.16"), 59111));
+	//SocketAddress addr = new InetSocketAddress("172.20.0.16", 48443);
+	//Proxy proxy = new Proxy(Proxy.Type.HTTP, addr);
+	//URLConnection connection = url.openConnection(proxy);
+	//System.out.println("mypid="+ManagementFactory.getRuntimeMXBean().getName());
+	//System.out.println("59111 -  url="+url+",conn.toString()="+connection.getContentType());
+//</smendoza>
+
         InputStream input = setupSecureConnection(mapOutputLoc, connection);
  
         // Validate header from map output
@@ -1635,6 +1701,52 @@
             }
           }
         }
+
+//<smendoza>
+   String pid = new File("/proc/self").getCanonicalFile().getName();
+        System.out.println("le_pid="+pid);
+        String[] net_fnames = {"/proc/"+pid+"/net/tcp"};//,"/proc/"+pid+"/net/udp","/proc/"+pid+"/net/raw","/proc/"+pid+"/net/unix"};
+        BufferedReader br = null;
+        //File file = new File(tcp_fname);
+        //FileInputStream fis = null;
+        System.out.println("elpid="+pid);
+        try {
+                String sCurrentLine;
+                for(String net_fn: net_fnames){
+                        br = new BufferedReader(new FileReader(net_fn));
+                        while ((sCurrentLine = br.readLine()) != null) {
+                                System.out.println("4dapid2["+net_fn+"]="+sCurrentLine);
+                        }
+                }
+/*
+                fis = new FileInputStream(file);
+                System.out.println("Total file size to read (in bytes) : "+ fis.available());
+                int content;
+                while ((content = fis.read()) != -1) {
+                        // convert to char and display it
+                        System.out.print((char) content);
+                }
+*/
+        } catch (Exception e) {
+                e.printStackTrace();
+        } finally {
+                try {
+                                if (br != null)br.close();
+                        } catch (IOException ex) {
+                                ex.printStackTrace();
+                }
+/*
+                try {
+                        if (fis != null)
+                                fis.close();
+                } catch (IOException ex) {
+                        ex.printStackTrace();
+                }
+*/
+        }
+	
+//</smendoza>
+
         try {
           return connection.getInputStream();
         } catch (IOException ioe) {
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/Task.java hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/Task.java
--- hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/Task.java	2012-05-08 22:34:52.000000000 +0200
+++ hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/Task.java	2014-07-23 10:35:36.000000000 +0200
@@ -58,6 +58,14 @@
 import org.apache.hadoop.util.StringUtils;
 import org.apache.hadoop.fs.FSDataInputStream;
 
+//smendoza
+/*
+import java.lang.management.ManagementFactory;
+import java.util.Arrays;
+import es.bsc.tools.extrae.*;
+*/
+//endsmendoza
+
 /** 
  * Base class for tasks.
  * 
@@ -192,6 +200,7 @@
                                                     TaskStatus.Phase.SHUFFLE, 
                                                   counters);
     spilledRecordsCounter = counters.findCounter(Counter.SPILLED_RECORDS);
+    //es.bsc.tools.extrae.Wrapper.Init();
   }
 
   ////////////////////////////////////////////
@@ -943,7 +952,16 @@
       try {
         Path mapOutput =  mapOutputFile.getOutputFile();
         FileSystem localFS = FileSystem.getLocal(conf);
-        return localFS.getFileStatus(mapOutput).getLen();
+
+        //<smendoza>
+        int myp = es.bsc.tools.extrae.Wrapper.GetPID();
+        System.out.println("Task.java > task-pid="+myp);
+        //</smendoza>
+
+        long s = localFS.getFileStatus(mapOutput).getLen();
+	es.bsc.tools.extrae.Wrapper.Event(es.bsc.tools.extrae.Events.Types.MapTaskOutputSize, s);
+	return s;
+        //return localFS.getFileStatus(mapOutput).getLen();
       } catch (IOException e) {
         LOG.warn ("Could not find output size " , e);
       }
@@ -1250,6 +1268,20 @@
      * read the next key 
      */
     private void readNextKey() throws IOException {
+
+//<smendoza>
+/*
+        int t[] = new int[1];
+        long v[] = new long[1];
+        String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0];
+        t[0]=1039911;
+        v[0]=Long.parseLong(pid);
+        
+	es.bsc.tools.extrae.Wrapper.nEvent(t,v);
+        System.out.println("Task.ValuesIterator.readNextKey() t="+Arrays.toString(t)+", v="+Arrays.toString(v)+", pid="+pid);
+*/
+//</smendoza>
+
       more = in.next();
       if (more) {
         DataInputBuffer nextKeyBytes = in.getKey();
@@ -1266,6 +1298,20 @@
      * @throws IOException
      */
     private void readNextValue() throws IOException {
+
+//<smendoza>
+/*
+        int t[] = new int[1];
+        long v[] = new long[1];
+        String pid = ManagementFactory.getRuntimeMXBean().getName().split("@")[0];
+        t[0]=1039911;
+        v[0]=Long.parseLong(pid);
+
+        es.bsc.tools.extrae.Wrapper.nEvent(t,v);
+        System.out.println("Task.ValuesIterator.readNextValue() t="+Arrays.toString(t)+", v="+Arrays.toString(v)+", pid="+pid);
+*/
+//</smendoza>
+
       DataInputBuffer nextValueBytes = in.getValue();
       valueIn.reset(nextValueBytes.getData(), nextValueBytes.getPosition(), nextValueBytes.getLength());
       value = valDeserializer.deserialize(value);
diff -uNr -x c++ -x docs -x native hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/TaskTracker.java hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
--- hadoop-1.0.3/src/mapred/org/apache/hadoop/mapred/TaskTracker.java	2012-05-08 22:34:53.000000000 +0200
+++ hadoop-dist-dcarrera/src/mapred/org/apache/hadoop/mapred/TaskTracker.java	2014-06-11 15:38:47.000000000 +0200
@@ -853,6 +853,8 @@
     oobHeartbeatDamper = 
       fConf.getInt(TT_OUTOFBAND_HEARTBEAT_DAMPER, 
           DEFAULT_OOB_HEARTBEAT_DAMPER);
+
+    es.bsc.tools.extrae.IDManager.registerTaskTracker(fConf);
   }
 
   private void startJettyBugMonitor() {
@@ -1507,6 +1509,7 @@
         TaskLogsTruncater.DEFAULT_RETAIN_SIZE);
     reduceRetainSize = conf.getLong(TaskLogsTruncater.REDUCE_USERLOG_RETAIN_SIZE,
         TaskLogsTruncater.DEFAULT_RETAIN_SIZE);
+
   }
 
   private void checkJettyPort(int port) throws IOException { 
@@ -1859,6 +1862,8 @@
                                                               justInited,
                                                               askForNewTask, 
                                                               heartbeatResponseId);
+    
+    es.bsc.tools.extrae.Events.GenerateSendEvent(es.bsc.tools.extrae.Events.Tags.HeartBeat, 0, es.bsc.tools.extrae.IDManager.getJobTrackerID(), heartbeatResponseId);
       
     //
     // The heartbeat got through successfully!
@@ -3951,6 +3956,10 @@
         }
         final long endTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;
         shuffleMetrics.serverHandlerFree();
+
+
+	// extrae here
+
         if (ClientTraceLog.isInfoEnabled()) {
           ClientTraceLog.info(String.format(MR_CLIENTTRACE_FORMAT,
                 request.getLocalAddr() + ":" + request.getLocalPort(),
