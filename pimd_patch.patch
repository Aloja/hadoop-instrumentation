diff --git a/bin/hadoop b/bin/hadoop
index c7cdd2e..023cfbb 100755
--- a/bin/hadoop
+++ b/bin/hadoop
@@ -124,19 +124,20 @@ fi
 CLASSPATH="${HADOOP_CONF_DIR}"
 CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
 
+## JLUPOX
 # for developers, add Hadoop classes to CLASSPATH
-if [ -d "$HADOOP_HOME/build/classes" ]; then
-  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/classes
-fi
-if [ -d "$HADOOP_HOME/build/webapps" ]; then
-  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build
-fi
-if [ -d "$HADOOP_HOME/build/test/classes" ]; then
-  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/test/classes
-fi
-if [ -d "$HADOOP_HOME/build/tools" ]; then
-  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/tools
-fi
+#if [ -d "$HADOOP_HOME/build/classes" ]; then
+#  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/classes
+#fi
+#if [ -d "$HADOOP_HOME/build/webapps" ]; then
+#  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build
+#fi
+#if [ -d "$HADOOP_HOME/build/test/classes" ]; then
+#  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/test/classes
+#fi
+#if [ -d "$HADOOP_HOME/build/tools" ]; then
+#  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/tools
+#fi
 
 # so that filenames w/ spaces are handled correctly in loops below
 IFS=
@@ -302,6 +303,7 @@ HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"
 # run it
 #echo "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
 exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
+#exec "$JAVA" "-javaagent:profile.jar" "-Dprofile.properties=profile.properties" $JAVA_HEAP_MAX $HADOOP_OPTS $CLASS "$@"
 
 echo
 echo "******************************************************************"
diff --git a/bin/hadoop-daemon-debug.sh b/bin/hadoop-daemon-debug.sh
old mode 100644
new mode 100755
index cf7373e..24d1c5e
--- a/bin/hadoop-daemon-debug.sh
+++ b/bin/hadoop-daemon-debug.sh
@@ -114,7 +114,23 @@ case $startStop in
     hadoop_rotate_log $log
     echo starting $command, logging to $log
     cd "$HADOOP_HOME"
-    nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hdebug --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+#Debugging {
+		if [ "$command" = "jobtracker" ] ; then
+			echo "*** START JOBTRACKER ***"
+			echo "-n $HADOOP_NICENESS $HADOOP_HOME/bin/hdebug-jobtracker --config $HADOOP_CONF_DIR $command $@ > $log 2>&1 < /dev/null &"
+			nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hdebug-jobtracker --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+			echo "*** STOP JOBTRACKER ***"
+			echo "-n" $HADOOP_NICENESS "$HADOOP_HOME""/bin/hdebug-jobtracker --config" $HADOOP_CONF_DIR $command "$@" ">" "$log" "2>&1 < /dev/null &"
+		elif [ "$command" = "tasktracker" ] ; then
+			echo "*** START TASKTRACKER ***"
+			nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hdebug-tasktracker --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+			echo "-n $HADOOP_NICENESS $HADOOP_HOME/bin/hdebug-tasktracker --config $HADOOP_CONF_DIR $command $@ > $log 2>&1 < /dev/null &"
+			echo "*** STOP TASKTRACKER ***"
+		else
+			echo $COMMAND - INVALID DEBUGNING COMMANDS!!!!
+			exit
+		fi
+# } Debugging
     echo $! > $pid
     sleep 1; head "$log"
     ;;
diff --git a/bin/hadoop-daemon.sh b/bin/hadoop-daemon.sh
index e10390a..9b28d4e 100755
--- a/bin/hadoop-daemon.sh
+++ b/bin/hadoop-daemon.sh
@@ -114,7 +114,27 @@ case $startStop in
     hadoop_rotate_log $log
     echo starting $command, logging to $log
     cd "$HADOOP_HOME"
-    nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hadoop --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+#Individual starting {
+		if [ "$command" = "jobtracker" ] ; then
+			echo "*** START JOBTRACKER ***"
+#      echo "-n" $HADOOP_NICENESS "$HADOOP_HOME""/bin/hdebug-jobtracker --config" $HADOOP_CONF_DIR $command "$@" ">" "$log" "2>&1 < /dev/null &"
+				nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hadoop --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+				echo "*** STOP JOBTRACKER ***"
+			elif [ "$command" = "tasktracker" ] ; then
+				echo "*** START TASKTRACKER ***"
+#      echo "-n" $HADOOP_NICENESS "$HADOOP_HOME""/bin/hdebug-tasktracker --config" $HADOOP_CONF_DIR $command "$@" ">" "$log" "2>&1 < /dev/null &"
+				nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hadoop --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+				echo "*** STOP TASKTRACKER ***"
+			elif [ "$command" = "namenode" ] || [ "$command" = "datanode" ] || [ "$command" = "secondarynamenode" ] ; then
+				echo "*** START DFS***"
+				nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hadoop --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
+				echo "*** STOP DFS***"
+			else
+				echo $COMMAND - INVALID DEBUGNING COMMANDS!!!!
+				exit
+			fi
+# } Indivudial starting
+#Original call    nohup nice -n $HADOOP_NICENESS "$HADOOP_HOME"/bin/hadoop --config $HADOOP_CONF_DIR $command "$@" > "$log" 2>&1 < /dev/null &
     echo $! > $pid
     sleep 1; head "$log"
     ;;
diff --git a/bin/hadoop-daemons-debug.sh b/bin/hadoop-daemons-debug.sh
new file mode 100755
index 0000000..3b50ca8
--- /dev/null
+++ b/bin/hadoop-daemons-debug.sh
@@ -0,0 +1,38 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+# Run a Hadoop command on all slave hosts.
+
+usage="Usage: hadoop-daemons.sh [--config confdir] [--hosts hostlistfile] [start|stop] command args..."
+
+# if no args specified, show usage
+if [ $# -le 1 ]; then
+  echo $usage
+  exit 1
+fi
+
+bin=`dirname "$0"`
+bin=`cd "$bin"; pwd`
+
+. $bin/hadoop-config.sh
+
+#$@ = start tasktracker
+#echo "the parameter $ @ is: " "$@"
+echo "Vamos alla"
+echo "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_HOME" \; "$bin/hadoop-daemon-debug.sh" --config $HADOOP_CONF_DIR "$@"
+exec "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_HOME" \; "$bin/hadoop-daemon-debug.sh" --config $HADOOP_CONF_DIR "$@"
diff --git a/bin/hadoop-daemons.sh b/bin/hadoop-daemons.sh
index 894d8ab..0fa4a4d 100755
--- a/bin/hadoop-daemons.sh
+++ b/bin/hadoop-daemons.sh
@@ -31,4 +31,8 @@ bin=`cd "$bin"; pwd`
 
 . $bin/hadoop-config.sh
 
+#$@ = start tasktracker
+#echo "the parameter $ @ is: " "$@"
+echo "Vamos alla"
+#echo "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_HOME" \; "$bin/hadoop-daemon.sh" --config $HADOOP_CONF_DIR "$@"
 exec "$bin/slaves.sh" --config $HADOOP_CONF_DIR cd "$HADOOP_HOME" \; "$bin/hadoop-daemon.sh" --config $HADOOP_CONF_DIR "$@"
diff --git a/bin/hdebug b/bin/hdebug
old mode 100644
new mode 100755
index d0a825b..ed7318a
--- a/bin/hdebug
+++ b/bin/hdebug
@@ -304,7 +304,7 @@ fi
 HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"
 
 # run it
-#echo "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
+echo "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
 exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
 
 echo
diff --git a/bin/hdebug-jobtracker b/bin/hdebug-jobtracker
new file mode 100755
index 0000000..568b1c4
--- /dev/null
+++ b/bin/hdebug-jobtracker
@@ -0,0 +1,315 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+# The Hadoop command script
+#
+# Environment Variables
+#
+#   JAVA_HOME        The java implementation to use.  Overrides JAVA_HOME.
+#
+#   HADOOP_CLASSPATH Extra Java CLASSPATH entries.
+#
+#   HADOOP_HEAPSIZE  The maximum amount of heap to use, in MB. 
+#                    Default is 1000.
+#
+#   HADOOP_OPTS      Extra Java runtime options.
+#   
+#   HADOOP_NAMENODE_OPTS       These options are added to HADOOP_OPTS 
+#   HADOOP_CLIENT_OPTS         when the respective command is run.
+#   HADOOP_{COMMAND}_OPTS etc  HADOOP_JT_OPTS applies to JobTracker 
+#                              for e.g.  HADOOP_CLIENT_OPTS applies to 
+#                              more than one command (fs, dfs, fsck, 
+#                              dfsadmin etc)  
+#
+#   HADOOP_CONF_DIR  Alternate conf dir. Default is ${HADOOP_HOME}/conf.
+#
+#   HADOOP_ROOT_LOGGER The root appender. Default is INFO,console
+#
+
+echo
+echo "******************************************************************"
+echo "******* [JLUPOX] hadoop-0.20.2 in workspacke/hadoop-0.20.2 *******"
+echo "******************************************************************"
+echo
+
+bin=`dirname "$0"`
+bin=`cd "$bin"; pwd`
+
+. "$bin"/hadoop-config.sh
+
+cygwin=false
+case "`uname`" in
+CYGWIN*) cygwin=true;;
+esac
+
+# if no args specified, show usage
+if [ $# = 0 ]; then
+  echo "Usage: hadoop [--config confdir] COMMAND"
+  echo "where COMMAND is one of:"
+  echo "  namenode -format     format the DFS filesystem"
+  echo "  secondarynamenode    run the DFS secondary namenode"
+  echo "  namenode             run the DFS namenode"
+  echo "  datanode             run a DFS datanode"
+  echo "  dfsadmin             run a DFS admin client"
+  echo "  mradmin              run a Map-Reduce admin client"
+  echo "  fsck                 run a DFS filesystem checking utility"
+  echo "  fs                   run a generic filesystem user client"
+  echo "  balancer             run a cluster balancing utility"
+  echo "  jobtracker           run the MapReduce job Tracker node" 
+  echo "  pipes                run a Pipes job"
+  echo "  tasktracker          run a MapReduce task Tracker node" 
+  echo "  job                  manipulate MapReduce jobs"
+  echo "  queue                get information regarding JobQueues" 
+  echo "  version              print the version"
+  echo "  jar <jar>            run a jar file"
+  echo "  distcp <srcurl> <desturl> copy file or directories recursively"
+  echo "  archive -archiveName NAME <src>* <dest> create a hadoop archive"
+  echo "  daemonlog            get/set the log level for each daemon"
+  echo " or"
+  echo "  CLASSNAME            run the class named CLASSNAME"
+  echo "Most commands print help when invoked w/o parameters."
+echo
+echo "******************************************************************"
+echo "******* [JLUPOX] hadoop-0.20.2 in workspacke/hadoop-0.20.2 *******"
+echo "******************************************************************"
+echo
+  exit 1
+fi
+
+# get arguments
+COMMAND=$1
+shift
+
+if [ -f "${HADOOP_CONF_DIR}/hadoop-env.sh" ]; then
+  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
+fi
+
+# some Java parameters
+if [ "$JAVA_HOME" != "" ]; then
+  #echo "run java in $JAVA_HOME"
+  JAVA_HOME=$JAVA_HOME
+fi
+  
+if [ "$JAVA_HOME" = "" ]; then
+  echo "Error: JAVA_HOME is not set."
+  exit 1
+fi
+
+JAVA=$JAVA_HOME/bin/java
+JAVA_HEAP_MAX=-Xmx1000m 
+
+# check envvars which might override default args
+if [ "$HADOOP_HEAPSIZE" != "" ]; then
+  #echo "run with heapsize $HADOOP_HEAPSIZE"
+  JAVA_HEAP_MAX="-Xmx""$HADOOP_HEAPSIZE""m"
+  #echo $JAVA_HEAP_MAX
+fi
+
+# CLASSPATH initially contains $HADOOP_CONF_DIR
+CLASSPATH="${HADOOP_CONF_DIR}"
+CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
+
+# for developers, add Hadoop classes to CLASSPATH
+if [ -d "$HADOOP_HOME/build/classes" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/classes
+fi
+if [ -d "$HADOOP_HOME/build/webapps" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build
+fi
+if [ -d "$HADOOP_HOME/build/test/classes" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/test/classes
+fi
+if [ -d "$HADOOP_HOME/build/tools" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/tools
+fi
+
+# so that filenames w/ spaces are handled correctly in loops below
+IFS=
+
+# for releases, add core hadoop jar & webapps to CLASSPATH
+if [ -d "$HADOOP_HOME/webapps" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME
+fi
+for f in $HADOOP_HOME/hadoop-*-core.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+
+# add libs to CLASSPATH
+for f in $HADOOP_HOME/lib/*.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+
+if [ -d "$HADOOP_HOME/build/ivy/lib/Hadoop/common" ]; then
+for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/*.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+fi
+
+for f in $HADOOP_HOME/lib/jsp-2.1/*.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+
+for f in $HADOOP_HOME/hadoop-*-tools.jar; do
+  TOOL_PATH=${TOOL_PATH}:$f;
+done
+for f in $HADOOP_HOME/build/hadoop-*-tools.jar; do
+  TOOL_PATH=${TOOL_PATH}:$f;
+done
+
+# add user-specified CLASSPATH last
+if [ "$HADOOP_CLASSPATH" != "" ]; then
+  CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}
+fi
+
+# default log directory & file
+if [ "$HADOOP_LOG_DIR" = "" ]; then
+  HADOOP_LOG_DIR="$HADOOP_HOME/logs"
+fi
+if [ "$HADOOP_LOGFILE" = "" ]; then
+  HADOOP_LOGFILE='hadoop.log'
+fi
+
+# default policy file for service-level authorization
+if [ "$HADOOP_POLICYFILE" = "" ]; then
+  HADOOP_POLICYFILE="hadoop-policy.xml"
+fi
+
+# restore ordinary behaviour
+unset IFS
+
+# figure out which class to run
+if [ "$COMMAND" = "namenode" ] ; then
+  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_NAMENODE_OPTS"
+elif [ "$COMMAND" = "secondarynamenode" ] ; then
+  CLASS='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_SECONDARYNAMENODE_OPTS"
+elif [ "$COMMAND" = "datanode" ] ; then
+  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_DATANODE_OPTS"
+elif [ "$COMMAND" = "fs" ] ; then
+  CLASS=org.apache.hadoop.fs.FsShell
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "dfs" ] ; then
+  CLASS=org.apache.hadoop.fs.FsShell
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "dfsadmin" ] ; then
+  CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "mradmin" ] ; then
+  CLASS=org.apache.hadoop.mapred.tools.MRAdmin
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "fsck" ] ; then
+  CLASS=org.apache.hadoop.hdfs.tools.DFSck
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "balancer" ] ; then
+  CLASS=org.apache.hadoop.hdfs.server.balancer.Balancer
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_BALANCER_OPTS"
+elif [ "$COMMAND" = "jobtracker" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobTracker
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_JOBTRACKER_OPTS"
+elif [ "$COMMAND" = "tasktracker" ] ; then
+  CLASS=org.apache.hadoop.mapred.TaskTracker
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_TASKTRACKER_OPTS"
+elif [ "$COMMAND" = "job" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobClient
+elif [ "$COMMAND" = "queue" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobQueueClient
+elif [ "$COMMAND" = "pipes" ] ; then
+  CLASS=org.apache.hadoop.mapred.pipes.Submitter
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "version" ] ; then
+  CLASS=org.apache.hadoop.util.VersionInfo
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "jar" ] ; then
+  CLASS=org.apache.hadoop.util.RunJar
+elif [ "$COMMAND" = "distcp" ] ; then
+  CLASS=org.apache.hadoop.tools.DistCp
+  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "daemonlog" ] ; then
+  CLASS=org.apache.hadoop.log.LogLevel
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "archive" ] ; then
+  CLASS=org.apache.hadoop.tools.HadoopArchives
+  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "sampler" ] ; then
+  CLASS=org.apache.hadoop.mapred.lib.InputSampler
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+else
+  CLASS=$COMMAND
+fi
+
+# cygwin path translation
+if $cygwin; then
+  CLASSPATH=`cygpath -p -w "$CLASSPATH"`
+  HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
+  HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
+  TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
+fi
+# setup 'java.library.path' for native-hadoop code if necessary
+JAVA_LIBRARY_PATH=''
+if [ -d "${HADOOP_HOME}/build/native" -o -d "${HADOOP_HOME}/lib/native" ]; then
+  JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} -Xmx32m org.apache.hadoop.util.PlatformName | sed -e "s/ /_/g"`
+  
+  if [ -d "$HADOOP_HOME/build/native" ]; then
+    JAVA_LIBRARY_PATH=${HADOOP_HOME}/build/native/${JAVA_PLATFORM}/lib
+  fi
+  
+  if [ -d "${HADOOP_HOME}/lib/native" ]; then
+    if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
+      JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
+    else
+      JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
+    fi
+  fi
+fi
+
+# jlupox - to read LD_LIBRARY_PATH
+JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${LD_LIBRARY_PATH}
+
+# cygwin path translation
+if $cygwin; then
+  JAVA_LIBRARY_PATH=`cygpath -p "$JAVA_LIBRARY_PATH"`
+fi
+
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_HOME"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}"
+#Para activar debugacion jlupox
+#Fuente http://sigizmund.com/debugging-hadoop-applications-using-your-eclipse/
+#vvvvvvvvv
+HADOOP_OPTS="$HADOOP_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,address=8005,server=y,suspend=y"
+if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
+  HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
+fi  
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"
+
+# run it
+echo "$JAVA $JAVA_HEAP_MAX $HADOOP_OPTS -classpath $CLASSPATH $CLASS $@"
+exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
+
+echo
+echo "******************************************************************"
+echo "******* [JLUPOX] hadoop-0.20.2 in workspacke/hadoop-0.20.2 *******"
+echo "*************************** [DEBUG] ******************************"
+echo "******************************************************************"
+echo
diff --git a/bin/hdebug-tasktracker b/bin/hdebug-tasktracker
new file mode 100755
index 0000000..9046e88
--- /dev/null
+++ b/bin/hdebug-tasktracker
@@ -0,0 +1,315 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+
+# The Hadoop command script
+#
+# Environment Variables
+#
+#   JAVA_HOME        The java implementation to use.  Overrides JAVA_HOME.
+#
+#   HADOOP_CLASSPATH Extra Java CLASSPATH entries.
+#
+#   HADOOP_HEAPSIZE  The maximum amount of heap to use, in MB. 
+#                    Default is 1000.
+#
+#   HADOOP_OPTS      Extra Java runtime options.
+#   
+#   HADOOP_NAMENODE_OPTS       These options are added to HADOOP_OPTS 
+#   HADOOP_CLIENT_OPTS         when the respective command is run.
+#   HADOOP_{COMMAND}_OPTS etc  HADOOP_JT_OPTS applies to JobTracker 
+#                              for e.g.  HADOOP_CLIENT_OPTS applies to 
+#                              more than one command (fs, dfs, fsck, 
+#                              dfsadmin etc)  
+#
+#   HADOOP_CONF_DIR  Alternate conf dir. Default is ${HADOOP_HOME}/conf.
+#
+#   HADOOP_ROOT_LOGGER The root appender. Default is INFO,console
+#
+
+echo
+echo "******************************************************************"
+echo "******* [JLUPOX] hadoop-0.20.2 in workspacke/hadoop-0.20.2 *******"
+echo "******************************************************************"
+echo
+
+bin=`dirname "$0"`
+bin=`cd "$bin"; pwd`
+
+. "$bin"/hadoop-config.sh
+
+cygwin=false
+case "`uname`" in
+CYGWIN*) cygwin=true;;
+esac
+
+# if no args specified, show usage
+if [ $# = 0 ]; then
+  echo "Usage: hadoop [--config confdir] COMMAND"
+  echo "where COMMAND is one of:"
+  echo "  namenode -format     format the DFS filesystem"
+  echo "  secondarynamenode    run the DFS secondary namenode"
+  echo "  namenode             run the DFS namenode"
+  echo "  datanode             run a DFS datanode"
+  echo "  dfsadmin             run a DFS admin client"
+  echo "  mradmin              run a Map-Reduce admin client"
+  echo "  fsck                 run a DFS filesystem checking utility"
+  echo "  fs                   run a generic filesystem user client"
+  echo "  balancer             run a cluster balancing utility"
+  echo "  jobtracker           run the MapReduce job Tracker node" 
+  echo "  pipes                run a Pipes job"
+  echo "  tasktracker          run a MapReduce task Tracker node" 
+  echo "  job                  manipulate MapReduce jobs"
+  echo "  queue                get information regarding JobQueues" 
+  echo "  version              print the version"
+  echo "  jar <jar>            run a jar file"
+  echo "  distcp <srcurl> <desturl> copy file or directories recursively"
+  echo "  archive -archiveName NAME <src>* <dest> create a hadoop archive"
+  echo "  daemonlog            get/set the log level for each daemon"
+  echo " or"
+  echo "  CLASSNAME            run the class named CLASSNAME"
+  echo "Most commands print help when invoked w/o parameters."
+echo
+echo "******************************************************************"
+echo "******* [JLUPOX] hadoop-0.20.2 in workspacke/hadoop-0.20.2 *******"
+echo "******************************************************************"
+echo
+  exit 1
+fi
+
+# get arguments
+COMMAND=$1
+shift
+
+if [ -f "${HADOOP_CONF_DIR}/hadoop-env.sh" ]; then
+  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
+fi
+
+# some Java parameters
+if [ "$JAVA_HOME" != "" ]; then
+  #echo "run java in $JAVA_HOME"
+  JAVA_HOME=$JAVA_HOME
+fi
+  
+if [ "$JAVA_HOME" = "" ]; then
+  echo "Error: JAVA_HOME is not set."
+  exit 1
+fi
+
+JAVA=$JAVA_HOME/bin/java
+JAVA_HEAP_MAX=-Xmx1000m 
+
+# check envvars which might override default args
+if [ "$HADOOP_HEAPSIZE" != "" ]; then
+  #echo "run with heapsize $HADOOP_HEAPSIZE"
+  JAVA_HEAP_MAX="-Xmx""$HADOOP_HEAPSIZE""m"
+  #echo $JAVA_HEAP_MAX
+fi
+
+# CLASSPATH initially contains $HADOOP_CONF_DIR
+CLASSPATH="${HADOOP_CONF_DIR}"
+CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar
+
+# for developers, add Hadoop classes to CLASSPATH
+if [ -d "$HADOOP_HOME/build/classes" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/classes
+fi
+if [ -d "$HADOOP_HOME/build/webapps" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build
+fi
+if [ -d "$HADOOP_HOME/build/test/classes" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/test/classes
+fi
+if [ -d "$HADOOP_HOME/build/tools" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/tools
+fi
+
+# so that filenames w/ spaces are handled correctly in loops below
+IFS=
+
+# for releases, add core hadoop jar & webapps to CLASSPATH
+if [ -d "$HADOOP_HOME/webapps" ]; then
+  CLASSPATH=${CLASSPATH}:$HADOOP_HOME
+fi
+for f in $HADOOP_HOME/hadoop-*-core.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+
+# add libs to CLASSPATH
+for f in $HADOOP_HOME/lib/*.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+
+if [ -d "$HADOOP_HOME/build/ivy/lib/Hadoop/common" ]; then
+for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/*.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+fi
+
+for f in $HADOOP_HOME/lib/jsp-2.1/*.jar; do
+  CLASSPATH=${CLASSPATH}:$f;
+done
+
+for f in $HADOOP_HOME/hadoop-*-tools.jar; do
+  TOOL_PATH=${TOOL_PATH}:$f;
+done
+for f in $HADOOP_HOME/build/hadoop-*-tools.jar; do
+  TOOL_PATH=${TOOL_PATH}:$f;
+done
+
+# add user-specified CLASSPATH last
+if [ "$HADOOP_CLASSPATH" != "" ]; then
+  CLASSPATH=${CLASSPATH}:${HADOOP_CLASSPATH}
+fi
+
+# default log directory & file
+if [ "$HADOOP_LOG_DIR" = "" ]; then
+  HADOOP_LOG_DIR="$HADOOP_HOME/logs"
+fi
+if [ "$HADOOP_LOGFILE" = "" ]; then
+  HADOOP_LOGFILE='hadoop.log'
+fi
+
+# default policy file for service-level authorization
+if [ "$HADOOP_POLICYFILE" = "" ]; then
+  HADOOP_POLICYFILE="hadoop-policy.xml"
+fi
+
+# restore ordinary behaviour
+unset IFS
+
+# figure out which class to run
+if [ "$COMMAND" = "namenode" ] ; then
+  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_NAMENODE_OPTS"
+elif [ "$COMMAND" = "secondarynamenode" ] ; then
+  CLASS='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_SECONDARYNAMENODE_OPTS"
+elif [ "$COMMAND" = "datanode" ] ; then
+  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_DATANODE_OPTS"
+elif [ "$COMMAND" = "fs" ] ; then
+  CLASS=org.apache.hadoop.fs.FsShell
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "dfs" ] ; then
+  CLASS=org.apache.hadoop.fs.FsShell
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "dfsadmin" ] ; then
+  CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "mradmin" ] ; then
+  CLASS=org.apache.hadoop.mapred.tools.MRAdmin
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "fsck" ] ; then
+  CLASS=org.apache.hadoop.hdfs.tools.DFSck
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "balancer" ] ; then
+  CLASS=org.apache.hadoop.hdfs.server.balancer.Balancer
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_BALANCER_OPTS"
+elif [ "$COMMAND" = "jobtracker" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobTracker
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_JOBTRACKER_OPTS"
+elif [ "$COMMAND" = "tasktracker" ] ; then
+  CLASS=org.apache.hadoop.mapred.TaskTracker
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_TASKTRACKER_OPTS"
+elif [ "$COMMAND" = "job" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobClient
+elif [ "$COMMAND" = "queue" ] ; then
+  CLASS=org.apache.hadoop.mapred.JobQueueClient
+elif [ "$COMMAND" = "pipes" ] ; then
+  CLASS=org.apache.hadoop.mapred.pipes.Submitter
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "version" ] ; then
+  CLASS=org.apache.hadoop.util.VersionInfo
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "jar" ] ; then
+  CLASS=org.apache.hadoop.util.RunJar
+elif [ "$COMMAND" = "distcp" ] ; then
+  CLASS=org.apache.hadoop.tools.DistCp
+  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "daemonlog" ] ; then
+  CLASS=org.apache.hadoop.log.LogLevel
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "archive" ] ; then
+  CLASS=org.apache.hadoop.tools.HadoopArchives
+  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+elif [ "$COMMAND" = "sampler" ] ; then
+  CLASS=org.apache.hadoop.mapred.lib.InputSampler
+  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
+else
+  CLASS=$COMMAND
+fi
+
+# cygwin path translation
+if $cygwin; then
+  CLASSPATH=`cygpath -p -w "$CLASSPATH"`
+  HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
+  HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
+  TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
+fi
+# setup 'java.library.path' for native-hadoop code if necessary
+JAVA_LIBRARY_PATH=''
+if [ -d "${HADOOP_HOME}/build/native" -o -d "${HADOOP_HOME}/lib/native" ]; then
+  JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} -Xmx32m org.apache.hadoop.util.PlatformName | sed -e "s/ /_/g"`
+  
+  if [ -d "$HADOOP_HOME/build/native" ]; then
+    JAVA_LIBRARY_PATH=${HADOOP_HOME}/build/native/${JAVA_PLATFORM}/lib
+  fi
+  
+  if [ -d "${HADOOP_HOME}/lib/native" ]; then
+    if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
+      JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
+    else
+      JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
+    fi
+  fi
+fi
+
+# jlupox - to read LD_LIBRARY_PATH
+JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${LD_LIBRARY_PATH}
+
+# cygwin path translation
+if $cygwin; then
+  JAVA_LIBRARY_PATH=`cygpath -p "$JAVA_LIBRARY_PATH"`
+fi
+
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_HOME"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING"
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}"
+#Para activar debugacion jlupox
+#Fuente http://sigizmund.com/debugging-hadoop-applications-using-your-eclipse/
+#vvvvvvvvv
+HADOOP_OPTS="$HADOOP_OPTS -Xdebug -Xrunjdwp:transport=dt_socket,address=8010,server=y,suspend=y"
+if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
+  HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
+fi  
+HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"
+
+# run it
+echo "$JAVA $JAVA_HEAP_MAX $HADOOP_OPTS -classpath $CLASSPATH $CLASS $@"
+exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS -classpath "$CLASSPATH" $CLASS "$@"
+
+echo
+echo "******************************************************************"
+echo "******* [JLUPOX] hadoop-0.20.2 in workspacke/hadoop-0.20.2 *******"
+echo "*************************** [DEBUG] ******************************"
+echo "******************************************************************"
+echo
diff --git a/bin/start-mapred-debug.sh b/bin/start-mapred-debug.sh
old mode 100644
new mode 100755
diff --git a/build.xml b/build.xml
index 602fcfe..fbf961d 100644
--- a/build.xml
+++ b/build.xml
@@ -61,6 +61,10 @@
   <property name="build.examples" value="${build.dir}/examples"/>
   <property name="build.anttasks" value="${build.dir}/ant"/>
   <property name="build.librecordio" value="${build.dir}/librecordio"/>
+<!--  Author: [JLUPOX]-->
+	<property name="user.libs" value="${basedir}/lib/memcachedfs.jar:${basedir}/lib/pimdfs.jar:${basedir}/lib/memcachedKV.jar:${basedir}/lib/pimdKV.jar"/>
+
+
   <!-- convert spaces to _ so that mac os doesn't break things -->
   <exec executable="sed" inputstring="${os.name}" 
         outputproperty="nonspace.os">
@@ -200,6 +204,8 @@
 
   <!-- the unit test classpath: uses test.src.dir for configuration -->
   <path id="test.classpath">
+<!--    Author: [JLUPOX]-->
+		<pathelement location="${user.libs}"/>
     <pathelement location="${test.build.extraconf}"/>
     <pathelement location="${test.build.classes}" />
     <pathelement location="${test.src.dir}"/>
diff --git a/conf/core-site.xml b/conf/core-site.xml
index 66020f9..3059b11 100644
--- a/conf/core-site.xml
+++ b/conf/core-site.xml
@@ -5,11 +5,27 @@
 
 <configuration>
 
+<!--****************** LOCAL **********************-->
 <!--  <property>-->
 <!--    <name>fs.default.name</name>-->
-<!--    <value>hdfs://localhost/</value>-->
+<!--    <value>file:///</value>-->
 <!--  </property>-->
 
+  <property>
+    <name>fs.intermediateKV.name</name>
+    <value>pimdkv://10.0.0.1:11211</value>
+    <description>URI to locate the PIMD server</description>
+  </property>
+
+<!--*******************************************-->
+
+<!--****************** HDFS **********************-->
+	<property>
+		<name>fs.default.name</name>
+		<value>hdfs://localhost/</value>
+	</property>
+<!--*******************************************-->
+
 <!--<property>-->
 <!--  <name>fs.default.name</name>-->
 <!--  <value>pimdfs://10.0.0.1:11211</value> -->
diff --git a/conf/hadoop-env.sh b/conf/hadoop-env.sh
index 85f5211..416606d 100644
--- a/conf/hadoop-env.sh
+++ b/conf/hadoop-env.sh
@@ -7,13 +7,16 @@
 
 # The java implementation to use.  Required.
 # export JAVA_HOME=/usr/lib/j2sdk1.5-sun
+## LOCALS JAVA_HOMES
 #export JAVA_HOME=/usr/lib/jvm/java-6-sun
+export JAVA_HOME=/home/jlperez/Software/ibm-java-x86_64-60
+## IBM JAVA_HOME
 #export JAVA_HOME=/home/jlperez/hadoop-pimd/ibm-java-ppc64-60
-#export JAVA_HOME=/home/jlperez/Software/ibm-java-x86_64-60
 
 # jlupox
 # for access to libkfsClient.so to use KFS
-export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/home/jlperez/Software/kfs-0.3/build/lib/
+#export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/home/jlperez/Software/kfs-0.3/build/lib/
+export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${HADOOP_HOME}/pimd/lib
 
 
 # Extra Java CLASSPATH elements.  Optional.
diff --git a/conf/hdfs-site.xml b/conf/hdfs-site.xml
index aebc00f..71832bd 100644
--- a/conf/hdfs-site.xml
+++ b/conf/hdfs-site.xml
@@ -5,9 +5,9 @@
 
 <configuration>
 
-<!--  <property>-->
-<!--    <name>dfs.replication</name>-->
-<!--    <value>1</value>-->
-<!--  </property>-->
+	<property>
+		<name>dfs.replication</name>
+		<value>1</value>
+	</property>
 
 </configuration>
diff --git a/conf/log4j.properties b/conf/log4j.properties
index 67ac45d..1d1cb29 100644
--- a/conf/log4j.properties
+++ b/conf/log4j.properties
@@ -96,7 +96,10 @@ log4j.appender.EventCounter=org.apache.hadoop.metrics.jvm.EventCounter
 #
 # Els nostres loggers [JLUPOX]
 #
-log4j.logger.com.ibm.pimd=DEBUG
-log4j.logger.com.ibm.pimdKV=DEBUG
+#log4j.logger.com.ibm.pimd=DEBUG
+#log4j.logger.com.ibm.pimd.PIMD=DEBUG
+#log4j.logger.com.ibm.pimdKV=DEBUG
+#log4j.logger.com.ibm.pimd=INFO
+log4j.logger.com.ibm.pimdKV=INFO
 log4j.logger.org.memcachedKV=ERROR
 
diff --git a/conf/mapred-site.xml b/conf/mapred-site.xml
index 7fda157..4a4ea23 100644
--- a/conf/mapred-site.xml
+++ b/conf/mapred-site.xml
@@ -5,6 +5,13 @@
 
 <configuration>
 
+<!--****************** LOCAL **********************-->
+<!--  <property>-->
+<!--    <name>mapred.job.tracker</name>-->
+<!--    <value>local</value>-->
+<!--  </property>-->
+<!--*******************************************-->
+
 	<property>
 		<name>mapred.job.tracker</name>
 		<value>localhost:8021</value>
diff --git a/conf/slaves b/conf/slaves
index e69de29..2fbb50c 100644
--- a/conf/slaves
+++ b/conf/slaves
@@ -0,0 +1 @@
+localhost
diff --git a/hadoop-0.20.2-ant.jar b/hadoop-0.20.2-ant.jar
deleted file mode 100644
index ad504d3..0000000
Binary files a/hadoop-0.20.2-ant.jar and /dev/null differ
diff --git a/hadoop-0.20.2-core.jar b/hadoop-0.20.2-core.jar
deleted file mode 100644
index 32ae0a1..0000000
Binary files a/hadoop-0.20.2-core.jar and /dev/null differ
diff --git a/hadoop-0.20.2-examples.jar b/hadoop-0.20.2-examples.jar
deleted file mode 100644
index 0b1f328..0000000
Binary files a/hadoop-0.20.2-examples.jar and /dev/null differ
diff --git a/hadoop-0.20.2-test.jar b/hadoop-0.20.2-test.jar
deleted file mode 100644
index 8ee3099..0000000
Binary files a/hadoop-0.20.2-test.jar and /dev/null differ
diff --git a/hadoop-0.20.2-tools.jar b/hadoop-0.20.2-tools.jar
deleted file mode 100644
index deee713..0000000
Binary files a/hadoop-0.20.2-tools.jar and /dev/null differ
diff --git a/ibm-ibm.sh b/ibm-ibm.sh
new file mode 100644
index 0000000..099d629
--- /dev/null
+++ b/ibm-ibm.sh
@@ -0,0 +1,3 @@
+#!/bin/bash
+
+export WHERE=IBM_IBM
diff --git a/makefile b/makefile
index 2471920..e819839 100644
--- a/makefile
+++ b/makefile
@@ -1,42 +1,46 @@
-ifeq ($(WHERE),BSC_IBM)
-  export JAVA_HOME=/home/jlperez/Software/ibm-java-x86_64-60
-  export WRKDIRECTORY=/home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedfs
-  export CLASSPATH=.:/home/jlperez/Software/Java/memcached-2.6rc1.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes:lib/memcachedfs.jar:lib/pimdfs.jar:lib/memcachedKV.jar:lib/pimdKV.jar
-  export SRCMEMCACHEDFS=${WRKDIRECTORY}/sources/org/memcachedfs
-  export CLASSMEMCACHEDFS=classes#/org/memcachedfs
-endif
+export HADOOP_VERSION := 0.20.2-SNAPSHOT
+
+# ifeq ($(WHERE),BSC_IBM)
+#   export JAVA_HOME=/home/jlperez/Software/ibm-java-x86_64-60
+#   export WRKDIRECTORY=/home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedfs
+#   export CLASSPATH=.:/home/jlperez/Software/Java/memcached-2.6rc1.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes:lib/memcachedfs.jar:lib/pimdfs.jar:lib/memcachedKV.jar:lib/pimdKV.jar
+#   export SRCMEMCACHEDFS=${WRKDIRECTORY}/sources/org/memcachedfs
+#   export CLASSMEMCACHEDFS=classes#/org/memcachedfs
+# endif
 ifeq ($(WHERE),BSC_JDK)
 	export JAVA_HOME := /usr/lib/jvm/java-6-sun
-	export WRKDIRECTORY := /home/jlperez/Work/workspace/hadoop-0.20.2
+#   export WRKDIRECTORY := $(PWD)
 	export CLASSPATH := .:/home/jlperez/Software/Java/memcached-2.6rc1.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes
 #   export SRCMEMCACHEDFS=${WRKDIRECTORY}/sources/org/memcachedfs
 #   export CLASSMEMCACHEDFS=classes#/org/memcachedfs
 endif
 ifeq ($(WHERE),IBM_IBM)
-  export JAVA_HOME := /home/jlperez/hadoop-pimd/ibm-java-ppc64-60
-  export ANT_HOME := /home/jlperez/hadoop-pimd/apache-ant-1.8.2/
-  export PATH := ${JAVA_HOME}/bin:${ANT_HOME}/bin:${PATH}
-#   export WRKDIRECTORY=/home/jlperez/hadoop-pimd/hadoop-memcachedfs/jlupox/memcachedfs
-  export WRKDIRECTORY := $(PWD)/../jlupox/memcachedfs
-  export CLASSPATH=.:/home/jlperez/hadoop-pimd/lib/memcached-2.6rc1.jar:/home/jlperez/hadoop-pimd/lib/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes:lib/memcachedfs.jar:lib/pimdfs.jar:lib/memcachedKV.jar:lib/pimdKV.jar
-  export SRCMEMCACHEDFS=${WRKDIRECTORY}/sources/org/memcachedfs
-  export CLASSMEMCACHEDFS=classes#/org/memcachedfs
+	export JAVA_HOME := /home/jlperez/hadoop-pimd/ibm-java-ppc64-60
+	export ANT_HOME := /home/jlperez/hadoop-pimd/apache-ant-1.8.2/
+	export WRKDIRECTORY := $(PWD)
+	export CLASSPATH=.:/home/jlperez/hadoop-pimd/lib/memcached-2.6rc1.jar:/home/jlperez/hadoop-pimd/lib/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes
+#   export CLASSPATH=.:/home/jlperez/hadoop-pimd/lib/memcached-2.6rc1.jar:/home/jlperez/hadoop-pimd/lib/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes:lib/memcachedfs.jar:lib/pimdfs.jar:lib/memcachedKV.jar:lib/pimdKV.jar
+#   export SRCMEMCACHEDFS=${WRKDIRECTORY}/sources/org/memcachedfs
+#   export CLASSMEMCACHEDFS=classes#/org/memcachedfs
+	export PATH := ${JAVA_HOME}/bin:${ANT_HOME}/bin:${PATH}
 endif
 
 echo:
+	@echo HADOOP_VERSION = ${HADOOP_VERSION}
 	@echo JAVA_HOME = ${JAVA_HOME}
 	@echo ANT_HOME = ${ANT_HOME}
 	@echo PATH = ${PATH}
 	@echo WRKDIRECTORY = ${WRKDIRECTORY}
 	@echo CLASSPATH = ${CLASSPATH}
-	@echo SRCMEMCACHEDFS = ${SRCMEMCACHEDFS}
-	@echo CLASSMEMCACHEDFS = ${CLASSMEMCACHEDFS}
+#   @echo SRCMEMCACHEDFS = ${SRCMEMCACHEDFS}
+#   @echo CLASSMEMCACHEDFS = ${CLASSMEMCACHEDFS}
 	
 ant-compile:
 	@ant compile 
 
 ant-common:
-	@ant compile-core-classes 
+	@ant compile-core-classes -verbose
+#   @ant compile-core-classes -lib lib/memcachedfs.jar:lib/pimdfs.jar:lib/memcachedKV.jar:lib/pimdKV.jar -verbose
 
 ant-mapred:
 	@ant compile-mapred-classes
@@ -44,5 +48,15 @@ ant-mapred:
 ant-jar:
 	@ant jar
 
+ant-tools:
+	@ant tools-jar
+
+examples:
+	@ant examples
+	sh moving-build.sh
+
 all: ant-jar
 	sh moving-build.sh
+
+ant-clean:
+	@ant clean
diff --git a/memcached/conf/hadoop-memcachedKV.xml b/memcached/conf/hadoop-memcachedKV.xml
index 0d47d43..8950121 100644
--- a/memcached/conf/hadoop-memcachedKV.xml
+++ b/memcached/conf/hadoop-memcachedKV.xml
@@ -7,9 +7,22 @@
 		<description>URI to locate the memcached server</description>
 	</property>
 
-	<property>
-    <name>mapred.job.tracker</name>
-    <value>local</value>
+  <property>
+    <name>mapred.task.timeout</name>
+    <value>1800000</value> <!-- 30 minutes -->
+  </property>
+
+  <property>
+    <name>mapred.child.java.opts</name>
+    <value>-Xmx200m</value>
+    <final>true</final>
+    <description>........</description>
   </property>
 
+
+<!--  <property>-->
+<!--    <name>mapred.job.tracker</name>-->
+<!--    <value>local</value>-->
+<!--  </property>-->
+
 </configuration>
diff --git a/memcached/conf/hadoop-pimdKV.xml b/memcached/conf/hadoop-pimdKV.xml
index dcdd19a..c59838b 100644
--- a/memcached/conf/hadoop-pimdKV.xml
+++ b/memcached/conf/hadoop-pimdKV.xml
@@ -18,6 +18,18 @@
 <!--    <description>The location of the meta server's port.</description>-->
 <!--  </property>-->
 
+<!-- *** To local input *** -->
+	<property>
+		<name>fs.default.name</name>
+		<value>file:///</value>
+	</property>
+
+	<property>
+		<name>mapred.job.tracker</name>
+		<value>local</value>
+	</property>
+<!-- *** End local input *** -->
+
 
 	<property>
 		<name>fs.intermediateKV.name</name>
diff --git a/memcached/ibm-wordcount/input/file01 b/memcached/ibm-wordcount/input/file01
deleted file mode 100644
index eef3e28..0000000
--- a/memcached/ibm-wordcount/input/file01
+++ /dev/null
@@ -1 +0,0 @@
-Hello World Bye World
diff --git a/memcached/ibm-wordcount/input/file02 b/memcached/ibm-wordcount/input/file02
deleted file mode 100644
index 9ad4f68..0000000
--- a/memcached/ibm-wordcount/input/file02
+++ /dev/null
@@ -1 +0,0 @@
-Hello Hadoop Goodby Hadoop
diff --git a/memcached/ibm-wordcount/launch b/memcached/ibm-wordcount/launch
deleted file mode 100644
index 97f74a9..0000000
--- a/memcached/ibm-wordcount/launch
+++ /dev/null
@@ -1,3 +0,0 @@
-hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
-
-
diff --git a/memcached/ibm-wordcount/makefile b/memcached/ibm-wordcount/makefile
new file mode 100644
index 0000000..86ea7eb
--- /dev/null
+++ b/memcached/ibm-wordcount/makefile
@@ -0,0 +1,150 @@
+export HADOOP_VERSION := 0.20.2-SNAPSHOT
+export HADOOP_HOME := $(PWD)/../..
+export HADOOP_COMMON_HOME := ${HADOOP_HOME}
+export HADOOP_INSTALL := ${HADOOP_HOME}
+
+export ANT_HOME := /home/jlperez/hadoop-pimd/apache-ant-1.8.2
+export JAVA_HOME := /home/jlperez/hadoop-pimd/ibm-java-ppc64-60
+export PATH := ${JAVA_HOME}/bin:${PATH}
+
+export LD_LIBRARY_PATH := ${HADOOP_INSTALL}/pimd/lib
+export CLASSPATH := ${HADOOP_INSTALL}/hadoop-${HADOOP_VERSION}-core.jar:${UNIT_TEST}
+
+#LD_LIBRARY_PATH=~/Software/kfs-0.3/build/lib/
+#CLASSPATH :=/home/jlperez/Software/kfs-0.5/build/lib/kfs-0.5.jar:.:..:${HADOOP_INSTALL}/hadoop-hdfs-0.21.0.jar:${HADOOP_INSTALL}/hadoop-mapred-0.21.0.jar:${HADOOP_INSTALL}/hadoop-common-0.21.0.jar
+
+# export PATH := ${HADOOP_INSTALL}/bin:${PATH}
+export PATH := ${JAVA_HOME}/bin:${HADOOP_INSTALL}/bin:${PATH}
+
+echo:
+	@echo HADOOP_HOME = ${HADOOP_HOME}
+	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
+	@echo HADOOP_INSTALL = ${HADOOP_INSTALL}
+	@echo LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}
+	@echo CLASSPATH = ${CLASSPATH}
+	@echo JAVA_HOME = ${JAVA_HOME}
+	@echo PATH = ${PATH}
+
+format:
+	hadoop namenode -format
+
+start-all:
+	../../bin/start-dfs.sh
+	../../bin/start-mapred.sh
+
+start-dfs:
+	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
+	../../bin/start-dfs.sh
+
+start-mapred:
+	../../bin/start-mapred.sh
+	
+start-mapred-debug:
+	  ../../bin/start-mapred-debug.sh
+
+start-jobtracker:
+	$(HADOOP_HOME)/bin/hadoop-daemon.sh --config $(HADOOP_HOME)/bin/../conf start jobtracker
+
+start-jobtracker-debug:
+	$(HADOOP_HOME)/bin/hadoop-daemon-debug.sh --config $(HADOOP_HOME)/bin/../conf start jobtracker 
+
+start-tasktracker:
+	$(HADOOP_HOME)/bin/hadoop-daemons.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+#   $(HADOOP_HOME)/bin/hadoop-daemon.sh \
+#   --config $(HADOOP_HOME)/bin/../conf \
+#   --script $(HADOOP_HOME)/bin/mapred start tasktracker
+
+start-tasktracker-debug:
+	$(HADOOP_HOME)/bin/hadoop-daemons-debug.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+
+
+stop-all:
+	../../bin/stop-dfs.sh
+	../../bin/stop-mapred.sh
+
+stop-dfs:
+	../../bin/stop-dfs.sh
+
+stop-mapred:
+	../../bin/stop-mapred.sh
+
+jps:
+	@ps aux | grep java | grep -v grep | awk '{print $$NF}'
+
+cpinput:
+	hadoop fs -mkdir input
+	hadoop fs -put input/* input/
+	hadoop fs -ls input
+
+view-output:
+	hadoop fs -ls output
+	hadoop fs -cat output/*
+
+view-input:
+	hadoop fs -ls input
+	hadoop fs -cat input/*
+
+rm-output:
+	hadoop fs -rmr output
+
+rm-input:
+	hadoop fs -rmr input
+
+delnamenode:
+	rm -r /tmp/hadoop-y99yse83*
+
+kfs-ls:
+	hadoop fs -fs kfs://localhost:40000 -ls /
+
+local-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
+
+local-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
+
+hdfs-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount input/ output
+
+hdfs-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount input/ output
+
+kfs-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
+
+kfs-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
+
+memcachedfs-hadoop:
+#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
+	hadoop jar wordcount.jar org.myorg.WordCount input/ output
+
+memcachedfs-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
+
+memcachedkv-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedkv.xml input/ output
+
+memcachedkv-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedkv.xml input/ output
+
+prueba:
+	hadoop classpath
+
+pimd-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
+
+pimd-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
+
+pimdkv-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+
+pimdkv-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+
+WordCount:
+	@javac -Xlint ./org/myorg/WordCount.java -d ./classes
+	@cp ./org/myorg/WordCount.java ./classes
+	@jar -cvf wordcount.jar classes/WordCount.java -C classes org/myorg
+	@rm -r classes/*
diff --git a/memcached/ibm-wordcount/makefile.bsc b/memcached/ibm-wordcount/makefile.bsc
deleted file mode 100644
index 6523f88..0000000
--- a/memcached/ibm-wordcount/makefile.bsc
+++ /dev/null
@@ -1,112 +0,0 @@
-export HADOOP_VERSION := "0.21.0-SNAPSHOT"
-export HADOOP_HOME := /home/jlperez/hadoop-pimd/hadoop-memcachedfs
-export HADOOP_COMMON_HOME := ${HADOOP_HOME}
-export HADOOP_INSTALL := ${HADOOP_HOME}
-
-#CLASSPATH :=/home/jlperez/Software/kfs-0.5/build/lib/kfs-0.5.jar:.:..:${HADOOP_INSTALL}/hadoop-hdfs-0.21.0.jar:${HADOOP_INSTALL}/hadoop-mapred-0.21.0.jar:${HADOOP_INSTALL}/hadoop-common-0.21.0.jar
-
-#LD_LIBRARY_PATH=~/Software/kfs-0.3/build/lib/
-export CLASSPATH=.:..:${HADOOP_INSTALL}/hadoop-hdfs-${HADOOP_VERSION}.jar:${HADOOP_INSTALL}/hadoop-mapred-${HADOOP_VERSION}.jar:${HADOOP_INSTALL}/hadoop-common-${HADOOP_VERSION}.jar:${UNIT_TEST}
-
-# export PATH := ${HADOOP_INSTALL}/bin:${PATH}
-export PATH := ${JAVA_HOME}/bin:${HADOOP_INSTALL}:${PATH}
-
-echo:
-	@echo HADOOP_HOME = ${HADOOP_HOME}
-	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
-	@echo HADOOP_INSTALL = ${HADOOP_INSTALL}
-	@echo LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}
-	@echo CLASSPATH = ${CLASSPATH}
-	@echo JAVA_HOME = ${JAVA_HOME}
-	@echo PATH = ${PATH}
-
-format:
-	hadoop namenode -format
-
-start-all:
-	../../bin/start-dfs.sh
-	../../bin/start-mapred.sh
-
-start-dfs:
-	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
-	../../bin/start-dfs.sh
-
-start-mapred:
-	../../bin/start-mapred.sh
-
-start-tasktracker:
-	$(HADOOP_HOME)/bin/hadoop-daemon.sh \
-	--config $(HADOOP_HOME)/bin/../conf \
-	--script $(HADOOP_HOME)/bin/mapred start tasktracker
-
-stop-all:
-	../../bin/stop-dfs.sh
-	../../bin/stop-mapred.sh
-
-stop-dfs:
-	../../bin/stop-dfs.sh
-
-stop-mapred:
-	../../bin/stop-mapred.sh
-
-jps:
-	@ps aux | grep java | grep -v grep | awk '{print $NF}'
-
-cpinput:
-	hadoop fs -mkdir input
-	hadoop fs -put input/* input/
-	hadoop fs -ls input
-
-view-output:
-	hadoop fs -ls output
-	hadoop fs -cat output/*
-
-view-input:
-	hadoop fs -ls input
-	hadoop fs -cat input/*
-
-rm-output:
-	hadoop fs -rmr output
-
-rm-input:
-	hadoop fs -rmr input
-
-delnamenode:
-	rm -r /tmp/hadoop-y99yse83*
-
-kfs-ls:
-	hadoop fs -fs kfs://localhost:40000 -ls /
-
-local-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
-
-local-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
-
-hdfs-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount input/ output
-
-hdfs-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount input/ output
-
-kfs-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
-
-kfs-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
-
-memcachedfs-hadoop:
-#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
-	hadoop jar wordcount.jar org.myorg.WordCount input/ output
-
-memcachedfs-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
-
-prueba:
-	hadoop classpath
-
-pimd-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
-
-pimd-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
diff --git a/memcached/ibm-wordcount/makefile.ibm b/memcached/ibm-wordcount/makefile.ibm
deleted file mode 100644
index b55d98a..0000000
--- a/memcached/ibm-wordcount/makefile.ibm
+++ /dev/null
@@ -1,139 +0,0 @@
-# To eliminate var.sh
-# @source export ANT_HOME=/home/jlperez/hadoop-pimd/apache-ant-1.8.2
-# @source export JAVA_HOME=/home/jlperez/hadoop-pimd/ibm-java-ppc64-60
-# @source export PATH=$JAVA_HOME/bin:$PATH
-
-export ANT_HOME := /home/jlperez/hadoop-pimd/apache-ant-1.8.2
-export JAVA_HOME := /home/jlperez/hadoop-pimd/ibm-java-ppc64-60
-export PATH := ${JAVA_HOME}/bin:${PATH}
-
-export HADOOP_VERSION := "0.21.0-SNAPSHOT"
-# export HADOOP_HOME := /home/jlperez/hadoop-pimd/hadoop-memcachedfs
-export HADOOP_HOME := $(PWD)/../..
-export HADOOP_COMMON_HOME := ${HADOOP_HOME}
-export HADOOP_INSTALL := ${HADOOP_HOME}
-
-#CLASSPATH :=/home/jlperez/Software/kfs-0.5/build/lib/kfs-0.5.jar:.:..:${HADOOP_INSTALL}/hadoop-hdfs-0.21.0.jar:${HADOOP_INSTALL}/hadoop-mapred-0.21.0.jar:${HADOOP_INSTALL}/hadoop-common-0.21.0.jar
-
-#LD_LIBRARY_PATH=~/Software/kfs-0.3/build/lib/
-export LD_LIBRARY_PATH := ${HADOOP_INSTALL}/pimd/lib
-export CLASSPATH := .:..:${HADOOP_INSTALL}/hadoop-hdfs-${HADOOP_VERSION}.jar:${HADOOP_INSTALL}/hadoop-mapred-${HADOOP_VERSION}.jar:${HADOOP_INSTALL}/hadoop-common-${HADOOP_VERSION}.jar:${UNIT_TEST}
-
-# export PATH := ${HADOOP_INSTALL}/bin:${PATH}
-export PATH := ${JAVA_HOME}/bin:${HADOOP_INSTALL}/bin:${PATH}
-
-echo:
-	@echo HADOOP_HOME = ${HADOOP_HOME}
-	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
-	@echo HADOOP_INSTALL = ${HADOOP_INSTALL}
-	@echo LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}
-	@echo CLASSPATH = ${CLASSPATH}
-	@echo JAVA_HOME = ${JAVA_HOME}
-	@echo PATH = ${PATH}
-
-format:
-	hadoop namenode -format
-
-start-all:
-	../../bin/start-dfs.sh
-	../../bin/start-mapred.sh
-
-start-dfs:
-	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
-	../../bin/start-dfs.sh
-
-start-mapred:
-	../../bin/start-mapred.sh
-	
-start-mapred-debug:
-	  ../../bin/start-mapred-debug.sh
-
-start-tasktracker:
-	$(HADOOP_HOME)/bin/hadoop-daemon.sh \
-	--config $(HADOOP_HOME)/bin/../conf \
-	--script $(HADOOP_HOME)/bin/mapred start tasktracker
-
-stop-all:
-	../../bin/stop-dfs.sh
-	../../bin/stop-mapred.sh
-
-stop-dfs:
-	../../bin/stop-dfs.sh
-
-stop-mapred:
-	../../bin/stop-mapred.sh
-
-jps:
-	@ps aux | grep java | grep -v grep | awk '{print $$NF}'
-
-cpinput:
-	hadoop fs -mkdir input
-	hadoop fs -put input/* input/
-	hadoop fs -ls input
-
-view-output:
-	hadoop fs -ls output
-	hadoop fs -cat output/*
-
-view-input:
-	hadoop fs -ls input
-	hadoop fs -cat input/*
-
-rm-output:
-	hadoop fs -rmr output
-
-rm-input:
-	hadoop fs -rmr input
-
-delnamenode:
-	rm -r /tmp/hadoop-y99yse83*
-
-kfs-ls:
-	hadoop fs -fs kfs://localhost:40000 -ls /
-
-local-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
-
-local-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
-
-hdfs-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount input/ output
-
-hdfs-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount input/ output
-
-kfs-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
-
-kfs-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
-
-memcachedfs-hadoop:
-#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
-	hadoop jar wordcount.jar org.myorg.WordCount input/ output
-
-memcachedfs-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
-
-memcachedkv-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedkv.xml input/ output
-
-memcachedkv-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedkv.xml input/ output
-
-prueba:
-	hadoop classpath
-
-pimd-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
-
-pimd-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
-
-pimdkv-debug:
-	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
-
-pimdkv-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml large-input/ output
-#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
diff --git a/memcached/ibm-wordcount/org/myorg/WordCount.java b/memcached/ibm-wordcount/org/myorg/WordCount.java
index 3f752bc..b8db511 100644
--- a/memcached/ibm-wordcount/org/myorg/WordCount.java
+++ b/memcached/ibm-wordcount/org/myorg/WordCount.java
@@ -51,7 +51,7 @@ public class WordCount extends Configured implements Tool {
      job.setOutputValueClass(IntWritable.class);
 
      job.setMapperClass(Map.class);
-     job.setCombinerClass(Reduce.class);
+//     job.setCombinerClass(Reduce.class);
      job.setReducerClass(Reduce.class);
 
      job.setInputFormatClass(TextInputFormat.class);
@@ -65,7 +65,10 @@ public class WordCount extends Configured implements Tool {
    }
 
    public static void main(String[] args) throws Exception {
-     int ret = ToolRunner.run(new WordCount(), args);
+			 long startWordCount = System.currentTimeMillis(); //[Time_Calc]
+		 int ret = ToolRunner.run(new WordCount(), args);
+			 long stopWordCount = System.currentTimeMillis(); //[Time_Calc]
+			 System.out.println("[TIME]:   [WordCount] Final WordCount Time: "+ (stopWordCount-startWordCount) + " ms"); //[Time_Calc]
      System.exit(ret);
    }
 }
diff --git a/memcached/memcachedKV/makefile b/memcached/memcachedKV/makefile
index 7db6029..76efc57 100644
--- a/memcached/memcachedKV/makefile
+++ b/memcached/memcachedKV/makefile
@@ -1,7 +1,8 @@
 
 ifeq ($(WHERE),BSC_IBM)
 	export JAVA_HOME := /home/jlperez/Software/ibm-java-x86_64-60
-	export WRKDIRECTORY := /home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedKV
+#   export WRKDIRECTORY := /home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedKV
+	export WRKDIRECTORY := $(PWD)
 	export SRCMEMCACHEDKV := ${WRKDIRECTORY}/sources/org/memcachedKV
 	export CONF_FOLDER := ${WRKDIRECTORY}/conf
 #   export CLASSPATH := .:/home/jlperez/Software/Java/spymemcached-2.7.3.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar:/home/jlperez/Software/Java/log4j-1.2.16.jar:CONF_FOLDER:${WRKDIRECTORY}/classes
@@ -9,7 +10,8 @@ ifeq ($(WHERE),BSC_IBM)
 	export CLASSMEMCACHEDKV := classes/org/memcachedKV
 endif
 ifeq ($(WHERE),BSC_JDK)
-	export WRKDIRECTORY := /home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedKV
+#   export WRKDIRECTORY := /home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedKV
+	export WRKDIRECTORY := $(PWD)
 	export SRCMEMCACHEDKV := ${WRKDIRECTORY}/sources/org/memcachedKV
 	export CONF_FOLDER := ${WRKDIRECTORY}/conf
 	export CLASSPATH := .:/home/jlperez/Software/Java/spymemcached-2.7.3.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar:/home/jlperez/Software/Java/log4j-1.2.16.jar:CONF_FOLDER:${WRKDIRECTORY}/classes
@@ -20,7 +22,8 @@ ifeq ($(WHERE),IBM_IBM)
 	export ANT_HOME := /home/jlperez/hadoop-pimd/apache-ant-1.8.2
 	export PATH := ${JAVA_HOME}/bin:${PATH}
 	@echo "CAGADA, CAMBIAR LA MANERA DE DEFINIR EL WRKDIRECTORY POR PWD"
-	export WRKDIRECTORY := /home/jlperez/hadoop-pimd/hadoop-memcachedfs/jlupox/memcachedKV
+#   export WRKDIRECTORY := /home/jlperez/hadoop-pimd/hadoop-memcachedfs/jlupox/memcachedKV
+	export WRKDIRECTORY := $(PWD)
 	export SRCMEMCACHEDKV := ${WRKDIRECTORY}/sources/org/memcachedKV
 	export CONF_FOLDER := ${WRKDIRECTORY}/conf
 	export CLASSPATH := .:/home/jlperez/hadoop-pimd/lib/spymemcached-2.7.3.jar:/home/jlperez/hadoop-pimd/lib/java-getopt-1.0.13.jar:/home/jlperez/hadoop-pimd/lib/log4j-1.2.16.jar:CONF_FOLDER:${WRKDIRECTORY}/classes
@@ -49,8 +52,9 @@ jar-memcachedKV: memcachedKV
 #	@ant jar
 
 copy-jars:
-	@echo "Copying memcachedKV.jar to hadoop-memcachedfs/lib"
-	@cp  memcachedKV.jar ~/Work/workspace/hadoop-memcachedfs/lib
+	@echo "Copying memcachedKV.jar to $(WRKDIRECTORY)/../../lib"
+#   @cp  memcachedKV.jar ~/Work/workspace/hadoop-memcachedfs/lib
+	@cp  memcachedKV.jar $(WRKDIRECTORY)/../../lib
 
 # tools: memcachedKV
 tools:
diff --git a/memcached/memcachedfs/makefile.bsc b/memcached/memcachedfs/makefile.bsc
index dbf4df7..c10962a 100644
--- a/memcached/memcachedfs/makefile.bsc
+++ b/memcached/memcachedfs/makefile.bsc
@@ -1,6 +1,7 @@
 export JAVA_HOME := /home/jlperez/Software/ibm-java-x86_64-60
 
-export WRKDIRECTORY:= /home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedfs
+# export WRKDIRECTORY:= /home/jlperez/Work/workspace/hadoop-memcachedfs/jlupox/memcachedfs
+export WRKDIRECTORY:= $(PWD)
 #CLASSPATH := .:./memcachedfs.jar:./memcachedfstools.jar:${WRKDIRECTORY}/classes:/home/jlperez/Software/Java/memcached-2.6rc1.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar
 export CLASSPATH := .:/home/jlperez/Software/Java/memcached-2.6rc1.jar:/home/jlperez/Software/Java/java-getopt-1.0.13.jar:${WRKDIRECTORY}/classes
 export SRCMEMCACHEDFS := ${WRKDIRECTORY}/sources/org/memcachedfs
@@ -53,8 +54,8 @@ jar-tools: tools
 	@jar -cvf memcachedfstools.jar -C classes org/memcachedfs/tools
 
 copy:
-	@echo "Copying memcachedfs.jar to hadoop-memcachedfs/lib"
-	@cp memcachedfs.jar ~/Work/workspace/hadoop-memcachedfs/lib
+	@echo "Copying memcachedfs.jar to $(WRKDIRECTORY)/../../lib"
+	@cp memcachedfs.jar $(WRKDIRECTORY)/../../lib
 
 sync-workspace2:
 	@echo "sync with workspace2"
diff --git a/memcached/wordcount/combine.bat b/memcached/wordcount/combine.bat
deleted file mode 100644
index 298709c..0000000
--- a/memcached/wordcount/combine.bat
+++ /dev/null
@@ -1,76 +0,0 @@
-rem Run this batch file if you want one big text file for the Bible and one for the Apocrypha.
-copy Genesis.txt web.txt
-type Exodus.txt >> web.txt
-type Lev.txt >> web.txt
-type Num.txt >> web.txt
-type Deut.txt >> web.txt
-type Joshua.txt >> web.txt
-type Judges.txt >> web.txt
-type Ruth.txt >> web.txt
-type 1Sam.txt >> web.txt
-type 2Sam.txt >> web.txt
-type 1Kings.txt >> web.txt
-type 2Kings.txt >> web.txt
-type 1Chron.txt >> web.txt
-type 2Chron.txt >> web.txt
-type Ezra.txt >> web.txt
-type Nehemiah.txt >> web.txt
-type Esther.txt >> web.txt
-type Job.txt >> web.txt
-type Psalms.txt >> web.txt
-type Proverbs.txt >> web.txt
-type Eccl.txt >> web.txt
-type Song.txt >> web.txt
-type Isaiah.txt >> web.txt
-type Jeremiah.txt >> web.txt
-type Lament.txt >> web.txt
-type Ezekiel.txt >> web.txt
-type Daniel.txt >> web.txt
-type Hosea.txt >> web.txt
-type Joel.txt >> web.txt
-type Amos.txt >> web.txt
-type Obadiah.txt >> web.txt
-type Jonah.txt >> web.txt
-type Micah.txt >> web.txt
-type Nahum.txt >> web.txt
-type Habakkuk.txt >> web.txt
-type Zeph.txt >> web.txt
-type Haggai.txt >> web.txt
-type Zech.txt >> web.txt
-type Malachi.txt >> web.txt
-type Matthew.txt >> web.txt
-type Mark.txt >> web.txt
-type Luke.txt >> web.txt
-type John.txt >> web.txt
-type Acts.txt >> web.txt
-type Romans.txt >> web.txt
-type 1Cor.txt >> web.txt
-type 2Cor.txt >> web.txt
-type Gal.txt >> web.txt
-type Eph.txt >> web.txt
-type Philip.txt >> web.txt
-type Col.txt >> web.txt
-type 1Thes.txt >> web.txt
-type 2Thes.txt >> web.txt
-type 1Tim.txt >> web.txt
-type 2Tim.txt >> web.txt
-type Titus.txt >> web.txt
-type Philemon.txt >> web.txt
-type Hebrews.txt >> web.txt
-type James.txt >> web.txt
-type 1Peter.txt >> web.txt
-type 2Peter.txt >> web.txt
-type 1John.txt >> web.txt
-type 2John.txt >> web.txt
-type 3John.txt >> web.txt
-type Jude.txt >> web.txt
-type Rev.txt >> web.txt
-copy Tobit.txt apoc.txt
-type Judith.txt >> apoc.txt
-type GkEsther.txt >> apoc.txt
-rem type Wisdom.txt >> apoc.txt
-rem type Sirach.txt >> apoc.txt
-type Baruch.txt >> apoc.txt
-type Let.txt >> apoc.txt
-type AddDan.txt >> apoc.txt
-
diff --git a/memcached/wordcount/file01 b/memcached/wordcount/file01
deleted file mode 100644
index eef3e28..0000000
--- a/memcached/wordcount/file01
+++ /dev/null
@@ -1 +0,0 @@
-Hello World Bye World
diff --git a/memcached/wordcount/file02 b/memcached/wordcount/file02
deleted file mode 100644
index 9ad4f68..0000000
--- a/memcached/wordcount/file02
+++ /dev/null
@@ -1 +0,0 @@
-Hello Hadoop Goodby Hadoop
diff --git a/memcached/wordcount/input/file01 b/memcached/wordcount/input/file01
deleted file mode 100644
index eef3e28..0000000
--- a/memcached/wordcount/input/file01
+++ /dev/null
@@ -1 +0,0 @@
-Hello World Bye World
diff --git a/memcached/wordcount/input/file02 b/memcached/wordcount/input/file02
deleted file mode 100644
index 9ad4f68..0000000
--- a/memcached/wordcount/input/file02
+++ /dev/null
@@ -1 +0,0 @@
-Hello Hadoop Goodby Hadoop
diff --git a/memcached/wordcount/launch b/memcached/wordcount/launch
deleted file mode 100644
index 97f74a9..0000000
--- a/memcached/wordcount/launch
+++ /dev/null
@@ -1,3 +0,0 @@
-hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
-
-
diff --git a/memcached/wordcount/makefile b/memcached/wordcount/makefile
index bbf0a3b..f844d12 100644
--- a/memcached/wordcount/makefile
+++ b/memcached/wordcount/makefile
@@ -1,5 +1,5 @@
 HADOOP_VERSION := 0.20.2-SNAPSHOT
-HADOOP_HOME := /home/jlperez/Work/workspace/hadoop-0.20.2
+HADOOP_HOME := $(PWD)/../..
 HADOOP_COMMON_HOME := ${HADOOP_HOME}
 HADOOP_INSTALL := ${HADOOP_HOME}
 
@@ -37,16 +37,23 @@ start-mapred:
 start-mapred-debug:
 	../../bin/start-mapred-debug.sh
 
+start-jobtracker:
+	$(HADOOP_HOME)/bin/hadoop-daemon.sh --config $(HADOOP_HOME)/bin/../conf start jobtracker 
+
+start-jobtracker-debug:
+	$(HADOOP_HOME)/bin/hadoop-daemon-debug.sh --config $(HADOOP_HOME)/bin/../conf start jobtracker 
+
 start-tasktracker:
-	$(HADOOP_HOME)/bin/hadoop-daemon.sh \
-	--config $(HADOOP_HOME)/bin/../conf \
-	--script $(HADOOP_HOME)/bin/mapred start tasktracker
-#      $(HADOOP_HOME)/bin/hadoop-daemon.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+	$(HADOOP_HOME)/bin/hadoop-daemons.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+#   $(HADOOP_HOME)/bin/hadoop-daemon.sh \
+#   --config $(HADOOP_HOME)/bin/../conf \
+#   --script $(HADOOP_HOME)/bin/mapred start tasktracker
 
 start-tasktracker-debug:
-	$(HADOOP_HOME)/bin/hadoop-daemon.sh \
-	--config $(HADOOP_HOME)/bin/../conf \
-	--script $(HADOOP_HOME)/bin/mapred-debug start tasktracker
+	$(HADOOP_HOME)/bin/hadoop-daemons-debug.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+#   $(HADOOP_HOME)/bin/hadoop-daemon.sh \
+#   --config $(HADOOP_HOME)/bin/../conf \
+#   --script $(HADOOP_HOME)/bin/mapred-debug start tasktracker
 
 stop-all:
 	../../bin/stop-dfs.sh
@@ -58,6 +65,9 @@ stop-dfs:
 stop-mapred:
 	../../bin/stop-mapred.sh
 
+jps:
+	@ps aux | grep java | grep -v grep | awk '{print $$NF}'
+
 cpinput:
 	hadoop fs -mkdir input
 	hadoop fs -put input/* input/
@@ -110,7 +120,7 @@ memcachedfs-debug:
 	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
 
 memcachedkv-hadoop:
-	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedKV.xml large-input/ output
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedKV.xml input/ output
 #   hadoop jar wordcount.jar org.myorg.WordCount input/ output
 
 memcachedkv-debug:
@@ -125,7 +135,13 @@ pimd-debug:
 pimd-hadoop:
 	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
 
-WorCount:
+pimdkv-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+
+pimdkv-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+
+WordCount:
 	@javac -Xlint ./org/myorg/WordCount.java -d ./classes
 	@cp ./org/myorg/WordCount.java ./classes
 	@jar -cvf wordcount.jar classes/WordCount.java -C classes org/myorg
diff --git a/memcached/wordcount/org/myorg/WordCount.base b/memcached/wordcount/org/myorg/WordCount.base
deleted file mode 100644
index 3f752bc..0000000
--- a/memcached/wordcount/org/myorg/WordCount.base
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.myorg;
-	
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.*;
-import org.apache.hadoop.io.*;
-import org.apache.hadoop.mapreduce.*;
-import org.apache.hadoop.mapreduce.lib.input.*;
-import org.apache.hadoop.mapreduce.lib.output.*;
-import org.apache.hadoop.util.*;
-
-public class WordCount extends Configured implements Tool {
-
-   public static class Map
-       extends Mapper<LongWritable, Text, Text, IntWritable> {
-     private final static IntWritable one = new IntWritable(1);
-     private Text word = new Text();
-
-     public void map(LongWritable key, Text value, Context context)
-         throws IOException, InterruptedException {
-       String line = value.toString();
-       StringTokenizer tokenizer = new StringTokenizer(line);
-       while (tokenizer.hasMoreTokens()) {
-         word.set(tokenizer.nextToken());
-         context.write(word, one);
-       }
-     }
-   }
-
-   public static class Reduce
-       extends Reducer<Text, IntWritable, Text, IntWritable> {
-     public void reduce(Text key, Iterable<IntWritable> values,
-         Context context) throws IOException, InterruptedException {
-
-       int sum = 0;
-       for (IntWritable val : values) {
-         sum += val.get();
-       }
-       context.write(key, new IntWritable(sum));
-     }
-   }
-
-   public int run(String [] args) throws Exception {
-     Job job = new Job(getConf());
-     job.setJarByClass(WordCount.class);
-     job.setJobName("wordcount");
-
-     job.setOutputKeyClass(Text.class);
-     job.setOutputValueClass(IntWritable.class);
-
-     job.setMapperClass(Map.class);
-     job.setCombinerClass(Reduce.class);
-     job.setReducerClass(Reduce.class);
-
-     job.setInputFormatClass(TextInputFormat.class);
-     job.setOutputFormatClass(TextOutputFormat.class);
-
-     FileInputFormat.setInputPaths(job, new Path(args[0]));
-     FileOutputFormat.setOutputPath(job, new Path(args[1]));
-
-     boolean success = job.waitForCompletion(true);
-     return success ? 0 : 1;
-   }
-
-   public static void main(String[] args) throws Exception {
-     int ret = ToolRunner.run(new WordCount(), args);
-     System.exit(ret);
-   }
-}
diff --git a/memcached/wordcount/org/myorg/WordCount.java b/memcached/wordcount/org/myorg/WordCount.java
index f56cfb9..6ff4dd5 100644
--- a/memcached/wordcount/org/myorg/WordCount.java
+++ b/memcached/wordcount/org/myorg/WordCount.java
@@ -44,12 +44,6 @@ public class WordCount extends Configured implements Tool {
 
    public int run(String [] args) throws Exception {
      Job job = new Job(getConf());
-		 //testing
-//     job.setProfileEnabled(true);
-//     job.setProfileParams("-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s");
-//     System.out.println("***** Activando profiling");
-//     job.setProfileTaskRange(true, "0-2");
-
      job.setJarByClass(WordCount.class);
      job.setJobName("wordcount");
 
@@ -62,8 +56,6 @@ public class WordCount extends Configured implements Tool {
 
      job.setInputFormatClass(TextInputFormat.class);
      job.setOutputFormatClass(TextOutputFormat.class);
-     
-	//     job.setNumReduceTasks(2);
 
      FileInputFormat.setInputPaths(job, new Path(args[0]));
      FileOutputFormat.setOutputPath(job, new Path(args[1]));
@@ -73,7 +65,10 @@ public class WordCount extends Configured implements Tool {
    }
 
    public static void main(String[] args) throws Exception {
+  	 	long startWordCount = System.currentTimeMillis(); //[Time_Calc]
      int ret = ToolRunner.run(new WordCount(), args);
+     	long stopWordCount = System.currentTimeMillis(); //[Time_Calc]
+			System.out.println("[TIME]:   [WordCount] Final WordCount Time: "+ (stopWordCount-startWordCount) + " ms"); //[Time_Calc]
      System.exit(ret);
    }
 }
diff --git a/memcached/wordcount/org/myorg/WordCount.modified b/memcached/wordcount/org/myorg/WordCount.modified
new file mode 100644
index 0000000..f56cfb9
--- /dev/null
+++ b/memcached/wordcount/org/myorg/WordCount.modified
@@ -0,0 +1,79 @@
+package org.myorg;
+	
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.conf.*;
+import org.apache.hadoop.io.*;
+import org.apache.hadoop.mapreduce.*;
+import org.apache.hadoop.mapreduce.lib.input.*;
+import org.apache.hadoop.mapreduce.lib.output.*;
+import org.apache.hadoop.util.*;
+
+public class WordCount extends Configured implements Tool {
+
+   public static class Map
+       extends Mapper<LongWritable, Text, Text, IntWritable> {
+     private final static IntWritable one = new IntWritable(1);
+     private Text word = new Text();
+
+     public void map(LongWritable key, Text value, Context context)
+         throws IOException, InterruptedException {
+       String line = value.toString();
+       StringTokenizer tokenizer = new StringTokenizer(line);
+       while (tokenizer.hasMoreTokens()) {
+         word.set(tokenizer.nextToken());
+         context.write(word, one);
+       }
+     }
+   }
+
+   public static class Reduce
+       extends Reducer<Text, IntWritable, Text, IntWritable> {
+     public void reduce(Text key, Iterable<IntWritable> values,
+         Context context) throws IOException, InterruptedException {
+
+       int sum = 0;
+       for (IntWritable val : values) {
+         sum += val.get();
+       }
+       context.write(key, new IntWritable(sum));
+     }
+   }
+
+   public int run(String [] args) throws Exception {
+     Job job = new Job(getConf());
+		 //testing
+//     job.setProfileEnabled(true);
+//     job.setProfileParams("-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s");
+//     System.out.println("***** Activando profiling");
+//     job.setProfileTaskRange(true, "0-2");
+
+     job.setJarByClass(WordCount.class);
+     job.setJobName("wordcount");
+
+     job.setOutputKeyClass(Text.class);
+     job.setOutputValueClass(IntWritable.class);
+
+     job.setMapperClass(Map.class);
+//     job.setCombinerClass(Reduce.class);
+     job.setReducerClass(Reduce.class);
+
+     job.setInputFormatClass(TextInputFormat.class);
+     job.setOutputFormatClass(TextOutputFormat.class);
+     
+	//     job.setNumReduceTasks(2);
+
+     FileInputFormat.setInputPaths(job, new Path(args[0]));
+     FileOutputFormat.setOutputPath(job, new Path(args[1]));
+
+     boolean success = job.waitForCompletion(true);
+     return success ? 0 : 1;
+   }
+
+   public static void main(String[] args) throws Exception {
+     int ret = ToolRunner.run(new WordCount(), args);
+     System.exit(ret);
+   }
+}
diff --git a/memcached/wordcount/output/part-r-00000 b/memcached/wordcount/output/part-r-00000
deleted file mode 100644
index bc927ae..0000000
--- a/memcached/wordcount/output/part-r-00000
+++ /dev/null
@@ -1,6 +0,0 @@
-Bye	182
-Hello	182
-Wo	1
-World	361
-World).	1
-Worlddddd	1
diff --git a/moving-build.sh b/moving-build.sh
index c877d19..fc20b71 100644
--- a/moving-build.sh
+++ b/moving-build.sh
@@ -3,6 +3,7 @@
 if [ "$(find ./build -maxdepth 1 -name '*.jar')" ]
 then
 	mv ./build/*.jar .
+	cp hadoop-${HADOOP_VERSION}-core.jar vanilla_jars/devel/
 	echo "All hadoop common jar are been moved"
 else
 	echo "There are NOT hadoop common jars"
diff --git a/pimd/Makefile b/pimd/Makefile
index ce49f80..3dc4508 100644
--- a/pimd/Makefile
+++ b/pimd/Makefile
@@ -2,8 +2,10 @@
 # Vars
 # ************************
 export JAVA_HOME=/home/jlperez/hadoop-pimd/ibm-java-ppc64-60
+# export JAVA_HOME=/home/jlperez/Software/ibm-java-x86_64-60
+# export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home
 # export JAVA_HOME=/gsa/yktgsa/home/y/5/y5469583/runtimes/ibm-java-ppc64-6.0-9.2/ibm-java-ppc64-60
-# export JAVA_HOME=/home/dcarrera/runtimes/jdk7/sdk
+#export JAVA_HOME=/home/dcarrera/runtimes/jdk7/sdk
 export JAVA=$(JAVA_HOME)/bin/java
 export JAVAC=$(JAVA_HOME)/bin/javac
 export JAVAH=$(JAVA_HOME)/bin/javah
@@ -29,7 +31,8 @@ export PIMDFS_JAR=$(LIBDIR)/pimdfs.jar
 export PIMDKV_JAR=$(LIBDIR)/pimdKV.jar
 export PIMD_WRAPPER_JAR=$(LIBDIR)/pimdwrapper.jar
 export BENCH_JAR=$(LIBDIR)/bench_java.jar
-export UNIT_TEST=/home/jlperez/hadoop-pimd/lib/junit-4.10.jar
+export UNIT_TEST=$(LIBDIR)/junit-4.10.jar
+export PIMD_UTILS_JAR=$(LIBDIR)/pimdutils.jar
 
 export JNI_JAVA_SRC=$(PWD)/jni_java/java/src
 export JIT_OPTIONS=
@@ -44,9 +47,11 @@ PIMDFILE2 = $(shell grep PERSISTENT_FILE_LOCAL_PATH  ~/.pimd_server.conf | grep
 
 #Add libPkLinux.a
 
-.PHONY: jni_c jni_java pimdfs utils
+.PHONY: jni_c jni_java pimdfs utils pimd_utils
 
-all: jni_c pimdfs pimdkv jni_java utils
+all: jni_c pimdfs pimdkv jni_java utils pimd_utils
+	@echo "Copying --- lib/pimdKV.jar lib/pimdfs.jar lib/pimdwrapper.jar lib/pimdutils.jar ../lib"
+	@cp lib/pimdKV.jar lib/pimdfs.jar lib/pimdwrapper.jar lib/pimdutils.jar ../lib
 
 # ************************
 # Compilation
@@ -57,6 +62,8 @@ pimdkv:
 	make -C pimdKV
 jni_java: 
 	make -C pimd_wrapper/jni_java
+pimd_utils:
+	make -C pimd_utils
 jni_c: jni_java
 	make -C pimd_wrapper/jni_c
 utils:
@@ -86,8 +93,12 @@ run4:
 	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test4
 run5:
 	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test5
+run5Async:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test5Async
 run6:
 	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test6
+runiter:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) -Xdebug -Xrunjdwp:transport=dt_socket,address=8001,server=y,suspend=y com.ibm.pimd.IteratorTest
 
 tester:
 	make -C utils tester
@@ -101,6 +112,9 @@ run_tester2:
 run_tester3:
 	make -C utils run_tester3
 
+run_tester4:
+	make -C utils run_tester4
+
 bench_c:
 	make -C bench bench_c
 
@@ -110,8 +124,11 @@ bench_java:
 run_bench_c:
 	make -C bench run_bench_c
 
-run_bench_java: jni_c jni_java bench_java
-	$(JAVA) $(JIT_OPTIONS) -cp $(PIMD_WRAPPER_JAR):$(BENCH_JAR):$(LOG4J_JAR):$(CONF_FOLDER) -Djava.library.path=$(LIB_PATH) com.ibm.pimd.BenchBuffers
+run_bench_java_sync: jni_c jni_java
+	make -C bench run_bench_java_sync
+
+run_bench_java_async: jni_c jni_java
+	make -C bench run_bench_java_async
 
 runpimdfs:
 	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):$(LIBDIR)/java-getopt-1.0.13.jar: -Djava.library.path=$(LIB_PATH) com.ibm.pimdfs.tools.MemShell -s localhost -p 2727
@@ -163,5 +180,6 @@ clean:
 	make -C pimd_wrapper/jni_c clean
 	make -C pimdfs clean
 	make -C pimdKV clean
+	make -C pimd_utils clean
 	make -C utils clean
 	make -C bench clean
diff --git a/pimd/Makefile.fake b/pimd/Makefile.fake
new file mode 100644
index 0000000..5ac0c64
--- /dev/null
+++ b/pimd/Makefile.fake
@@ -0,0 +1,185 @@
+# ************************
+# Vars
+# ************************
+# export JAVA_HOME=/home/jlperez/hadoop-pimd/ibm-java-ppc64-60
+export JAVA_HOME=/home/jlperez/Software/ibm-java-x86_64-60
+# export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home
+# export JAVA_HOME=/gsa/yktgsa/home/y/5/y5469583/runtimes/ibm-java-ppc64-6.0-9.2/ibm-java-ppc64-60
+#export JAVA_HOME=/home/dcarrera/runtimes/jdk7/sdk
+export JAVA=$(JAVA_HOME)/bin/java
+export JAVAC=$(JAVA_HOME)/bin/javac
+export JAVAH=$(JAVA_HOME)/bin/javah
+export JAR=$(JAVA_HOME)/bin/jar
+export INCLUDE=$(PWD)/pimd_wrapper/jni_c/include
+export LIBDIR=$(PWD)/lib
+export CONF_FOLDER=$(PWD)/conf
+export PIMDLIB=/home/lschneid/opt/pimd/lib
+export PIMDINC=/home/lschneid/opt/pimd/include
+export MPIINC=/home/lschneid/opt/mpich/include
+export LIB_PATH=$(LIBDIR):$(PIMDLIB)
+export CC=gcc
+export CPP=g++
+
+export FXLOG_OPTIONS=
+#export FXLOG_OPTIONS=-DPKFXLOG
+
+export CFLAGS=-DPK_LINUX $(FXLOG_IOPTIONS) -DPIMD_CLIENT_UNI -L/usr/lib64 -libverbs -lrdmacm -lrt
+export LIBS=$(PIMDLIB)/libpimd_client.a $(PIMDLIB)/libpimd_relational.a $(PIMDLIB)/libpimd_common.a $(PIMDLIB)/libit_api_o_verbs.a $(PIMDLIB)/libPkLinux.a
+
+export PIMDFS_JAVA_SRC=$(PWD)/pimdfs
+export PIMDFS_JAR=$(LIBDIR)/pimdfs.jar
+export PIMDKV_JAR=$(LIBDIR)/pimdKV.jar
+export PIMD_WRAPPER_JAR=$(LIBDIR)/pimdwrapper.jar
+export BENCH_JAR=$(LIBDIR)/bench_java.jar
+export UNIT_TEST=$(LIBDIR)/junit-4.10.jar
+export PIMD_UTILS_JAR=$(LIBDIR)/pimdutils.jar
+
+export JNI_JAVA_SRC=$(PWD)/jni_java/java/src
+export JIT_OPTIONS=
+#export JIT_OPTIONS=-Xnojit
+
+export LOG4J_JAR=$(LIBDIR)/log4j-1.2.16.jar
+
+
+export ID=`whoami`
+PIMDFILE1 = $(shell grep PIMD_SERVER_READY_FILE  ~/.pimd_server.conf | grep -v "\#" |cut -d "=" -f 2 |  sed 's/^[ \t]*//')
+PIMDFILE2 = $(shell grep PERSISTENT_FILE_LOCAL_PATH  ~/.pimd_server.conf | grep -v "\#" |cut -d "=" -f 2 |  sed 's/^[ \t]*//').0
+
+#Add libPkLinux.a
+
+.PHONY: jni_c jni_java pimdfs utils pimd_utils
+
+all: jni_c pimdfs pimdkv jni_java utils pimd_utils
+	@echo "Copying --- lib/pimdKV.jar lib/pimdfs.jar lib/pimdwrapper.jar lib/pimdutils.jar ../lib"
+	@cp lib/pimdKV.jar lib/pimdfs.jar lib/pimdwrapper.jar lib/pimdutils.jar ../lib
+
+# ************************
+# Compilation
+# ************************
+pimdfs:
+	make -C pimdfs
+pimdkv:
+	make -C pimdKV
+jni_java: 
+	make -C pimd_wrapper/jni_java
+pimd_utils:
+	make -C pimd_utils
+# jni_c: jni_java
+#   make -C pimd_wrapper/jni_c
+# utils:
+#   make -C utils
+
+# ************************
+# Unit Tests
+# ************************
+# pimdfs:
+#   make -C pimdfs
+# pimdkv:
+#   make -C pimdKV
+unit_tests: 
+#   make -C pimd_wrapper/jni_java unit_test
+	make -C pimdKV unit_test
+
+# ***************************
+# Runners
+# ***************************
+run:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test
+run2:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test2
+run3:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test3
+run4:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test4
+run5:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test5
+run5Async:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test5Async
+run6:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) com.ibm.pimd.Test6
+runiter:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER)  -Djava.library.path=$(LIB_PATH) -Xdebug -Xrunjdwp:transport=dt_socket,address=8001,server=y,suspend=y com.ibm.pimd.IteratorTest
+
+tester:
+	make -C utils tester
+
+run_tester1:
+	make -C utils run_tester1
+
+run_tester2:
+	make -C utils run_tester2
+
+run_tester3:
+	make -C utils run_tester3
+
+run_tester4:
+	make -C utils run_tester4
+
+bench_c:
+	make -C bench bench_c
+
+bench_java:
+	make -C bench bench_jar
+
+run_bench_c:
+	make -C bench run_bench_c
+
+run_bench_java_sync: jni_c jni_java
+	make -C bench run_bench_java_sync
+
+run_bench_java_async: jni_c jni_java
+	make -C bench run_bench_java_async
+
+runpimdfs:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):$(LIBDIR)/java-getopt-1.0.13.jar: -Djava.library.path=$(LIB_PATH) com.ibm.pimdfs.tools.MemShell -s localhost -p 2727
+
+runtest:
+	$(JAVA) -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):$(LIBDIR)/java-getopt-1.0.13.jar: -Djava.library.path=$(LIB_PATH) com.ibm.pimdfs.tools.Test
+
+run-shell:
+	@echo "Running Shell"
+	@java -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):$(LIBDIR)/java-getopt-1.0.13.jar -Djava.library.path=$(LIB_PATH) com.ibm.pimdfs.tools.MemShell -s 10.0.0.1 -p MyPDS # 11211
+
+run-hadoop:
+	@echo "Filling data in PIMDFS"
+	@java -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):utils -Djava.library.path=$(LIB_PATH) hadoop
+
+run-pimdkv-tester1:
+	@echo "Filling data in PIMDKV Intermediate"
+	@java -cp $(PIMD_WRAPPER_JAR):$(PIMDKV_JAR):$(LOG4J_JAR):$(CONF_FOLDER):utils -Djava.library.path=$(LIB_PATH) pimdkv_tester1
+
+run-view:
+	@echo "run-view"
+	@java -cp $(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):utils -Djava.library.path=$(LIB_PATH) view
+
+run-viewKV:
+	@echo "run-viewKV"
+	@java -cp $(PIMD_WRAPPER_JAR):$(PIMDKV_JAR):$(LOG4J_JAR):$(CONF_FOLDER):utils -Djava.library.path=$(LIB_PATH) viewKV
+
+# ***************************
+# Utils
+# ***************************
+ps:
+	@ps aux | grep ${ID} | grep mpd | grep mpi | grep -v grep
+	@ps aux | grep ${ID} | grep PIMDServer | grep -v grep
+start-mpi01:
+	@/home/lschneid/opt/mpich/bin/mpd &
+start-mpi02:
+	@/home/lschneid/opt/mpich/bin/mpirun -n 1 /home/lschneid/opt/pimd/bin/PIMDServer </dev/null &
+
+pimd:	clean-pimd start-mpi01 start-mpi02
+
+clean-pimd:
+	@echo Killing PIMDServer and removing $(PIMDFILE1) and $(PIMDFILE2)
+	@skill -9 PIMDServer
+	@rm -f $(PIMDFILE1)
+	@rm -f $(PIMDFILE2)
+
+clean:
+	make -C pimd_wrapper/jni_java clean
+	make -C pimd_wrapper/jni_c clean
+	make -C pimdfs clean
+	make -C pimdKV clean
+	make -C pimd_utils clean
+	make -C utils clean
+	make -C bench clean
diff --git a/pimd/bench/Makefile b/pimd/bench/Makefile
index 5de69f7..6422490 100644
--- a/pimd/bench/Makefile
+++ b/pimd/bench/Makefile
@@ -6,7 +6,7 @@ bench_c:
 
 bench_java:
 	mkdir -p java/bin
-	$(JAVAC) -cp $(PIMD_WRAPPER_JAR):$(LOG4J_JAR) -d java/bin java/src/com/ibm/pimd/BenchBuffers.java
+	$(JAVAC) -cp $(PIMD_WRAPPER_JAR):$(LOG4J_JAR) -d java/bin java/src/com/ibm/pimd/BenchBuffers.java java/src/com/ibm/pimd/BenchBuffersAsync.java
 
 bench_jar: bench_java
 	$(JAR) cvf $(BENCH_JAR) -C java/bin . 
@@ -15,8 +15,12 @@ bench_jar: bench_java
 run_bench_c: bench_c
 	./bench
 
-run_bench_java: bench_java
-	$(JAVA) -jar $(BENCH_JAR):$(LOG4J_JAR):$(PIMD_WRAPPER_JAR) com.ibm.pimd.BenchBuffers
+run_bench_java_sync: bench_jar
+	$(JAVA) $(JIT_OPTIONS) -cp $(PIMD_WRAPPER_JAR):$(BENCH_JAR):$(LOG4J_JAR):$(CONF_FOLDER) -Djava.library.path=$(LIB_PATH) com.ibm.pimd.BenchBuffers
+
+run_bench_java_async: bench_jar
+	$(JAVA) $(JIT_OPTIONS) -cp $(PIMD_WRAPPER_JAR):$(BENCH_JAR):$(LOG4J_JAR):$(CONF_FOLDER) -Djava.library.path=$(LIB_PATH) com.ibm.pimd.BenchBuffersAsync
+
 
 clean:
 	rm -f bench
diff --git a/pimd/bench/bench.cpp b/pimd/bench/bench.cpp
index 8fa1b53..c4bbffa 100644
--- a/pimd/bench/bench.cpp
+++ b/pimd/bench/bench.cpp
@@ -26,8 +26,8 @@ typedef int pimd_client_t;
 #define NUM_RUNS				5
 //#define MAX_KEY_SIZE			32			// 32 bytes
 #define MAX_KEY_SIZE			256			// 256 bytes
-//#define MAX_VALUE_SIZE			2097152		// 2MB
-#define MAX_VALUE_SIZE			33554432		// 32MB
+#define MAX_VALUE_SIZE			2097152		// 2MB
+//#define MAX_VALUE_SIZE			33554432		// 32MB
 #define FOLDER_NAME_RAWDATA		"rawdata"
 #define FOLDER_NAME_STATS		"stats"
 
diff --git a/pimd/bench/java/src/com/ibm/pimd/BenchBuffers.java b/pimd/bench/java/src/com/ibm/pimd/BenchBuffers.java
index 4a2d4fe..94f25a2 100644
--- a/pimd/bench/java/src/com/ibm/pimd/BenchBuffers.java
+++ b/pimd/bench/java/src/com/ibm/pimd/BenchBuffers.java
@@ -40,7 +40,7 @@ public class BenchBuffers {
 		folder_rawdata.mkdir();
 
 
-		PIMD pimd = new PIMD();
+		PIMD pimd = new PIMD(PIMD.SYNCHRONOUS);
 		pimd.Connect("10.0.0.1","MyPDS");
 		System.out.println("Connected!");
 
@@ -50,6 +50,7 @@ public class BenchBuffers {
 			e.printStackTrace();
 			System.exit(0);
 		}
+		pimd.Wait();
 
 
 	}
diff --git a/pimd/bench/java/src/com/ibm/pimd/BenchBuffersAsync.java b/pimd/bench/java/src/com/ibm/pimd/BenchBuffersAsync.java
new file mode 100644
index 0000000..617cef3
--- /dev/null
+++ b/pimd/bench/java/src/com/ibm/pimd/BenchBuffersAsync.java
@@ -0,0 +1,653 @@
+package com.ibm.pimd;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.io.File;
+import java.io.FileWriter;
+import java.io.BufferedWriter;
+import java.io.PrintWriter;
+import java.io.IOException;
+
+import com.ibm.pimd.*;
+
+
+
+
+public class BenchBuffersAsync {
+
+
+	public static int GROWING_SCALE			=	2 ;
+	public static int TEST_STEPS			=	1;
+
+	public static int OPERATIONS_IN_TEST	=	100;
+	public static int NUM_RUNS				=	5;
+	public static int MAX_KEY_SIZE			=	32;			// 32 bytes
+	public static int MAX_VALUE_SIZE		=	2097152;	// 2MB
+	public static String FOLDER_NAME_RAWDATA=	"rawdata";
+	public static String FOLDER_NAME_STATS	=	"stats";
+
+
+	private static PrintWriter testoutput_stats;
+
+
+	public static void main(String args[]) {
+
+
+		File folder_stats = new File(FOLDER_NAME_STATS);
+		File folder_rawdata = new File(FOLDER_NAME_RAWDATA);	
+
+		folder_stats.mkdir();
+		folder_rawdata.mkdir();
+
+
+		PIMD pimd = new PIMD(PIMD.ASYNCHRONOUS);
+		pimd.Connect("10.0.0.1","MyPDS");
+		System.out.println("Connected!");
+
+		try {
+			runTests(pimd);
+		} catch (IOException e) {
+			e.printStackTrace();
+			System.exit(0);
+		}
+		pimd.Wait();
+
+
+	}
+
+
+	private static void runTests(PIMD pimd) throws java.io.IOException {
+
+		_runTestInsertScalability(pimd, OPERATIONS_IN_TEST);
+		_runTestUpdateScalability(pimd, OPERATIONS_IN_TEST);
+		_runTestRetrieveScalability(pimd, OPERATIONS_IN_TEST);
+		_runTestRemoveScalability(pimd, OPERATIONS_IN_TEST);
+			
+	}
+
+
+
+	/****************************************************************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 ****************************    INSERT TEST   ******************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 **************************************************************************************** 
+	 */
+
+
+
+
+	private static void _runTestInsertScalability(PIMD pimd, int numOperations) throws IOException {
+		// TODO: Check Flag for Inserts (no update)		
+
+		testoutput_stats = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_STATS+File.separator+"insert_stats.csv")));
+		testoutput_stats.println("keySize, valueSize, operations, errors, mean_time(ns), mean_ops/s");
+
+
+		for(int KeySize=4; KeySize<=MAX_KEY_SIZE; KeySize*=GROWING_SCALE) {
+			for(int ValueSize=4; ValueSize<=MAX_VALUE_SIZE; ValueSize*=GROWING_SCALE) {
+
+				_runTestInsertSingle(pimd, numOperations, GROWING_SCALE, TEST_STEPS, KeySize, ValueSize);
+			}
+			testoutput_stats.println();
+		}
+
+
+		testoutput_stats.close();	
+
+	}
+
+
+
+
+
+
+	private static void _runTestInsertSingle(PIMD pimd, int numOperations, int scale, int steps, int KeySize, int ValueSize) throws IOException {
+
+
+
+		int errors = 0;
+		int keyVal = 0;
+		long diff = 0;
+		int current_step = 0;
+
+
+
+		//		ByteBuffer Key = ByteBuffer.allocateDirect(KeySize);
+		ByteBuffer Value = ByteBuffer.allocateDirect(ValueSize);
+
+
+		System.out.println("Insert Test. KeySize: "+KeySize+", ValueSize: "+ValueSize+", Test to be repeated "+NUM_RUNS+" times.... ");
+
+
+
+		for (int current_scale=1; current_step++<steps; current_scale*=scale){
+
+			int operations_in_this_loop = numOperations * current_scale;
+
+
+
+			PrintWriter testoutput_all = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_RAWDATA+File.separator+"insert_"+KeySize+"_"+ValueSize+"_all.csv")));
+
+			testoutput_all.println("iteration, keySize, valueSize, operations, errors, time(ns), ops/s");
+
+
+
+			long cumulative_diff = 0;
+			long cumulative_ops = 0;
+
+
+			for(int iteration=0;iteration<NUM_RUNS;iteration++) {
+
+
+				errors = 0;
+
+
+
+				int last_opertation_in_this_loop = keyVal + operations_in_this_loop;
+
+
+				//TIME IN
+				long tp_begin = System.nanoTime();
+
+
+				int keyFoo = keyVal;
+				ByteBuffer keys[] = new ByteBuffer[last_opertation_in_this_loop - keyFoo];
+				for(;keyFoo<last_opertation_in_this_loop;keyFoo++) {
+					keys[last_opertation_in_this_loop - keyFoo-1] = ByteBuffer.allocateDirect(KeySize);
+					keys[last_opertation_in_this_loop - keyFoo-1].putInt(0,keyFoo);
+				}
+
+				for(;keyVal<last_opertation_in_this_loop;keyVal++) {
+
+					// INSERT
+					pimd.Insert(keys[last_opertation_in_this_loop - keyVal-1], Value);
+
+
+				}
+
+				// TIME_OUT
+				long tp_end = System.nanoTime();				
+
+
+				diff = tp_end - tp_begin;				
+
+				if(tp_end < tp_begin)
+					System.err.println("Time error - Begin: "+tp_begin+" > End: "+tp_end+" -- diff: "+diff);
+
+				cumulative_diff += diff/operations_in_this_loop;
+				cumulative_ops += 1000000000L*operations_in_this_loop/diff;
+
+				testoutput_all.println(iteration+","+KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+diff/operations_in_this_loop+","+(1000000000L*operations_in_this_loop/diff));
+
+			}
+
+			testoutput_stats.println(KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+cumulative_diff/NUM_RUNS+","+(cumulative_ops/NUM_RUNS));
+			testoutput_stats.flush();
+
+
+			testoutput_all.close();
+
+
+
+		}
+
+		System.out.println(keyVal+" non-updating insert operations completed!");;
+
+		clean(pimd, keyVal, KeySize);
+
+
+	}
+
+
+
+
+	/****************************************************************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 ****************************    UPDATE TEST   ******************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 **************************************************************************************** 
+	 */
+
+	private static void _runTestUpdateScalability(PIMD pimd, int numOperations) throws IOException {
+		// TODO: Check Flag for Inserts (no update)		
+
+		testoutput_stats = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_STATS+File.separator+"update_stats.csv")));
+		testoutput_stats.println("keySize, valueSize, operations, errors, mean_time(ns), mean_ops/s");
+
+
+		for(int KeySize=4; KeySize<=MAX_KEY_SIZE; KeySize*=GROWING_SCALE) {
+			for(int ValueSize=4; ValueSize<=MAX_VALUE_SIZE; ValueSize*=GROWING_SCALE) {
+
+				_runTestUpdateSingle(pimd, numOperations, GROWING_SCALE, TEST_STEPS, KeySize, ValueSize);
+			}
+			testoutput_stats.println();
+		}
+		testoutput_stats.close();	
+	}
+
+
+
+
+
+
+	private static void _runTestUpdateSingle(PIMD pimd, int numOperations, int scale, int steps, int KeySize, int ValueSize) throws IOException {
+
+
+
+		int errors = 0;
+		int keyVal = 0;
+		long diff = 0;
+		int current_step = 0;
+
+
+		ByteBuffer Value = ByteBuffer.allocateDirect(ValueSize);
+
+
+		System.out.println("Update Test. KeySize: "+KeySize+", ValueSize: "+ValueSize+", Test to be repeated "+NUM_RUNS+" times.... ");
+
+
+
+		for (int current_scale=1; current_step++<steps; current_scale*=scale){
+
+			int operations_in_this_loop = numOperations * current_scale;
+
+
+
+			PrintWriter testoutput_all = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_RAWDATA+File.separator+"update_"+KeySize+"_"+ValueSize+"_all.csv")));
+
+			testoutput_all.println("iteration, keySize, valueSize, operations, errors, time(ns), ops/s");
+
+
+
+			long cumulative_diff = 0;
+			long cumulative_ops = 0;
+
+
+			for(int iteration=0;iteration<NUM_RUNS;iteration++) {
+
+
+				errors = 0;
+
+
+
+				int last_opertation_in_this_loop = keyVal + operations_in_this_loop;
+
+
+				populate(pimd, keyVal, operations_in_this_loop, KeySize, ValueSize);
+
+
+
+				//TIME IN
+				long tp_begin = System.nanoTime();
+
+
+				int keyFoo = keyVal;
+				ByteBuffer keys[] = new ByteBuffer[last_opertation_in_this_loop - keyFoo];
+				for(;keyFoo<last_opertation_in_this_loop;keyFoo++) {
+					keys[last_opertation_in_this_loop - keyFoo-1] = ByteBuffer.allocateDirect(KeySize);
+					keys[last_opertation_in_this_loop - keyFoo-1].putInt(0,keyFoo);
+				}
+
+				for(;keyVal<last_opertation_in_this_loop;keyVal++) {
+
+					// UPDATE
+					pimd.Update(keys[last_opertation_in_this_loop - keyVal-1], Value);
+
+
+				}
+
+				// TIME_OUT
+				long tp_end = System.nanoTime();				
+
+
+				diff = tp_end - tp_begin;				
+
+				if(tp_end < tp_begin)
+					System.err.println("Time error - Begin: "+tp_begin+" > End: "+tp_end+" -- diff: "+diff);
+
+				cumulative_diff += diff/operations_in_this_loop;
+				cumulative_ops += 1000000000L*operations_in_this_loop/diff;
+
+				testoutput_all.println(iteration+","+KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+diff/operations_in_this_loop+","+(1000000000L*operations_in_this_loop/diff));
+
+			}
+
+			testoutput_stats.println(KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+cumulative_diff/NUM_RUNS+","+(cumulative_ops/NUM_RUNS));
+			testoutput_stats.flush();
+
+
+			testoutput_all.close();
+
+
+
+		}
+
+		System.out.println(keyVal+" non-updating update operations completed!");;
+
+		clean(pimd, keyVal, KeySize);
+
+
+	}
+
+
+	/****************************************************************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 ****************************    RETRIEVE TEST   ****************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 **************************************************************************************** 
+	 */
+
+	private static void _runTestRetrieveScalability(PIMD pimd, int numOperations) throws IOException {
+		// TODO: Check Flag for Inserts (no update)		
+
+		testoutput_stats = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_STATS+File.separator+"retrieve_stats.csv")));
+		testoutput_stats.println("keySize, valueSize, operations, errors, mean_time(ns), mean_ops/s");
+
+
+		for(int KeySize=4; KeySize<=MAX_KEY_SIZE; KeySize*=GROWING_SCALE) {
+			for(int ValueSize=4; ValueSize<=MAX_VALUE_SIZE; ValueSize*=GROWING_SCALE) {
+
+				_runTestRetrieveSingle(pimd, numOperations, GROWING_SCALE, TEST_STEPS, KeySize, ValueSize);
+			}
+			testoutput_stats.println();
+		}
+		testoutput_stats.close();	
+	}
+
+
+
+
+
+
+	private static void _runTestRetrieveSingle(PIMD pimd, int numOperations, int scale, int steps, int KeySize, int ValueSize) throws IOException {
+
+
+
+		int errors = 0;
+		int keyVal = 0;
+		long diff = 0;
+		int current_step = 0;
+
+
+		ByteBuffer Value = ByteBuffer.allocateDirect(ValueSize);
+
+
+		System.out.println("Retrieve Test. KeySize: "+KeySize+", ValueSize: "+ValueSize+", Test to be repeated "+NUM_RUNS+" times.... ");
+
+
+
+		for (int current_scale=1; current_step++<steps; current_scale*=scale){
+
+			int operations_in_this_loop = numOperations * current_scale;
+
+
+
+			PrintWriter testoutput_all = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_RAWDATA+File.separator+"retrieve_"+KeySize+"_"+ValueSize+"_all.csv")));
+
+			testoutput_all.println("iteration, keySize, valueSize, operations, errors, time(ns), ops/s");
+
+
+
+			long cumulative_diff = 0;
+			long cumulative_ops = 0;
+
+
+			for(int iteration=0;iteration<NUM_RUNS;iteration++) {
+
+
+				errors = 0;
+
+
+
+				int last_opertation_in_this_loop = keyVal + operations_in_this_loop;
+
+
+				populate(pimd, keyVal, operations_in_this_loop, KeySize, ValueSize);
+
+
+
+				//TIME IN
+				long tp_begin = System.nanoTime();
+
+
+				int keyFoo = keyVal;
+				ByteBuffer keys[] = new ByteBuffer[last_opertation_in_this_loop - keyFoo];
+				for(;keyFoo<last_opertation_in_this_loop;keyFoo++) {
+					keys[last_opertation_in_this_loop - keyFoo-1] = ByteBuffer.allocateDirect(KeySize);
+					keys[last_opertation_in_this_loop - keyFoo-1].putInt(0,keyFoo);
+				}
+
+				for(;keyVal<last_opertation_in_this_loop;keyVal++) {
+
+					// RETRIEVE
+					pimd.Retrieve(keys[last_opertation_in_this_loop - keyVal-1], ValueSize);
+
+
+				}
+
+				// TIME_OUT
+				long tp_end = System.nanoTime();				
+
+
+				diff = tp_end - tp_begin;				
+
+				if(tp_end < tp_begin)
+					System.err.println("Time error - Begin: "+tp_begin+" > End: "+tp_end+" -- diff: "+diff);
+
+				cumulative_diff += diff/operations_in_this_loop;
+				cumulative_ops += 1000000000L*operations_in_this_loop/diff;
+
+				testoutput_all.println(iteration+","+KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+diff/operations_in_this_loop+","+(1000000000L*operations_in_this_loop/diff));
+
+			}
+
+			testoutput_stats.println(KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+cumulative_diff/NUM_RUNS+","+(cumulative_ops/NUM_RUNS));
+			testoutput_stats.flush();
+
+
+			testoutput_all.close();
+
+
+
+		}
+
+		System.out.println(keyVal+" non-updating retrieve operations completed!");;
+
+		clean(pimd, keyVal, KeySize);
+
+
+	}
+
+
+	/****************************************************************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 ****************************    REMOVE TEST   ******************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 **************************************************************************************** 
+	 */
+
+	private static void _runTestRemoveScalability(PIMD pimd, int numOperations) throws IOException {
+		// TODO: Check Flag for Inserts (no update)		
+
+		testoutput_stats = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_STATS+File.separator+"remove_stats.csv")));
+		testoutput_stats.println("keySize, valueSize, operations, errors, mean_time(ns), mean_ops/s");
+
+
+		for(int KeySize=4; KeySize<=MAX_KEY_SIZE; KeySize*=GROWING_SCALE) {
+			for(int ValueSize=4; ValueSize<=MAX_VALUE_SIZE; ValueSize*=GROWING_SCALE) {
+
+				_runTestRemoveSingle(pimd, numOperations, GROWING_SCALE, TEST_STEPS, KeySize, ValueSize);
+			}
+			testoutput_stats.println();
+		}
+		testoutput_stats.close();	
+	}
+
+
+
+
+
+
+	private static void _runTestRemoveSingle(PIMD pimd, int numOperations, int scale, int steps, int KeySize, int ValueSize) throws IOException {
+
+
+
+		int errors = 0;
+		int keyVal = 0;
+		long diff = 0;
+		int current_step = 0;
+
+
+		ByteBuffer Value = ByteBuffer.allocateDirect(ValueSize);
+
+
+		System.out.println("Remove Test. KeySize: "+KeySize+", ValueSize: "+ValueSize+", Test to be repeated "+NUM_RUNS+" times.... ");
+
+
+
+		for (int current_scale=1; current_step++<steps; current_scale*=scale){
+
+			int operations_in_this_loop = numOperations * current_scale;
+
+
+
+			PrintWriter testoutput_all = new PrintWriter(new BufferedWriter(new FileWriter(FOLDER_NAME_RAWDATA+File.separator+"remove_"+KeySize+"_"+ValueSize+"_all.csv")));
+
+			testoutput_all.println("iteration, keySize, valueSize, operations, errors, time(ns), ops/s");
+
+
+
+			long cumulative_diff = 0;
+			long cumulative_ops = 0;
+
+
+			for(int iteration=0;iteration<NUM_RUNS;iteration++) {
+
+
+				errors = 0;
+
+
+
+				int last_opertation_in_this_loop = keyVal + operations_in_this_loop;
+
+
+				populate(pimd, keyVal, operations_in_this_loop, KeySize, ValueSize);
+
+
+
+				//TIME IN
+				long tp_begin = System.nanoTime();
+
+
+				int keyFoo = keyVal;
+				ByteBuffer keys[] = new ByteBuffer[last_opertation_in_this_loop - keyFoo];
+				for(;keyFoo<last_opertation_in_this_loop;keyFoo++) {
+					keys[last_opertation_in_this_loop - keyFoo-1] = ByteBuffer.allocateDirect(KeySize);
+					keys[last_opertation_in_this_loop - keyFoo-1].putInt(0,keyFoo);
+				}
+
+				for(;keyVal<last_opertation_in_this_loop;keyVal++) {
+
+					// REMOVE
+					pimd.Remove(keys[last_opertation_in_this_loop - keyVal-1]);
+
+
+				}
+
+				// TIME_OUT
+				long tp_end = System.nanoTime();				
+
+
+				diff = tp_end - tp_begin;				
+
+				if(tp_end < tp_begin)
+					System.err.println("Time error - Begin: "+tp_begin+" > End: "+tp_end+" -- diff: "+diff);
+
+				cumulative_diff += diff/operations_in_this_loop;
+				cumulative_ops += 1000000000L*operations_in_this_loop/diff;
+
+				testoutput_all.println(iteration+","+KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+diff/operations_in_this_loop+","+(1000000000L*operations_in_this_loop/diff));
+
+			}
+
+			testoutput_stats.println(KeySize+","+ValueSize+","+operations_in_this_loop+","+errors+","+cumulative_diff/NUM_RUNS+","+(cumulative_ops/NUM_RUNS));
+			testoutput_stats.flush();
+
+
+			testoutput_all.close();
+
+
+
+		}
+
+		System.out.println(keyVal+" non-updating retrieve operations completed!");;
+
+		clean(pimd, keyVal, KeySize);
+
+
+	}
+
+	/****************************************************************************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 ****************************    POPULATE AND CLEAN   ***********************************
+	 ****************************************************************************************
+	 ****************************************************************************************
+	 **************************************************************************************** 
+	 */
+
+	private static void clean(PIMD pimd, int numOperations, int KeySize){
+
+		System.out.print("Cleaning "+numOperations+" keys... ");
+		int keyVal=0;
+
+
+		ByteBuffer Key = ByteBuffer.allocateDirect(KeySize);
+
+		for(keyVal = 0; keyVal < numOperations; keyVal++) {
+
+			Key.putInt(0,keyVal);
+			pimd.Remove(Key);
+
+		}
+
+
+		System.out.println("Done");
+
+	}	
+
+
+	private static void populate(PIMD pimd, int initialKey, int numOperations, int KeySize, int ValueSize){
+
+		System.out.print("Cleaning "+numOperations+" keys and repopulating... ");
+		int keyVal=0;
+
+
+		ByteBuffer Key = ByteBuffer.allocateDirect(KeySize);
+		ByteBuffer Value = ByteBuffer.allocateDirect(ValueSize);
+
+		for(keyVal = initialKey; keyVal < (initialKey+numOperations); keyVal++) {
+
+			Key.putInt(0,keyVal);
+			pimd.Remove(Key);
+			pimd.Insert(Key, Value);
+
+		}
+
+
+		System.out.println("Done");
+
+	}	
+
+}
+
+
diff --git a/pimd/clean_stats.sh b/pimd/clean_stats.sh
new file mode 100755
index 0000000..e3bd3db
--- /dev/null
+++ b/pimd/clean_stats.sh
@@ -0,0 +1 @@
+rm stats/* rawdata/* bench/stats/* bench/rawdata/*
diff --git a/pimd/lib/java-getopt-1.0.13.jar b/pimd/lib/java-getopt-1.0.13.jar
new file mode 100644
index 0000000..d108633
Binary files /dev/null and b/pimd/lib/java-getopt-1.0.13.jar differ
diff --git a/pimd/lib/junit-4.10.jar b/pimd/lib/junit-4.10.jar
new file mode 100644
index 0000000..bf5c0b9
Binary files /dev/null and b/pimd/lib/junit-4.10.jar differ
diff --git a/pimd/lib/log4j-1.2.16.jar b/pimd/lib/log4j-1.2.16.jar
new file mode 100644
index 0000000..3f9d847
Binary files /dev/null and b/pimd/lib/log4j-1.2.16.jar differ
diff --git a/pimd/lib/memcached-2.6rc1.jar b/pimd/lib/memcached-2.6rc1.jar
new file mode 100644
index 0000000..220790d
Binary files /dev/null and b/pimd/lib/memcached-2.6rc1.jar differ
diff --git a/pimd/pimdKV/Makefile b/pimd/pimdKV/Makefile
index c45d7f7..cd23abf 100644
--- a/pimd/pimdKV/Makefile
+++ b/pimd/pimdKV/Makefile
@@ -3,7 +3,7 @@ all:	pimdKV_jar
 
 pimdKV:
 #   $(JAVAC) -cp $(JNI_JAVA_SRC):$(LOG4J_JAR):$(PIMD_WRAPPER_JAR) com/ibm/pimdKV/*.java
-	$(JAVAC) -cp $(JNI_JAVA_SRC):$(LOG4J_JAR):$(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(UNIT_TEST) com/ibm/pimdKV/*.java
+	$(JAVAC) -cp $(JNI_JAVA_SRC):$(LOG4J_JAR):$(PIMD_WRAPPER_JAR):$(PIMDFS_JAR):$(PIMD_UTILS_JAR):$(UNIT_TEST) com/ibm/pimdKV/*.java
 
 pimdKV_jar: pimdKV
 #   $(JAR) cvf $(PIMDKV_JAR) com/ibm/pimdKV/*.class
diff --git a/pimd/pimdKV/com/ibm/pimdKV/PIMDKV.java b/pimd/pimdKV/com/ibm/pimdKV/PIMDKV.java
index 46bdae0..224b11f 100644
--- a/pimd/pimdKV/com/ibm/pimdKV/PIMDKV.java
+++ b/pimd/pimdKV/com/ibm/pimdKV/PIMDKV.java
@@ -2,13 +2,16 @@ package com.ibm.pimdKV;
 
 import java.io.*;
 import java.lang.*;
+import java.nio.ByteBuffer;
 import java.util.ArrayList;
 import java.util.StringTokenizer;
 import java.util.Arrays;
 import java.util.Iterator;
 
+import com.ibm.pimdutils.TimeCounter;
 import org.apache.log4j.Logger;
 
+import com.ibm.pimd.IteratorTuple;
 import com.ibm.pimd.PIMD;
 
 public class PIMDKV {
@@ -22,6 +25,12 @@ public class PIMDKV {
 	public static final int ENAMETOOLONG = 36; /* Key too long */
 	public static final int EINVAL = 22; /* Invalid argument */
 
+	private boolean writeKey;
+	private ByteBuffer bbWorkingKey;
+	private ByteBuffer bbWorkingValue;
+	private IteratorTuple tuple = null;
+	private Iterator<?> iter = null;
+	
 	private String workingKey;
 	private ArrayValues workingArrayValues;
 
@@ -29,25 +38,20 @@ public class PIMDKV {
 
 	public PIMDKV(String metaServerHost, String PDS)  throws IOException{
 
+//    pimdClient = new PIMD(true);
 		pimdClient = new PIMD();
 
 		try {
 
 			pimdClient.Connect(metaServerHost, PDS);
-				logger.debug("pimdClient = " + pimdClient);
+//			logger.debug("pimdClient = " + pimdClient);
+			
 			if (pimdClient == null) {
-				logger.debug("XXXXXXXXXXXXXXX");
+//				logger.debug("XXXXXXXXXXXXXXX");
 				throw new IOException("Unable to initialize PIMD Client");
 			}
-
-//      if (!exists(KEYS)) {
-				logger.debug("Retrieving KEYS");
-			if ((ArrayKeys)pimdClient.Retrieve(KEYS) == null) {
-				logger.debug("Inserting KEYS");
-				pimdClient.Insert(KEYS, new ArrayKeys());
-			} else {
-				logger.debug("NO Inserting KEYS");
-			}
+			
+			this.writeKey = true;
 
 		} catch (Exception e) {
 			throw new IOException("Couldn't create a connection: \nIOException " + e.getMessage());
@@ -57,10 +61,17 @@ public class PIMDKV {
 	public void shutdown() throws IOException {
 	}
 
+	public void flush() throws IOException {
+		// DC - ASYNC
+		
+				 pimdClient.Wait();
+		
+	}
+
 	public int remove(String Key) throws IOException {
-		logger.debug("-----------------");
-		logger.debug("remove - Key = " + Key);
-		logger.debug("-----------------");
+//		logger.debug("-----------------");
+//		logger.debug("remove - Key = " + Key);
+//		logger.debug("-----------------");
 
 		//check the path
 		checkPath(Key);
@@ -72,9 +83,9 @@ public class PIMDKV {
 
 			pimdClient.Remove(Key);
 
-				logger.debug("-----------------");
-				logger.debug("End - remove - Key = " + Key);
-				logger.debug("-----------------");
+//				logger.debug("-----------------");
+//				logger.debug("End - remove - Key = " + Key);
+//				logger.debug("-----------------");
 			return 0;
 
 		} catch (Exception e) {
@@ -82,83 +93,38 @@ public class PIMDKV {
 					"Error removing Key - REMOVE");
 		}
 	}
-
-	public int writeK(byte[] k, int start, int len) throws IOException {
-
-		try{
-
-			workingKey = new String(k);
-
-				logger.debug("*****Inserting Key*****");
-			if (!exists(workingKey)) {
-					logger.debug("Key = " + workingKey);
-				//Insert the working Key and its array of values
-				pimdClient.Insert(workingKey, new ArrayValues());
-
-				//Insert the working Key in global array Keys
-				ArrayKeys arrayKeys = (ArrayKeys)pimdClient.Retrieve(KEYS);
-				if (arrayKeys == null) {
-					throw new IllegalStateException("No ArrayKeys!!!!");
-				}
-					
-					if (logger.isDebugEnabled()) PrintArrayKeys(arrayKeys);
-				arrayKeys.add(workingKey);
-					if (logger.isDebugEnabled()) PrintArrayKeys(arrayKeys);
-
-				pimdClient.Insert(KEYS, arrayKeys);
+	
+	public int write(byte[] b, int start, int len) throws IOException {
+		
+		try {
+		
+			if (writeKey) {
+				bbWorkingKey = ByteBuffer.wrap(b, start, len);
+				bbWorkingKey.position(bbWorkingKey.limit());
+			} else {
+				ByteBuffer bbValue = ByteBuffer.wrap(b, start, len);
+				bbValue.position(bbValue.limit());
 				
-				if (logger.isDebugEnabled()) {
-					logger.debug("Inserted and Retrieving.....");
-					ArrayKeys afoo = (ArrayKeys)pimdClient.Retrieve(KEYS);
-					PrintArrayKeys(afoo);
-				}
+					long startT = System.currentTimeMillis(); //[Time_Calc]
+				pimdClient.Append(bbWorkingKey, bbValue, len);
 				
-				logger.debug("*****Key Inserted*****");
-
-				return len;
+					long stopT = System.currentTimeMillis(); //[Time_Calc]
+					TimeCounter.All_PIMDKV_Appends += (stopT - startT); //[Time_Calc]
 			}
-
-			return 0;
-
-		}catch (Exception e) {
-			throw new IllegalStateException("Error writing working Key/Value: WRITE");
-		}
-	}
-
-	public int writeV(byte[] v, int start, int len) throws IOException {
-
-		try{
-
-			ArrayValues arrayValues = LookupKey(workingKey);
-			if (logger.isDebugEnabled()) PrintArrayValues(arrayValues);
-			if (arrayValues == null) throw new IllegalStateException("No values for a Key!!!");
-
-//      arrayValues.arrayValues.add(String.valueOf(v));
-//      arrayValues.add(String.valueOf(v));
-			arrayValues.add(v);
-
-			if (logger.isDebugEnabled()) PrintArrayValues(arrayValues);
-
-			pimdClient.Insert(workingKey, arrayValues);
-
-			if (logger.isDebugEnabled()) {
-				logger.debug("Inserted value - Retrieving value.....");
-				ArrayValues afoo = (ArrayValues)pimdClient.Retrieve(workingKey);
-				PrintArrayValues(afoo);
-			}
-			logger.debug("*****Value Inserted*****");
-
+			
+			writeKey = !writeKey;
+			
 			return len;
-
+		
 		}catch (Exception e) {
-			throw new IllegalStateException("Error writing Key/Value: WRITE");
+			throw new IllegalStateException("Error writing working Key/Value: WRITE");
 		}
 	}
 
 	public String readKey(int pos) throws IOException {
 		try{
  
-				logger.debug("*****Reaking key*****");
+//				logger.debug("*****Reaking key*****");
 			ArrayKeys arrayKeys = (ArrayKeys)pimdClient.Retrieve(KEYS);
 			if (logger.isDebugEnabled()) PrintArrayKeys(arrayKeys);
 
@@ -166,64 +132,91 @@ public class PIMDKV {
 				throw new IllegalStateException("No ArrayKeys!!!!");
 			}
 			workingKey = arrayKeys.get(pos);
-				logger.debug("workingKey = " + workingKey);
-				logger.debug("*****key Readed*****");
+//				logger.debug("workingKey = " + workingKey);
+//				logger.debug("*****key Readed*****");
 			return workingKey;
-			//return arrayKeys.get(pos);
 
 		}catch (Exception e) {
 			throw new IllegalStateException("Error obtaining key");
 		}
 	}
 
-	public int readKey(int pos, byte buf[], int off, int len) throws IOException {
+	public int readKey(byte buf[], int off, int len) throws IOException {
 		try{
 			//Obtaining array of Keys
-				logger.debug("Obtaining the arrayKeys");
-			ArrayKeys arrayKeys = (ArrayKeys)pimdClient.Retrieve(KEYS);
-			if (logger.isDebugEnabled()) PrintArrayKeys(arrayKeys);
-
-			if (arrayKeys == null) {
-				throw new IllegalStateException("No ArrayKeys!!!!");
-			}
-
-			logger.debug("Reading pos = " + pos);
-			String keyReaded = arrayKeys.get(pos);
-			logger.debug("keyReaded = " + keyReaded);
-
-			len = Math.min(len, keyReaded.length() - off);
-//      logger.debug("len = " + len + " // keyReaded.length() = " + keyReaded.length() + " // off = " + off);
-//      logger.debug("keyReaded.getBytes() = " + keyReaded.getBytes() + " // buf.length= " + buf.length + " // ");
-//      if (logger.isDebugEnabled()) {
-//        printBytes(buf);
-//        System.out.println();
-//      }
-			System.arraycopy(keyReaded.getBytes(), off, buf, 0, len);
-
-			workingKey = keyReaded;
-
-			return keyReaded.length();
+//				logger.debug("Obtaining the arrayKeys");
+			
+//			logger.debug("bbWorkingKey.get(0) = " + bbWorkingKey.get(0));
+//			logger.debug("bbWorkingKey = " + bbWorkingKey);
+			bbWorkingKey.get(buf, 0, bbWorkingKey.limit());
+//			logger.debug("buf[0] = " + buf[0]);
+			
+			return buf.length;
 
 		}catch (Exception e) {
 			throw new IllegalStateException("Error reading key. Message: " + e.getMessage());
 		}
 	}
 
-	public int readValue(int pos, byte buf[], int off, int len) throws IOException {
+	public int readValue(byte buf[], int off, int len) throws IOException {
 		try{
 			// Checks
 			if (buf == null) throw new NullPointerException("Null buffer in read routine");
 			if ( off < 0 || len < 0 ) //|| len > (buf.off- start) ) 
 				throw new IndexOutOfBoundsException();
-
-			len = Math.min(len, workingArrayValues.get(pos).length - off);
-			System.arraycopy(workingArrayValues.get(pos), off, buf, 0, len);
+			
+//			logger.debug("bbWorkingValue.get(3) = " + bbWorkingValue.get(3));
+//			logger.debug("bbWorkingValue = " + bbWorkingValue);
+			bbWorkingValue.get(buf, 0, bbWorkingValue.limit());
+//			logger.debug("buf[3] = " + buf[3]);
 
 			return len; 
 		}catch (Exception e) {
 			throw new IllegalStateException("Error obtaining key size");
 		}
 	}
+	
+	public int seekKey()throws IOException {
+		try{
+				
+				tuple = pimdClient.BBGlobalIteratorNext();
+				
+				if(tuple != null) {
+					TimeCounter.Num_new_keys++; //[Time_Calc]
+					bbWorkingKey = tuple.getbbKey();
+//					logger.debug("bbWorkingKey = " + bbWorkingKey);
+					
+					iter = tuple.getValues();
+//					logger.debug("one more item, key: " + bbWorkingKey.toString());
+					
+					return bbWorkingKey.limit();
+					
+				} else {
+					
+					return -1;
+				}
+//			}
+			
+//			return bbWorkingKey.limit();
+		}catch (Exception e){
+			throw new IllegalStateException("Error seeking key");
+		}
+	}
+	
+	public int valueSize() throws IOException {
+		try{
+			if (iter.hasNext()) {
+				bbWorkingValue = (ByteBuffer)iter.next();
+//				logger.debug("bbWorkingValue = " + bbWorkingValue);
+				
+				return bbWorkingValue.limit();
+			}
+			
+			return 0;
+		}catch (Exception e) {
+			throw new IllegalStateException("Error obtaining value size");
+		}
+	}
 
 	public int seekKey(int pos) throws IOException {
 		try{
@@ -238,16 +231,13 @@ public class PIMDKV {
 
 			//Set the working Key
 			workingKey = arrayKeys.get(pos);
-				logger.debug("workingKey = " + workingKey);
+//				logger.debug("workingKey = " + workingKey);
 			if (workingKey == null) {
 				return -1;
 			}
 
 			//Set the working array of values for working Key
 			workingArrayValues = (ArrayValues)pimdClient.Retrieve(workingKey);
-			/*if (workingArrayValues == null) {
-				throw new IllegalStateException("No ArrayValues, at least one!!!!");
-			}*/
 
 			return arrayKeys.keyLength(pos);
 
@@ -270,34 +260,22 @@ public class PIMDKV {
 		}
 	}
 
-	public int numKeys() throws IOException {
-		//Obtain Array of Keys
-		ArrayKeys arrayKeys = (ArrayKeys)pimdClient.Retrieve(KEYS);
-		if (arrayKeys == null) {
-			throw new IllegalStateException("No ArrayKeys!!!!");
-		}
-		return arrayKeys.size();
-	}
-
-	public int numValues() throws IOException {
-		if (workingArrayValues == null) return -1;
-
-		return workingArrayValues.size();
-		/*ArrayValues arrayValues = (ArrayValues)memClient.get(workingKey);
-		if (arrayValues == null) {
-			throw new IllegalStateException("No ArrayValues, at least one!!!!");
-		}
-		return arrayValues.size();*/
+	public boolean hasNextValue() throws IOException {
+		
+		if (iter != null)
+			return iter.hasNext();
+		
+		return false;
 	}
 
 	public boolean exists(String Key) throws IOException {
 
-			logger.debug("*****Exists******");
+//			logger.debug("*****Exists******");
 
 		ArrayValues res = LookupKey(Key);
 
-			logger.debug("res = " + res);
-			logger.debug("*****End Exists******");
+//			logger.debug("res = " + res);
+//			logger.debug("*****End Exists******");
 
 		return (res != null);
 	}
@@ -306,7 +284,7 @@ public class PIMDKV {
 	private ArrayValues LookupKey(String Key) throws IOException {
 
 		try {
-				logger.debug("LookupKey Key = " + Key);
+//				logger.debug("LookupKey Key = " + Key);
 
 			ArrayValues arrayValues = (ArrayValues)pimdClient.Retrieve(Key);
 
@@ -336,7 +314,7 @@ public class PIMDKV {
 
 	public void PrintKeys() throws IOException {
 
-		logger.debug("*****PrintKeys*****");
+//		logger.debug("*****PrintKeys*****");
 
 		try {
 			ArrayKeys arrayKeys =(ArrayKeys)pimdClient.Retrieve(KEYS);
@@ -346,7 +324,7 @@ public class PIMDKV {
 
 			Iterator itr = arrayKeys.arrayKeys.iterator();
 			while (itr.hasNext()) {
-				logger.debug(itr.next());
+//				logger.debug(itr.next());
 			}
 
 		}catch (Exception e) {
diff --git a/pimd/pimdKV/com/ibm/pimdKV/PIMDKVTest.java b/pimd/pimdKV/com/ibm/pimdKV/PIMDKVTest.java
index c7badd3..2184ab9 100644
--- a/pimd/pimdKV/com/ibm/pimdKV/PIMDKVTest.java
+++ b/pimd/pimdKV/com/ibm/pimdKV/PIMDKVTest.java
@@ -31,14 +31,16 @@ public class PIMDKVTest {
 			PIMDKV pimdkv = new PIMDKV("10.0.0.1", "MyPDS");
 
 			System.out.println("\nInsertando key: " + strKey);
-			bytesWrited = pimdkv.writeK(key, 0, key.length);
+//      bytesWrited = pimdkv.writeK(key, 0, key.length);
+      bytesWrited = pimdkv.write(key, 0, key.length);
 
 			System.out.print("\nInsertando value: "); printBytes(value);
-			bytesWrited = pimdkv.writeV(value, 0, value.length);
+//      bytesWrited = pimdkv.writeV(value, 0, value.length);
+			bytesWrited = pimdkv.write(value, 0, value.length);
 
 			System.out.println("\nReading Key: ");
 			buf = new byte[key.length];
-			bytesReaded = pimdkv.readKey(0, buf, 0, key.length);
+			bytesReaded = pimdkv.readKey(buf, 0, key.length);
 			assertArrayEquals(strKey.getBytes(), buf);
 //      printBytes(buf);
 
@@ -66,7 +68,7 @@ public class PIMDKVTest {
 
 			System.out.println("\nReading Key inserted in Test01: ");
 			buf = new byte[key.length];
-			bytesReaded = pimdkv2.readKey(0, buf, 0, key.length);
+			bytesReaded = pimdkv2.readKey(buf, 0, key.length);
 			assertArrayEquals(strKey.getBytes(), buf);
 
 		} catch (Exception e) {
diff --git a/pimd/pimd_utils/Makefile b/pimd/pimd_utils/Makefile
new file mode 100644
index 0000000..d45dad7
--- /dev/null
+++ b/pimd/pimd_utils/Makefile
@@ -0,0 +1,17 @@
+# echo:
+#   @echo "asldkjfaskdjf"
+
+all: pimd_utils_jar
+
+pimd_utils:
+	@echo "pimd_utils"
+	mkdir -p bin
+	$(JAVAC) -cp $(LOG4J_JAR):$(CONF_FOLDER)$(UNIT_TEST) -d bin src/com/ibm/pimdutils/*.java
+
+pimd_utils_jar: pimd_utils
+	@echo "pimd_utils_jar"
+	$(JAR) cvf $(PIMD_UTILS_JAR) -C bin .
+
+clean:
+	rm -rf bin
+	rm -f $(PIMD_UTILS_JAR)
diff --git a/pimd/pimd_utils/src/com/ibm/pimdutils/TimeCounter.java b/pimd/pimd_utils/src/com/ibm/pimdutils/TimeCounter.java
new file mode 100644
index 0000000..f0e1baa
--- /dev/null
+++ b/pimd/pimd_utils/src/com/ibm/pimdutils/TimeCounter.java
@@ -0,0 +1,29 @@
+package com.ibm.pimdutils;
+
+import org.apache.log4j.Logger;
+
+
+public class TimeCounter
+{
+
+  public static long All_PIMDKV_Appends = 0;
+  public static long Append_op_timer = 0;
+  public static long PIMD_Append_allocate_key_value = 0;
+	public static long PIMDWrapper_Append = 0;
+	
+	public static long seekNextKey = 0;
+	public static long seekKey = 0;
+	public static long obtaining_tuples = 0;
+	public static int Num_new_keys = 0;
+	public static long PIMD_Allocate_key_value = 0;
+	public static long PIMD_sets_value = 0;
+	public static long PIMD_iterrator_close = 0;
+	public static long PIMDWrapper_GlobalIteratorNext = 0;
+	
+ 	static Logger logger = Logger.getLogger(TimeCounter.class);
+
+	public TimeCounter() {
+
+	}
+
+}
diff --git a/pimd/pimd_wrapper/jni_c/include/com_ibm_pimd_PIMDWrapper.h b/pimd/pimd_wrapper/jni_c/include/com_ibm_pimd_PIMDWrapper.h
index 2cffc70..db00231 100644
--- a/pimd/pimd_wrapper/jni_c/include/com_ibm_pimd_PIMDWrapper.h
+++ b/pimd/pimd_wrapper/jni_c/include/com_ibm_pimd_PIMDWrapper.h
@@ -17,6 +17,22 @@ JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Init
 
 /*
  * Class:     com_ibm_pimd_PIMDWrapper
+ * Method:    Init_Asynchronous
+ * Signature: ()V
+ */
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Init_1Asynchronous
+  (JNIEnv *, jclass);
+
+/*
+ * Class:     com_ibm_pimd_PIMDWrapper
+ * Method:    Init_Bulk
+ * Signature: ()V
+ */
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Init_1Bulk
+  (JNIEnv *, jclass);
+
+/*
+ * Class:     com_ibm_pimd_PIMDWrapper
  * Method:    Connect
  * Signature: (Ljava/lang/String;)V
  */
@@ -79,6 +95,38 @@ JNIEXPORT jint JNICALL Java_com_ibm_pimd_PIMDWrapper_Retrieve
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Remove
   (JNIEnv *, jclass, jobject, jobject, jint);
 
+/*
+ * Class:     com_ibm_pimd_PIMDWrapper
+ * Method:    Append
+ * Signature: (Lcom/ibm/pimd/PIMDPDSID;Ljava/nio/ByteBuffer;ILjava/nio/ByteBuffer;I)V
+ */
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Append
+  (JNIEnv *, jclass, jobject, jobject, jint, jobject, jint);
+
+/*
+ * Class:     com_ibm_pimd_PIMDWrapper
+ * Method:    GlobalIteratorNext
+ * Signature: (Lcom/ibm/pimd/PIMDPDSID;Ljava/nio/ByteBuffer;Ljava/nio/ByteBuffer;)I
+ */
+JNIEXPORT jint JNICALL Java_com_ibm_pimd_PIMDWrapper_GlobalIteratorNext
+  (JNIEnv *, jclass, jobject, jobject, jobject);
+
+/*
+ * Class:     com_ibm_pimd_PIMDWrapper
+ * Method:    GlobalIteratorClose
+ * Signature: (Lcom/ibm/pimd/PIMDPDSID;)V
+ */
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_GlobalIteratorClose
+  (JNIEnv *, jclass, jobject);
+
+/*
+ * Class:     com_ibm_pimd_PIMDWrapper
+ * Method:    Wait
+ * Signature: ()V
+ */
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Wait
+  (JNIEnv *, jclass);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/pimd/pimd_wrapper/jni_c/pimd_wrapper.cpp b/pimd/pimd_wrapper/jni_c/pimd_wrapper.cpp
index bf9c1c4..7cc82dc 100644
--- a/pimd/pimd_wrapper/jni_c/pimd_wrapper.cpp
+++ b/pimd/pimd_wrapper/jni_c/pimd_wrapper.cpp
@@ -6,129 +6,161 @@
 
 
 pimd_pds_id_t getPIMDPDSID (JNIEnv *, jobject);
+void inc_async_ops(void);
 
 // Signatura: javap -s  X.class
 
+enum ModeType {SYNCHRONOUS, ASYNCHRONOUS, BULK};
+
+unsigned int pending_async_ops = 0;
+
+
+
 pimd_client_t Client;
+pimd_client_cursor_ext_hdl_t current_cursor;
+int current_cursor_open = 0;
+
 char hostname[256];
 const char *ServerAddress;
 const char *PDSName;
+ModeType Mode = SYNCHRONOUS;
 
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Init (JNIEnv *env, jclass jc) {
 
-	gethostname(hostname, sizeof(hostname));
+  gethostname(hostname, sizeof(hostname));
 
 #ifndef LOGGER_INIT_DONE
-	FxLogger_Init("pimdwrapper_jni_c", 0); 
-	printf("Starting FxLogger\n");
+  FxLogger_Init("pimdwrapper_jni_c", 0); 
+  printf("Starting FxLogger. PIMD in mode %d\n",Mode);
 #else
 #define LOGGER_INIT_DONE
 #endif
 
-     	BegLogLine( 1 )
-        << "Wrapper.Init in "<< hostname
-        << EndLogLine;
+  BegLogLine( 1 )
+    << "Wrapper.Init in "<< hostname
+    << EndLogLine;
+
+  BegLogLine( 1 )
+    << "PIMD mode: "<< Mode
+    << EndLogLine;
 
-	//printf("Wrapper.Init in %s\n", hostname);
- 	pimd_status_t status = Client.Init(0, 0);
+  //printf("Wrapper.Init in %s\n", hostname);
+  pimd_status_t status = Client.Init(0, 0);
 
 }
 
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Init_1Bulk (JNIEnv *env, jclass jc) {
+
+  //Mode = BULK;
+  //Java_com_ibm_pimd_PIMDWrapper_Init(env,jc);
+  BegLogLine( 1 )
+    << "Bulk Mode not yet implemented. Defaulting to SYNCHRONOUS mode. "<< Mode
+    << EndLogLine;
+
+}
+
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Init_1Asynchronous (JNIEnv *env, jclass jc) {
+
+  Mode = ASYNCHRONOUS;
+  Java_com_ibm_pimd_PIMDWrapper_Init(env,jc);
+
+}
+
+
+
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Connect (JNIEnv *env, jclass jc, jstring name){
 
-	jboolean iscopy;
+  jboolean iscopy;
+
+  ServerAddress = env->GetStringUTFChars(name, &iscopy);
+  //  printf("Wrapper.Connect in %s. Connecting to %s\n", hostname,ServerAddress);
+  BegLogLine( 1 )
+    << "Wrapper.Connect in "<< hostname
+    << ". Connecting to " << ServerAddress
+    << EndLogLine;
 
-	ServerAddress = env->GetStringUTFChars(name, &iscopy);
-//	printf("Wrapper.Connect in %s. Connecting to %s\n", hostname,ServerAddress);
-     	BegLogLine( 1 )
-        << "Wrapper.Connect in "<< hostname
-	<< ". Connecting to " << ServerAddress
-        << EndLogLine;
+  pimd_status_t status = Client.Connect( ServerAddress, 0 );
 
-	pimd_status_t status = Client.Connect( ServerAddress, 0 );
-	
-//printf("Wrapper.Connect in %s. Connected to %s\n", hostname,ServerAddress);
-     	BegLogLine( 1 )
-        << "Wrapper.Connect in "<< hostname
-	<< ". Connected to " << ServerAddress
-        << EndLogLine;
+  //printf("Wrapper.Connect in %s. Connected to %s\n", hostname,ServerAddress);
+  BegLogLine( 1 )
+    << "Wrapper.Connect in "<< hostname
+    << ". Connected to " << ServerAddress
+    << EndLogLine;
 
 
 }
 
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Disconnect(JNIEnv *env, jclass jc) {
 
-	pimd_status_t status = Client.Disconnect();
+  pimd_status_t status = Client.Disconnect();
 }
 
 JNIEXPORT jobject JNICALL Java_com_ibm_pimd_PIMDWrapper_Open
-  (JNIEnv * env, jclass jc, jstring _PDSName, jint privs, jint flags) {
-
-	pimd_pds_id_t	_PDSId;
-	jboolean iscopy;
-	const char *PDSName = env->GetStringUTFChars(_PDSName, &iscopy);
-	pimd_status_t status = Client.Open( 	(char *)PDSName,
-						(pimd_pds_priv_t)(PIMD_PDS_READ|PIMD_PDS_WRITE),
-						(pimd_cmd_open_flags_t)PIMD_COMMAND_OPEN_FLAGS_CREATE,
-						&_PDSId);
-
-	jclass cls = env->FindClass("com/ibm/pimd/PIMDPDSID");
-	if(cls==NULL) {
-		perror("FindClass");
-		exit(1);
-	}
-	jmethodID methodID = env->GetMethodID(cls, "<init>","(II)V");
-	if(methodID==NULL) {
-		perror("GetMethodID");
-		exit(1);
-	}
-	jobject PDSId = env->NewObject(cls, methodID, _PDSId.mOwnerNodeId, _PDSId.mIdOnOwner);
-	
-	//printf("Wrapper. Open in %s. Opening PDS %s, using handler <%d,%d>\n", hostname, PDSName, _PDSId.mOwnerNodeId, _PDSId.mIdOnOwner);
-     	BegLogLine( 1 )
-        << "Wrapper.Open in "<< hostname
-	<< ". Opening " << PDSName
-	<< ", using hadler <" << _PDSId.mOwnerNodeId << "," <<  _PDSId.mIdOnOwner << ">"
-        << EndLogLine;
-	
-	return PDSId;
+(JNIEnv * env, jclass jc, jstring _PDSName, jint privs, jint flags) {
+
+  pimd_pds_id_t _PDSId;
+  jboolean iscopy;
+  const char *PDSName = env->GetStringUTFChars(_PDSName, &iscopy);
+  pimd_status_t status = Client.Open(   (char *)PDSName,
+      (pimd_pds_priv_t)(PIMD_PDS_READ|PIMD_PDS_WRITE),
+      (pimd_cmd_open_flags_t)PIMD_COMMAND_OPEN_FLAGS_CREATE,
+      &_PDSId);
+
+  jclass cls = env->FindClass("com/ibm/pimd/PIMDPDSID");
+  if(cls==NULL) {
+    perror("FindClass");
+    exit(1);
+  }
+  jmethodID methodID = env->GetMethodID(cls, "<init>","(II)V");
+  if(methodID==NULL) {
+    perror("GetMethodID");
+    exit(1);
+  }
+  jobject PDSId = env->NewObject(cls, methodID, _PDSId.mOwnerNodeId, _PDSId.mIdOnOwner);
+
+  //printf("Wrapper. Open in %s. Opening PDS %s, using handler <%d,%d>\n", hostname, PDSName, _PDSId.mOwnerNodeId, _PDSId.mIdOnOwner);
+  BegLogLine( 1 )
+    << "Wrapper.Open in "<< hostname
+    << ". Opening " << PDSName
+    << ", using hadler <" << _PDSId.mOwnerNodeId << "," <<  _PDSId.mIdOnOwner << ">"
+    << EndLogLine;
+
+  return PDSId;
 
 }
 
 
 pimd_pds_id_t getPIMDPDSID (JNIEnv *env, jobject PDSID) {
 
-	pimd_pds_id_t PDSId;
+  pimd_pds_id_t PDSId;
 
-	jclass cls = env->FindClass("com/ibm/pimd/PIMDPDSID");
-        if(cls==NULL) {
-                perror("FindClass");
-                exit(1);
-        }
+  jclass cls = env->FindClass("com/ibm/pimd/PIMDPDSID");
+  if(cls==NULL) {
+    perror("FindClass");
+    exit(1);
+  }
 
-	
-        jmethodID methodID1 = env->GetMethodID(cls, "getmOwnerNodeId","()I");
-        if(methodID1==NULL) {
-                perror("GetMethodID");
-                exit(1);
-        }
-	
+  jmethodID methodID1 = env->GetMethodID(cls, "getmOwnerNodeId","()I");
+  if(methodID1==NULL) {
+    perror("GetMethodID");
+    exit(1);
+  }
 
-        jmethodID methodID2 = env->GetMethodID(cls, "getmIdOnOwner","()I");
-        if(methodID2==NULL) {
-                perror("GetMethodID");
-                exit(1);
-        }
+  jmethodID methodID2 = env->GetMethodID(cls, "getmIdOnOwner","()I");
+  if(methodID2==NULL) {
+    perror("GetMethodID");
+    exit(1);
+  }
 
 
-	jint mOwnerNodeId = env->CallIntMethod(PDSID, methodID1);
-	jint mIdOnOwner = env->CallIntMethod(PDSID, methodID2);
+  jint mOwnerNodeId = env->CallIntMethod(PDSID, methodID1);
+  jint mIdOnOwner = env->CallIntMethod(PDSID, methodID2);
 
 
-	PDSId.mOwnerNodeId = mOwnerNodeId;
-	PDSId.mIdOnOwner = mIdOnOwner;
+  PDSId.mOwnerNodeId = mOwnerNodeId;
+  PDSId.mIdOnOwner = mIdOnOwner;
 
-	return PDSId;
+  return PDSId;
 
 }
 
@@ -136,173 +168,193 @@ pimd_pds_id_t getPIMDPDSID (JNIEnv *env, jobject PDSID) {
 
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Close (JNIEnv *env, jclass jc, jobject PDSID) {
 
-	
-	pimd_pds_id_t PDSId = getPIMDPDSID(env, PDSID);
 
-	//printf("Wrapper. Close PDS. mOwnerNodeId: %d, mIdOnOwner:%d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner);
-     	BegLogLine( 1 )
-        << "Wrapper.Close PDS. "
-	<< "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
-        << EndLogLine;
-	
+  pimd_pds_id_t PDSId = getPIMDPDSID(env, PDSID);
 
-	pimd_status_t status = Client.Close(&PDSId);
+  //printf("Wrapper. Close PDS. mOwnerNodeId: %d, mIdOnOwner:%d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner);
+  BegLogLine( 1 )
+    << "Wrapper.Close PDS. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << EndLogLine;
 
-	//printf("Wrapper. Insert. Status: %s\n",pimd_status_to_string(status));
-     	BegLogLine( 1 )
-        << "Wrapper.Inser. Status: " << pimd_status_to_string(status) 
-        << EndLogLine;
 
-} 
+  pimd_status_t status = Client.Close(&PDSId);
+
+  //printf("Wrapper. Insert. Status: %s\n",pimd_status_to_string(status));
+  BegLogLine( 1 )
+    << "Wrapper.Inser. Status: " << pimd_status_to_string(status) 
+    << EndLogLine;
 
+} 
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Insert (JNIEnv *env, jclass jc, jobject PDSID, jobject key, jint keySize, jobject value, jint valueSize) {
 
-	pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
-	
-	char *Key    = (char*)env->GetDirectBufferAddress(key);
-	jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
-	char *Value    = (char*)env->GetDirectBufferAddress(value);
-	jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
-
-	if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
-		return;
-
-	StrongAssertLogLine ( keySize > 0 && valueSize > 0)
-    	<< "keySize or valueSize = 0, Insert failed in Wrapper"
-    	<< EndLogLine;
-	
-	if(keySize == 0 || valueSize == 0) {
-		printf("keySize or valueSize = 0, Insert failed in Wrapper\n");
-		return;
-	}
+  pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+
+  char *Key    = (char*)env->GetDirectBufferAddress(key);
+  jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
+  char *Value    = (char*)env->GetDirectBufferAddress(value);
+  jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
+
+  if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
+    return;
+
+  StrongAssertLogLine ( keySize > 0 && valueSize > 0)
+    << "keySize or valueSize = 0, Insert failed in Wrapper"
+    << EndLogLine;
+
+  if(keySize == 0 || valueSize == 0) {
+    printf("keySize or valueSize = 0, Insert failed in Wrapper\n");
+    return;
+  }
+
+
+  //printf("Wrapper. Insert. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
+  BegLogLine( 1 )
+    << "Wrapper.Insert. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << " - keySize: " << keySize << ", valueSize " << valueSize << ", Asynchronous Mode = " << (Mode == ASYNCHRONOUS)
+    << EndLogLine;
 
+  pimd_status_t status;
 
-	//printf("Wrapper. Insert. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
-     	BegLogLine( 1 )
-        << "Wrapper.Insert. "
-	<< "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
-	<< " - keySize: " << keySize << ", valueSize " << valueSize 
-        << EndLogLine;
-
-	pimd_status_t status = Client.Insert(	&PDSId,
-						Key,
-						keySize,
-						Value,
-						valueSize,
-						0,
-						(pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
-//            (pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_FLAGS_NONE);
-	
-	//printf("Wrapper. Insert. Status: %s\n",pimd_status_to_string(status));
-     	BegLogLine( 1 )
-	<< "Wrapper. Insert. Status: " << pimd_status_to_string(status)
-	<< EndLogLine;
+  if(Mode == SYNCHRONOUS) {
+
+    status = Client.Insert( &PDSId,
+        Key,
+        keySize,
+        Value,
+        valueSize,
+        0,
+        (pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
+
+  } else if(Mode == ASYNCHRONOUS) {
+
+
+    pimd_client_cmd_ext_hdl_t commandHandle;
+    status = Client.iInsert(  &PDSId,
+        Key,
+        keySize,
+        Value,
+        valueSize,
+        0,
+        (pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE,
+        &commandHandle);
+
+    inc_async_ops();
+
+  }
+
+
+
+  BegLogLine( 1 )
+    << "Wrapper. Insert. Status: " << pimd_status_to_string(status)
+    << EndLogLine;
 
 }
 
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Update (JNIEnv *env, jclass jc, jobject PDSID, jobject key, jint keySize, jobject value, jint valueSize) {
 
-	pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
-	
-	char *Key    = (char*)env->GetDirectBufferAddress(key);
-	jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
-	char *Value    = (char*)env->GetDirectBufferAddress(value);
-	jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
-
-	if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
-		return;
-
-	StrongAssertLogLine ( keySize > 0 && valueSize > 0)
-    	<< "keySize or valueSize = 0, Insert failed in Wrapper"
-    	<< EndLogLine;
-	
-	if(keySize == 0 || valueSize == 0) {
-		printf("keySize or valueSize = 0, Insert failed in Wrapper\n");
-		return;
-	}
+  pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+
+  char *Key    = (char*)env->GetDirectBufferAddress(key);
+  jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
+  char *Value    = (char*)env->GetDirectBufferAddress(value);
+  jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
+
+  if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
+    return;
+
+  StrongAssertLogLine ( keySize > 0 && valueSize > 0)
+    << "keySize or valueSize = 0, Insert failed in Wrapper"
+    << EndLogLine;
+
+  if(keySize == 0 || valueSize == 0) {
+    printf("keySize or valueSize = 0, Insert failed in Wrapper\n");
+    return;
+  }
 
 
-	//printf("Wrapper. Insert. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
-     	BegLogLine( 1 )
-        << "Wrapper.Update. "
-	<< "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
-	<< " - keySize: " << keySize << ", valueSize " << valueSize 
-        << EndLogLine;
-
-	pimd_status_t status = Client.Insert(	&PDSId,
-						Key,
-						keySize,
-						Value,
-						valueSize,
-						0,
-						(pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
-	
-	//printf("Wrapper. Update. Status: %s\n",pimd_status_to_string(status));
-     	BegLogLine( 1 )
-	<< "Wrapper. Update. Status: " << pimd_status_to_string(status)
-	<< EndLogLine;
+  //printf("Wrapper. Insert. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
+  BegLogLine( 1 )
+    << "Wrapper.Update. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << " - keySize: " << keySize << ", valueSize " << valueSize 
+    << EndLogLine;
+
+  pimd_status_t status = Client.Insert( &PDSId,
+      Key,
+      keySize,
+      Value,
+      valueSize,
+      0,
+      (pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
+
+  //printf("Wrapper. Update. Status: %s\n",pimd_status_to_string(status));
+  BegLogLine( 1 )
+    << "Wrapper. Update. Status: " << pimd_status_to_string(status)
+    << EndLogLine;
 
 }
 
 JNIEXPORT jint JNICALL Java_com_ibm_pimd_PIMDWrapper_Retrieve (JNIEnv *env, jclass jc, jobject PDSID, jobject key, jint keySize, jobject value, jint valueSize) {
 
-	pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+  pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
 
-	//printf("key = %p\n", key);
-	//printf("value = %p\n", value);
-	
-	char *Key    = (char*)env->GetDirectBufferAddress(key);
-	jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
-	char *Value    = (char*)env->GetDirectBufferAddress(value);
-	jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
+  //printf("key = %p\n", key);
+  //printf("value = %p\n", value);
 
-	if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
-		return 0;
+  char *Key    = (char*)env->GetDirectBufferAddress(key);
+  jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
+  char *Value    = (char*)env->GetDirectBufferAddress(value);
+  jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
 
-	StrongAssertLogLine(keySize > 0 && Key != NULL && Value != NULL)
-	<< "keySize = 0, or Key = NULL or VALUE = NULL. Retrieve failed in Wrapper"
-	<< EndLogLine;
+  if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
+    return 0;
 
-	if(keySize == 0) {
-		printf("keySize = 0, Retrieve failed in Wrapper\n");
-		return 0;
-	}
+  StrongAssertLogLine(keySize > 0 && Key != NULL && Value != NULL)
+    << "keySize = 0, or Key = NULL or VALUE = NULL. Retrieve failed in Wrapper"
+    << EndLogLine;
 
-	if(Key==NULL||Value==NULL) {
-		printf("Key: %p, Value: %p\n", Key, Value);
-		exit(0);
-	}
+  if(keySize == 0) {
+    printf("keySize = 0, Retrieve failed in Wrapper\n");
+    return 0;
+  }
 
-	//printf("Wrapper. Retrieve. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
-     	BegLogLine( 1 )
-        << "Wrapper. Retrieve. "
-	<< "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
-	<< " - keySize: " << keySize << ", valueSize " << valueSize 
-        << EndLogLine;
+  if(Key==NULL||Value==NULL) {
+    printf("Key: %p, Value: %p\n", Key, Value);
+    exit(0);
+  }
 
-	int len = 0;
-	pimd_status_t status = Client.Retrieve(	&PDSId,
-						Key,
-						keySize,
-						Value,
-						valueSize,
-						&len,
-						0,
-						(pimd_cmd_RIU_flags_t)0);  //PIMD_COMMAND_RIU_FLAGS_NONE
+  //printf("Wrapper. Retrieve. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
+  BegLogLine( 1 )
+    << "Wrapper. Retrieve. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << " - keySize: " << keySize << ", valueSize " << valueSize 
+    << EndLogLine;
 
+  int len = 0;
+  pimd_status_t status = Client.Retrieve( &PDSId,
+      Key,
+      keySize,
+      Value,
+      valueSize,
+      &len,
+      0,
+      (pimd_cmd_RIU_flags_t)0);  //PIMD_COMMAND_RIU_FLAGS_NONE
 
-	if(PIMD_SUCCESS != status)
-		len = 0;
 
-	//printf("Wrapper. Retrieve. Status: %s - bytes read: %d\n",pimd_status_to_string(status),len);
-     	BegLogLine( 1 )
-	<< "Wrapper. Retrieve. Status: " << pimd_status_to_string(status)
-	<< " - bytes read: " << len
-	<< EndLogLine;
+  if(PIMD_SUCCESS != status)
+    len = 0;
 
-	
+  //printf("Wrapper. Retrieve. Status: %s - bytes read: %d\n",pimd_status_to_string(status),len);
+  BegLogLine( 1 )
+    << "Wrapper. Retrieve. Status: " << pimd_status_to_string(status)
+    << " - bytes read: " << len
+    << EndLogLine;
 
-	return len;
+
+
+  return len;
 
 }
 
@@ -310,40 +362,226 @@ JNIEXPORT jint JNICALL Java_com_ibm_pimd_PIMDWrapper_Retrieve (JNIEnv *env, jcla
 JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Remove (JNIEnv *env, jclass jc, jobject PDSID, jobject key, jint keySize) {
 
 
-	pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+  pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+
+  char *Key    = (char*)env->GetDirectBufferAddress(key);
+  jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
 
-	char *Key    = (char*)env->GetDirectBufferAddress(key);
-	jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
+  if(keySize > KeyMaxLen)
+    return;
+
+  StrongAssertLogLine ( keySize > 0 )
+    << "keySize = 0. Remove failed in Wrapper"
+    << EndLogLine;
+
+  if(keySize == 0) {
+    printf("keySize = 0, Remove failed in Wrapper\n");
+    return;
+  }
+
+  //printf("Wrapper. Remove. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize);
+  BegLogLine( 1 )
+    << "Wrapper. Remove. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << " - keySize: " << keySize
+    << EndLogLine;
+
+  pimd_status_t status = Client.Remove( &PDSId,
+      Key,
+      keySize,
+      (pimd_cmd_remove_flags_t)PIMD_COMMAND_REMOVE_FLAGS_NONE);
+
+  //printf("Wrapper. Remove. Status: %s\n",pimd_status_to_string(status));
+  //  StrongAssertLogLine( status == PIMD_SUCCESS ) << " Status : " << ,pimd_status_to_string(status) << EndLogLine;
+  BegLogLine( 1 )
+    << "Wrapper. Remove. Status: " << pimd_status_to_string(status)
+    << EndLogLine;
+
+}
 
-	if(keySize > KeyMaxLen)
-                return;
 
-	StrongAssertLogLine ( keySize > 0 )
-    	<< "keySize = 0. Remove failed in Wrapper"
-    	<< EndLogLine;
 
-	if(keySize == 0) {
-		printf("keySize = 0, Remove failed in Wrapper\n");
-		return;
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Append (JNIEnv *env, jclass jc, jobject PDSID, jobject key, jint keySize, jobject value, jint valueSize) {
+
+
+  pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+
+  char *Key    = (char*)env->GetDirectBufferAddress(key);
+  jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
+  char *Value    = (char*)env->GetDirectBufferAddress(value);
+  jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
+
+  if(keySize > KeyMaxLen||valueSize > ValueMaxLen)
+    return;
+
+  StrongAssertLogLine ( keySize > 0 && valueSize > 0)
+    << "keySize or valueSize = 0, Append failed in Wrapper"
+    << EndLogLine;
+
+  if(keySize == 0 || valueSize == 0) {
+    printf("keySize or valueSize = 0, Append failed in Wrapper\n");
+    return;
+  }
+
+
+  //printf("Wrapper. Insert. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d, valueSize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize,valueSize);
+  BegLogLine( 1 )
+    << "Wrapper.Append. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << " - keySize: " << keySize << ", valueSize " << valueSize << ", Asynchronous Mode = " << (Mode == ASYNCHRONOUS)
+    << EndLogLine;
+
+  pimd_status_t status;
+
+  if(Mode == SYNCHRONOUS) {
+
+    status = Client.Insert( &PDSId,
+        Key,
+        keySize,
+        Value,
+        valueSize,
+        0,
+        (pimd_cmd_RIU_flags_t)(PIMD_COMMAND_RIU_APPEND|PIMD_COMMAND_RIU_INSERT_USE_RECORD_LOCKS));
+
+  } else if(Mode == ASYNCHRONOUS) {
+
+
+    pimd_client_cmd_ext_hdl_t commandHandle;
+    status = Client.iInsert(        &PDSId,
+        Key,
+        keySize,
+        Value,
+        valueSize,
+        0,
+        (pimd_cmd_RIU_flags_t)(PIMD_COMMAND_RIU_APPEND|PIMD_COMMAND_RIU_INSERT_USE_RECORD_LOCKS),
+        &commandHandle);
+
+    inc_async_ops();
+
+  }
+
+
+
+  BegLogLine( 1 )
+    << "Wrapper. Append. Status: " << pimd_status_to_string(status)
+    << EndLogLine;
+
+
+
+}
+
+
+JNIEXPORT jint JNICALL Java_com_ibm_pimd_PIMDWrapper_GlobalIteratorNext (JNIEnv *env, jclass jc, jobject PDSID, jobject key, jobject value) {
+
+  pimd_pds_id_t PDSId = getPIMDPDSID(env,PDSID);
+
+  char *Key    = (char*)env->GetDirectBufferAddress(key);
+  jlong KeyMaxLen      = env->GetDirectBufferCapacity(key);
+  char *Value    = (char*)env->GetDirectBufferAddress(value);
+  jlong ValueMaxLen      = env->GetDirectBufferCapacity(value);
+
+
+  BegLogLine( 1 )
+    << "Wrapper.GlobalIteratorNext. "
+    << "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
+    << EndLogLine;
+
+  pimd_status_t status;
+  int keySize;
+  int valueSize;
+
+  if(!current_cursor_open) {
+
+    status = Client.OpenCursor( &PDSId,
+                                &current_cursor );
+
+    if(status == PIMD_SUCCESS) {
+
+      status = Client.GetFirstElement( current_cursor,
+                                      Key,
+                                      &keySize,
+                                      KeyMaxLen,
+                                      Value,
+                                      &valueSize,
+                                      ValueMaxLen,
+                                      (pimd_cursor_flags_t)PIMD_CURSOR_NONE_FLAG);
+      current_cursor_open = 1;
+
+    }
+  } else {
+    status = Client.GetNextElement( current_cursor,
+                                    Key,
+                                    &keySize,
+                                    KeyMaxLen,
+                                    Value,
+                                    &valueSize,
+                                    ValueMaxLen,
+                                    (pimd_cursor_flags_t)PIMD_CURSOR_NONE_FLAG);
+
+  }
+
+  BegLogLine( 1 )
+    << "Wrapper. GlobalIteratorNext. Status: " << pimd_status_to_string(status)
+    << EndLogLine;
+
+  if(status==PIMD_ERRNO_END_OF_RECORDS) {
+    current_cursor_open=0;
+    return 0;
+  } else {
+
+		
+		jclass cls = env->FindClass("java/nio/Buffer");
+		if(cls==NULL) {
+			perror("FindClass");
+			exit(1);
+		}
+
+		jmethodID methodID1 = env->GetMethodID(cls, "limit","(I)Ljava/nio/Buffer;");
+//    jmethodID methodID1 = env->GetMethodID(cls, "limit","()I");
+		if(methodID1==NULL) {
+			perror("GetMethodID");
+			exit(1);
+		}
+
+	jobject mOwnerNodeId = env->CallObjectMethod(key, methodID1, (jint)keySize);
+//  jobject mOwnerNodeId = env->CallObjectMethod(key, methodID1);
+
+	return valueSize;
 	}
+}
+
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_GlobalIteratorClose (JNIEnv *env, jclass jc, jobject PDSID) {
+
+
+  pimd_status_t status;
+  if(current_cursor_open)
+    status = Client.CloseCursor(&current_cursor);
+  current_cursor_open = 0;
+
+
+  BegLogLine( 1 )
+    << "Wrapper. GlobalIteratorClose. Status: " << pimd_status_to_string(status)
+    << EndLogLine;
+}
 
-	//printf("Wrapper. Remove. mOwnerNodeId: %d, mIdOnOwner:%d - keySize: %d\n",PDSId.mOwnerNodeId,PDSId.mIdOnOwner,keySize);
-     	BegLogLine( 1 )
-        << "Wrapper. Remove. "
-	<< "mOwnerNodeId: " << PDSId.mOwnerNodeId << ", mIdOnOwner: " <<  PDSId.mIdOnOwner
-	<< " - keySize: " << keySize
-        << EndLogLine;
+JNIEXPORT void JNICALL Java_com_ibm_pimd_PIMDWrapper_Wait (JNIEnv *env, jclass jc) {
+
+  pimd_client_cmd_ext_hdl_t completeHandle;
+  while(pending_async_ops-->0) {
+    Client.WaitAny(&completeHandle);
+  }
+}
 
-	pimd_status_t status = Client.Remove( &PDSId,
-                                              Key,
-                                              keySize,
-                                              (pimd_cmd_remove_flags_t)PIMD_COMMAND_REMOVE_FLAGS_NONE);
 
-  	//printf("Wrapper. Remove. Status: %s\n",pimd_status_to_string(status));
- // 	StrongAssertLogLine( status == PIMD_SUCCESS ) << " Status : " << ,pimd_status_to_string(status) << EndLogLine;
-     	BegLogLine( 1 )
-	<< "Wrapper. Remove. Status: " << pimd_status_to_string(status)
-	<< EndLogLine;
 
+void inc_async_ops(void) {
+
+	if(pending_async_ops > 1000) {
+		Java_com_ibm_pimd_PIMDWrapper_Wait(NULL, NULL);
+	}
+
+	pending_async_ops++;
 }
 
+
+
diff --git a/pimd/pimd_wrapper/jni_java/Makefile b/pimd/pimd_wrapper/jni_java/Makefile
index 4bee434..96a916e 100644
--- a/pimd/pimd_wrapper/jni_java/Makefile
+++ b/pimd/pimd_wrapper/jni_java/Makefile
@@ -2,10 +2,11 @@ all:	pimd_jar
 
 pimd:
 	mkdir -p java/bin
-	$(JAVAC) -cp $(PIMDFS_JAVA_SRC):$(LOG4J_JAR):$(CONF_FOLDER):$(UNIT_TEST) -d java/bin java/src/com/ibm/pimd/*.java
+	$(JAVAC) -cp $(PIMDFS_JAVA_SRC):$(PIMD_UTILS_JAR):$(LOG4J_JAR):$(CONF_FOLDER):$(UNIT_TEST) -d java/bin java/src/com/ibm/pimd/*.java
 
 javah:  pimd
 	$(JAVAH) -classpath java/bin -d $(INCLUDE) -jni com.ibm.pimd.PIMDWrapper
+#	$(JAVAH) -classpath java/bin -d $(INCLUDE) -jni com.ibm.pimd.PIMDWrapperFAKE
 
 pimd_jar: javah pimd
 	$(JAR) cvf $(PIMD_WRAPPER_JAR) -C java/bin .
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTest.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTest.java
new file mode 100644
index 0000000..2d68851
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTest.java
@@ -0,0 +1,89 @@
+package com.ibm.pimd;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Iterator;
+
+import com.ibm.pimdfs.*;
+
+
+public class IteratorTest {
+		private static ByteBuffer bbWorkingKey;
+		private static ByteBuffer bbValue;
+
+	public static void main(String args[]) {
+
+		ArrayList<Integer> itemIn = new ArrayList<Integer>();
+		itemIn.add(2);
+		itemIn.add(3);
+		itemIn.add(4);
+		ArrayList<Integer> itemOut;
+		
+		PIMD pimd = new PIMD();
+
+		pimd.Connect("10.0.0.1","MyPDS");
+
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+//		pimd.Append("this is key 1",itemIn);
+		
+		byte[] b = new byte[6];
+		b[0] = 5; b[1] = 87; b[2] = 111; b[3] = 114; b[4] = 108; b[5] = 100;
+		bbWorkingKey = ByteBuffer.wrap(b, 0, 6);
+		bbWorkingKey.position(bbWorkingKey.limit());
+		
+		byte[] v = new byte[8];
+		v[0] = 0; v[1] = 0; v[2] = 0; v[3] = 1; b[4] = 0; v[5] = 0; v[6] = 0; v[6] = 0; v[7] = 1;
+		bbValue = ByteBuffer.wrap(v, 0, 8);
+		bbValue.position(bbValue.limit());
+		
+		pimd.Append(bbWorkingKey, bbValue);
+		
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 3",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 6",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 7",itemIn);
+		pimd.Append("this is key 8",itemIn);
+
+
+		IteratorTuple tuple = null;
+		do {
+			tuple = pimd.GlobalIteratorNext();
+			if(tuple != null) {
+				System.out.println("one more item, key: "+(String)tuple.getKey());
+				Iterator iter = tuple.getValues();
+				while(iter.hasNext()) {
+					ArrayList<Integer> x = (ArrayList<Integer>)iter.next();
+					System.out.println("\tValue: "+x);
+				}
+					
+			}
+		} while(tuple != null);
+	}
+}
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTest.new b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTest.new
new file mode 100644
index 0000000..cb7e34a
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTest.new
@@ -0,0 +1,72 @@
+package com.ibm.pimd;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Iterator;
+
+import com.ibm.pimdfs.*;
+
+
+public class IteratorTest {
+
+	public static void main(String args[]) {
+
+//    ArrayList<Integer> itemIn = new ArrayList<Integer>();
+		Integer itemIn = 1;
+
+		PIMD pimd = new PIMD();
+
+		pimd.Connect("10.0.0.1","MyPDS");
+
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		pimd.Append("this is key 1",itemIn);
+		
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 2",itemIn);
+		pimd.Append("this is key 3",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 4",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 6",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 5",itemIn);
+		pimd.Append("this is key 7",itemIn);
+		pimd.Append("this is key 8",itemIn);
+
+
+		IteratorTuple tuple = null;
+		do {
+			tuple = pimd.GlobalIteratorNext();
+			if(tuple != null) {
+				System.out.println("one more item, key: "+(String)tuple.getKey());
+				Iterator iter = tuple.getValues();
+				while(iter.hasNext()) {
+					Integer x = (Integer)iter.next();
+					System.out.println("\tValue: "+x);
+				}
+					
+			}
+		} while(tuple != null);
+	}
+}
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTuple.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTuple.java
new file mode 100644
index 0000000..abde570
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/IteratorTuple.java
@@ -0,0 +1,51 @@
+package com.ibm.pimd;
+
+import java.util.Iterator;
+import java.nio.ByteBuffer;
+import org.apache.log4j.Logger;
+
+public class IteratorTuple {
+
+	private final Object key;
+	private final ValuesIterator values;
+	
+	private final ByteBuffer bbkey;
+	
+	static Logger logger = Logger.getLogger(IteratorTuple.class);
+		
+	public IteratorTuple(Object key, ValuesIterator values) {
+		this.key = key;
+		this.values = values;
+		
+		this.bbkey = null;
+	}
+	
+	public IteratorTuple(ByteBuffer key, ValuesIterator values){
+		this.bbkey = key;
+		
+		this.key = null;
+		this.values = values;
+	}
+	
+	public  Object getKey() {
+		return key;
+	}
+	
+	public ByteBuffer getbbKey() {
+		ByteBuffer res = bbkey.duplicate();
+//		logger.debug("bbkey = " + bbkey);
+//		logger.debug("res = " + res);
+		res = res.slice();
+//		logger.debug("res = " + res);
+		
+		bbkey.position(bbkey.position());
+//		logger.debug("bbkey = " + bbkey);
+		
+		return res;
+	}
+	
+	public Iterator<?> getValues() {
+		return values.iterator();
+	}
+	
+}
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMD.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMD.java
index 9bc6aff..fcd2591 100644
--- a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMD.java
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMD.java
@@ -1,42 +1,52 @@
 package com.ibm.pimd;
 
-import java.nio.ByteBuffer;
-import java.io.ObjectOutputStream;
-import java.io.ObjectInputStream;
-import java.io.ByteArrayOutputStream;
 import java.io.ByteArrayInputStream;
-import org.apache.log4j.Logger;
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.nio.ByteBuffer;
+
+import com.ibm.pimdutils.TimeCounter;
 
-import javax.naming.OperationNotSupportedException;
+import org.apache.log4j.Logger;
 
 
 public class PIMD
 {
 
 	public static int MAX_VALUE_LENGTH = 1024*1024;
+	public static int MAX_KEY_LENGTH = 512*1024;
+	public final static boolean ASYNCHRONOUS = true;
+	public final static boolean SYNCHRONOUS = false;
 	
 	private PIMDPDSID handler = null;
 	private ByteArrayOutputStream bos=null;
 	private ObjectOutputStream os=null;
 	private ByteArrayInputStream bis=null;
 	private ObjectInputStream is=null;
-
- 	static Logger  logger = Logger.getLogger(PIMD.class);
+	private boolean AsynchronousPIMD = false;
+	
+ 	static Logger logger = Logger.getLogger(PIMD.class);
 
 	public PIMD() {
 
-		try {
-
-		} catch (Exception e) {
-			e.printStackTrace();
-			System.exit(-1);
-		}
+		this.AsynchronousPIMD = SYNCHRONOUS;  // Default should be SYNCHRONOUS
+		
+	}
 
+	public PIMD(boolean AsynchronousPIMD) {
+		
+		this.AsynchronousPIMD = AsynchronousPIMD;
+		
 	}
 
 	public void Connect(String Server, String PDS) {
 
-		PIMDWrapper.Init();
+		if(AsynchronousPIMD)
+			PIMDWrapper.Init_Asynchronous();
+		else
+			PIMDWrapper.Init();
 		logger.info("Init done");
 		PIMDWrapper.Connect(Server);
 		logger.info("Connected. Opening PDS");
@@ -44,11 +54,20 @@ public class PIMD
 		logger.info("Got PDSID: "+handler+" in Java code");
 
 	}
+	
+	public void Wait() {
+		if(AsynchronousPIMD) {
+			logger.info("Asynchronous PIMD WAIT");
+			PIMDWrapper.Wait();
+		}
+	}
+
 	public void Close() {
 
 		PIMDWrapper.Close(handler);
 
 	}
+	
 	public void Insert(Object keyIn, Object valueIn) {
 
 		Insert(ObjectToValue(keyIn),ObjectToValue(valueIn));
@@ -61,13 +80,23 @@ public class PIMD
 
 	}
 
+  public void Append(Object keyIn, Object valueIn) {
+
+    Append(ObjectToValue(keyIn),ObjectToValue(valueIn));
+
+  }
+  
+  public void Append(ByteBuffer keyIn, Object valueIn) {
+
+    Append(keyIn,ObjectToValue(valueIn));
+
+  }
 	
 	public Object Retrieve(Object keyIn) {
 
 		return ValueToObject(Retrieve(ObjectToValue(keyIn),MAX_VALUE_LENGTH));
 
 	}
-	
 
 	public Object Retrieve(Object keyIn, int size) {
 
@@ -82,7 +111,7 @@ public class PIMD
 		ByteBuffer key;
 		ByteBuffer value = ByteBuffer.allocateDirect(size);
 
-		logger.debug("KeyIn: "+keyIn);
+//		logger.debug("KeyIn: "+keyIn);
 
 		if(keyIn.isDirect()){
 			key = keyIn.duplicate();
@@ -96,11 +125,11 @@ public class PIMD
 		}
 
 
-		logger.debug("Retrieving value for key: "+key);
+//		logger.debug("Retrieving value for key: "+key);
                 int len = PIMDWrapper.Retrieve(handler,key,key.limit(),value,value.capacity());
-                logger.debug("Retrieved "+len+" bytes: "+value);
+//                logger.debug("Retrieved "+len+" bytes: "+value);
 
-        value.limit(len);
+        	value.limit(len);
 		return value;
 
 	}
@@ -122,7 +151,7 @@ public class PIMD
 		ByteBuffer key;
 		ByteBuffer value;
 		
-		logger.debug("KeyIn: "+keyIn);
+//		logger.debug("KeyIn: "+keyIn);
 
 		if(keyIn.isDirect()){
 			key = keyIn.duplicate();
@@ -146,9 +175,9 @@ public class PIMD
 			value.rewind();
 		}
 
-		logger.debug("keyIn: "+keyIn+" - key: "+key);
-		logger.debug("valueIn: "+valueIn+" - value: "+value);
-		logger.debug("Inserting key: "+key+" of "+key.limit()+" bytes, and value: "+value+" of "+value.limit()+" bytes");
+//		logger.debug("keyIn: "+keyIn+" - key: "+key);
+//		logger.debug("valueIn: "+valueIn+" - value: "+value);
+//		logger.debug("Inserting key: "+key+" of "+key.limit()+" bytes, and value: "+value+" of "+value.limit()+" bytes");
 
 		if(updateFlag)
 			PIMDWrapper.Update(handler,key,key.limit(),value,value.limit());
@@ -157,6 +186,101 @@ public class PIMD
 
 	}
 	
+    public void Append(ByteBuffer keyIn, ByteBuffer valueIn, int len) throws IOException{
+        ByteBuffer key;
+        ByteBuffer value;
+
+        //logger.debug("KeyIn: "+keyIn);
+
+        long startTA = System.currentTimeMillis(); //[Time_Calc]
+        if(keyIn.isDirect()){
+        	throw new IOException();
+        }
+        else {
+                key = ByteBuffer.allocateDirect(keyIn.limit());
+                for(int i = 0; i < keyIn.position(); i++) {
+                	key.put(keyIn.get(i));
+//                	logger.debug("keyIn.get(" + i + ") = " + keyIn.get(i));
+                	//logger.debug("key.get(" + i + ") = " + key.get(i));
+                }
+                key.rewind();
+        }
+
+        if(valueIn.isDirect()){
+        	throw new IOException();
+        }
+        else {
+                value = ByteBuffer.allocateDirect(valueIn.limit() + 4);
+//                byte[] b = new byte[] { (byte)(len >>> 24), (byte)(len >>> 16), (byte)(len >>> 8), (byte)len };
+//                value.put(b);
+                value.putInt(len);
+                for(int i = 0; i < valueIn.position(); i++) {
+                  value.put(valueIn.get(i));
+                	//logger.debug("valueIn.get(" + i + ") = " + valueIn.get(i));
+                }
+                value.rewind();
+        }
+					long stopTA = System.currentTimeMillis(); //[Time_Calc]
+					TimeCounter.PIMD_Append_allocate_key_value += (stopTA - startTA); //[Time_Calc]
+
+        //logger.debug("keyIn: "+keyIn+" - key: "+key);
+        //logger.debug("valueIn: "+valueIn+" - value: "+value);
+        //logger.debug("Appending to key: "+key+" of "+key.limit()+" bytes, and value: "+value+" of "+value.limit()+" bytes");
+
+					long startT = System.currentTimeMillis(); //[Time_Calc]
+        PIMDWrapper.Append(handler,key,key.limit(),value,value.limit());
+        
+					long stopT = System.currentTimeMillis(); //[Time_Calc]
+					TimeCounter.PIMDWrapper_Append += (stopT - startT); //[Time_Calc]
+    	
+    }
+
+    public void Append(ByteBuffer keyIn, ByteBuffer valueIn) {
+    	System.out.println("OJIIIIITO");
+
+        ByteBuffer key;
+        ByteBuffer value;
+
+//        logger.debug("KeyIn: "+keyIn);
+
+        if(keyIn.isDirect()){
+                key = keyIn.duplicate();
+                key.rewind();
+        }
+        else {
+                key = ByteBuffer.allocateDirect(keyIn.limit());
+                for(int i = 0; i < keyIn.position(); i++) {
+                	key.put(keyIn.get(i));
+//                	logger.debug("keyIn.get(" + i + ") = " + keyIn.get(i));
+//                	logger.debug("key.get(" + i + ") = " + key.get(i));
+                }
+                key.rewind();
+        }
+
+        if(valueIn.isDirect()){
+                value = valueIn.duplicate();
+                value.rewind();
+        }
+        else {
+                value = ByteBuffer.allocateDirect(valueIn.limit());
+                for(int i = 0; i < valueIn.position(); i++) {
+                  value.put(valueIn.get(i));
+//                	logger.debug("valueIn.get(" + i + ") = " + valueIn.get(i));
+                }
+                value.rewind();
+        }
+
+//        logger.debug("keyIn: "+keyIn+" - key: "+key);
+//        logger.debug("valueIn: "+valueIn+" - value: "+value);
+//        logger.debug("Appending to key: "+key+" of "+key.limit()+" bytes, and value: "+value+" of "+value.limit()+" bytes");
+
+				long startT = System.currentTimeMillis(); //[Time_Calc]
+        PIMDWrapper.Append(handler,key,key.limit(),value,value.limit());
+				long stopT = System.currentTimeMillis(); //[Time_Calc]
+				TimeCounter.PIMDWrapper_Append += (stopT - startT); //[Time_Calc]
+        
+
+}	
 	public void Remove(Object keyIn) { // throws OperationNotSupportedException {
 		
 			Remove(ObjectToValue(keyIn));
@@ -167,7 +291,7 @@ public class PIMD
 		
 		ByteBuffer key;
 
-		logger.debug("KeyIn: "+keyIn);
+//		logger.debug("KeyIn: "+keyIn);
 		
 		if(keyIn.isDirect()){
 			key = keyIn.duplicate();
@@ -180,12 +304,82 @@ public class PIMD
 			key.rewind();
 		}
 
-		logger.debug("Removing value for key: "+key);
+//		logger.debug("Removing value for key: "+key);
 	    	PIMDWrapper.Remove(handler,key,key.limit());
-		logger.debug("Removed");
+//		logger.debug("Removed");
       
 	}
 	
+	public void  GlobalIteratorClose() {
+
+		PIMDWrapper.GlobalIteratorClose(handler);
+		
+	}
+
+	public IteratorTuple GlobalIteratorNext() {
+
+		ByteBuffer key = ByteBuffer.allocateDirect(MAX_KEY_LENGTH);
+		ByteBuffer value = ByteBuffer.allocateDirect(MAX_VALUE_LENGTH);
+
+		int res = PIMDWrapper.GlobalIteratorNext(handler,key,value);
+		
+//		logger.debug("res = " + res);
+//		logger.debug("key = " + key);
+//		logger.debug("value = " + value);
+//  	logger.debug("key.get(0) = " + key.get(0));
+//  	logger.debug("value.get(3) = " + value.get(3));
+  	
+		if(res>0) {
+//			logger.debug("value = " + value);
+			value.limit(res);
+			value.position(res);
+
+//			logger.debug("value = " + value);
+//			logger.debug("key = " + key);
+			
+			return new IteratorTuple(ValueToObject(key), new ValuesIterator(value));
+		}
+		else {
+			GlobalIteratorClose();
+			return null;
+		}
+	}
+	
+	public IteratorTuple BBGlobalIteratorNext() {
+
+			long startTA = System.currentTimeMillis(); //[Time_Calc]
+		ByteBuffer key = ByteBuffer.allocateDirect(MAX_KEY_LENGTH);
+		ByteBuffer value = ByteBuffer.allocateDirect(MAX_VALUE_LENGTH);
+		
+			long stopTA = System.currentTimeMillis(); //[Time_Calc]
+			TimeCounter.PIMD_Allocate_key_value += (stopTA - startTA); //[Time_Calc]
+
+//	  logger.debug("key = " + key);
+		
+			long startT = System.currentTimeMillis(); //[Time_Calc]
+		int res = PIMDWrapper.GlobalIteratorNext(handler,key,value);
+		
+			long stopT = System.currentTimeMillis(); //[Time_Calc]
+			TimeCounter.PIMDWrapper_GlobalIteratorNext += (stopT - startT); //[Time_Calc]
+		
+//	  logger.debug("key = " + key);
+//    logger.debug(key.get(0));
+			long startTB = System.currentTimeMillis(); //[Time_Calc]
+		if(res>0) {
+			value.limit(res);
+			value.position(res);
+				long stopTB = System.currentTimeMillis(); //[Time_Calc]
+				TimeCounter.PIMD_sets_value += (stopTB - startTB); //[Time_Calc]
+			return new IteratorTuple(key, new ValuesIterator(value));
+		}
+		else {
+			GlobalIteratorClose();
+				long stopTB = System.currentTimeMillis(); //[Time_Calc]
+				TimeCounter.PIMD_iterrator_close += (stopTB - startTB); //[Time_Calc]
+			return null;
+		}
+	}
+	
 	
 
 	public ByteBuffer ObjectToValue(Object o) {
@@ -214,15 +408,17 @@ public class PIMD
 		
 		if(valueIn.isDirect()) {
 			value = ByteBuffer.allocate(valueIn.limit());
-                        for(int i = 0; i < valueIn.limit(); i++)
-                                value.put(valueIn.get(i));
-                        value.rewind();
+			for(int i = 0; i < valueIn.limit(); i++) {
+				value.put(valueIn.get(i));
+//        logger.debug("value.get(" + i + ") = " + value.get(i));
+			}
+			value.rewind();
 		} else {
 			value = valueIn;
 		}
 
 		try {
-			logger.debug("Deserializing "+value);
+//			logger.debug("Deserializing "+value);
 
 			bis = new ByteArrayInputStream(value.array());
 			is = new ObjectInputStream(bis);
@@ -239,10 +435,6 @@ public class PIMD
 
 		return null;
 	}
-/*
 
-    public static native void Insert(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize);
-    public static native int Retrieve(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize);
-*/
 }
 
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDMemoryManager.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDMemoryManager.java
new file mode 100644
index 0000000..d434053
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDMemoryManager.java
@@ -0,0 +1,54 @@
+package com.ibm.pimd;
+
+import java.nio.ByteBuffer;
+
+public class PIMDMemoryManager {
+
+        private static int BUFFER_SIZE = 8*1024*1024; 
+        private static ByteBuffer currentBuffer = ByteBuffer.allocateDirect(BUFFER_SIZE);
+        private static ByteBuffer backupBuffer = ByteBuffer.allocateDirect(BUFFER_SIZE);
+        private static int current_position = 0;
+        private static MemoryAllocatorThread memThread = new MemoryAllocatorThread();
+        
+        static {
+        	
+        	memThread.start();
+        	
+        }
+        
+        
+        public static ByteBuffer allocateDirect(int size) {
+
+                if(current_position + size > BUFFER_SIZE) {
+                        currentBuffer = backupBuffer;
+                        current_position=0;
+                        synchronized (memThread) {
+                        	memThread.notify();
+                        }
+                }
+                ByteBuffer res = currentBuffer.duplicate();
+                current_position += size;
+                res.position(current_position);
+                res.limit(res.position()+size);
+                return res.slice();
+
+        }
+        
+        private static class MemoryAllocatorThread
+        extends Thread {
+        public void run() {
+            while(true) {
+            	synchronized (this) {
+                        try {
+                          	wait();
+                          	backupBuffer = ByteBuffer.allocateDirect(BUFFER_SIZE);                        	
+                        } catch (Exception e) {
+                        }                    
+                }
+            }
+        }
+    }
+
+}
+
+
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapper.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapper.java
index 61070ce..0725b05 100644
--- a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapper.java
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapper.java
@@ -11,7 +11,9 @@ public class PIMDWrapper
     }
 
 
-    public static native void Init();
+    public static native void Init();  // Default: Synchronous
+    public static native void Init_Asynchronous();
+    public static native void Init_Bulk();
     public static native void Connect(String ServerAddress);
     public static native void Disconnect();
     public static native PIMDPDSID Open(String PDSName, int privs, int flags);
@@ -21,5 +23,11 @@ public class PIMDWrapper
     public static native int Retrieve(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize);
     public static native void Remove(PIMDPDSID id,ByteBuffer key, int keySize);
 
+    public static native void Append(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize);
+    public static native int GlobalIteratorNext(PIMDPDSID id,ByteBuffer key, ByteBuffer value);
+    public static native void GlobalIteratorClose(PIMDPDSID id);
+
+    public static native void Wait();
+
 }
 
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapperFAKE.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapperFAKE.java
new file mode 100644
index 0000000..e107e7f
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/PIMDWrapperFAKE.java
@@ -0,0 +1,205 @@
+package com.ibm.pimd;
+
+import java.nio.ByteBuffer;
+import java.util.HashMap;
+import java.util.Iterator;
+
+import com.ibm.pimdutils.TimeCounter;
+
+import org.apache.log4j.Logger;
+
+public class PIMDWrapperFAKE {
+
+	private static HashMap <ByteBuffer, ByteBuffer> map = null; 
+	private static Iterator<ByteBuffer> keyIterator = null;
+	
+	
+	static Logger logger = Logger.getLogger(PIMDWrapperFAKE.class);
+	
+	public static TimeCounter timec = null;
+	
+	
+    public static void Init() {
+    	
+    	map = new HashMap<ByteBuffer, ByteBuffer>();
+    	
+    }
+    public static void Init_Asynchronous() {
+    	
+    	Init();
+    }
+    public static  void Init_Bulk() {
+    	
+    	Init();
+    }
+    
+    public static void Connect(String ServerAddress) { }
+    public static void Disconnect() {}
+
+    public static PIMDPDSID Open(String PDSName, int privs, int flags) { return new PIMDPDSID(0,0); }
+    public static void Close(PIMDPDSID id) {}
+
+    public static void Insert(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize) {
+    
+    	map.remove(key);
+    	map.put(key, value);
+
+    }
+    
+    public static void Update(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize) {
+    
+    	map.remove(key);
+    	map.put(key, value);
+    
+    }
+    
+    public static int Retrieve(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize) {
+    	
+    	value.clear();
+    	value.put(map.get(key));
+    	return value.position();
+    	
+    }
+    public static void Remove(PIMDPDSID id,ByteBuffer key, int keySize) {
+    	
+    	map.remove(key);
+    	
+    }
+
+    public static void Append(PIMDPDSID id,ByteBuffer key, int keySize, ByteBuffer value, int valueSize) {
+    	
+    	long start_app_map1 = System.currentTimeMillis(); //[Time_Calc]
+    	
+    	ByteBuffer old = map.remove(key);
+    	
+    	long stop_app_map1 = System.currentTimeMillis(); //[Time_Calc]
+    	TimeCounter.Append_op_timer += (stop_app_map1 - start_app_map1); //[Time_Calc]
+    	
+    	ByteBuffer current;
+      
+    	if (old != null) {
+	      //logger.debug("old : " + old + " of " + old.limit() + " bytes");
+	    	current = ByteBuffer.allocateDirect(old.limit()+valueSize);
+	      //logger.debug("current : " + current + " of " + current.limit() + " bytes");
+	      
+	    	old.rewind();
+	    	current.put(old);
+    	} else {
+    		current = ByteBuffer.allocateDirect(valueSize);
+    	}
+    	
+//    	logger.debug("value = " + value);
+    	
+    	value.rewind();
+//    	logger.debug("value = " + value);
+//    	
+//    	logger.debug("current = " + current);
+    	
+    	current.put(value);
+    	
+//    	logger.debug("value = " + value);
+//    	logger.debug("current = " + current);
+    	
+    	current.rewind();
+    	
+//    	logger.debug("value = " + value);
+//    	logger.debug("current = " + current);
+    	
+    	start_app_map1 = System.currentTimeMillis(); //[Time_Calc]
+    	
+    	map.put(key, current);
+    	
+    	stop_app_map1 = System.currentTimeMillis(); //[Time_Calc]
+    	TimeCounter.Append_op_timer += (stop_app_map1 - start_app_map1); //[Time_Calc]
+//    	logger.debug("current = " + current);
+    	
+    }
+    
+    public static int[] Prueba(){
+    	int[] array = new int[2];
+    	
+    	array[0] = 1;
+    	array[1] = 2;
+    	
+			return array;
+    }
+    
+    public static int GlobalIteratorNext(PIMDPDSID id,ByteBuffer key, ByteBuffer value){
+    	
+  		int len = 0; 
+    		
+    	try {
+    		
+    		logger.debug("");
+	    	if(keyIterator == null)
+	    			keyIterator  = map.keySet().iterator();
+	    	
+	    	if(keyIterator.hasNext()) {
+	//    		logger.debug(map.get((keyIterator.next())).get(0));
+	    		
+	    		//Obtaining key
+	    		key.clear();
+	    		logger.debug("key = " + key);
+	    		
+	    		ByteBuffer skey = keyIterator.next();
+	    		
+	    		logger.debug("skey = " + skey);
+	        logger.debug(skey.get(0));
+	        
+	        key.limit(skey.limit());
+	        
+	    		logger.debug("key = " + key);
+	    		
+	    		key.put(skey);
+	    		
+	    		logger.debug("key = " + key);
+	    		logger.debug("skey = " + skey);
+	        logger.debug(key.get(0));
+	        
+	    		key.rewind();
+	    		skey.rewind();
+	    		
+	    		logger.debug("key = " + key);
+	    		logger.debug("skey = " + skey);
+	    		
+	    		//Obtaining value
+	    		value.clear();
+	    		value.put(map.get(skey));
+	    		
+	    		logger.debug("value = " + value);
+	        logger.debug(value.get(3));
+	    		logger.debug("value = " + value);
+	          
+	        len = value.position();
+				  value.rewind();
+				  
+	    		logger.debug("value = " + value);
+	    		
+	    		return len;
+	    		
+	    	} else
+	    		return 0;
+	    	
+	//    	return value.limit();
+    	} catch (Exception e) {
+    		System.err.println("Caught Exception: " 
+            +  e.getMessage());
+    		e.printStackTrace();
+    		System.exit(-1);
+    	}
+    	
+			return -1;
+    	
+    }
+    
+    public static void GlobalIteratorClose(PIMDPDSID id) {
+    	
+    	keyIterator = null;
+    	
+    }
+
+    public static void Wait() {}
+
+	
+	
+}
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/Test5Async.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/Test5Async.java
new file mode 100644
index 0000000..6d07b67
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/Test5Async.java
@@ -0,0 +1,47 @@
+package com.ibm.pimd;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+
+import com.ibm.pimdfs.*;
+
+
+public class Test5Async {
+
+	public static void main(String args[]) {
+
+		ArrayList<Integer> itemIn = new ArrayList<Integer>();
+		itemIn.add(2);
+		itemIn.add(3);
+		itemIn.add(4);
+		ArrayList<Integer> itemOut;
+
+		PIMD pimd = new PIMD(true);
+
+		pimd.Connect("10.0.0.1","MyPDS");
+
+		System.out.println();
+		pimd.Insert("this is the key",itemIn);
+		System.out.println();
+		itemOut = (ArrayList)pimd.Retrieve("this is the key",1024*1024);
+
+		System.out.println("*** Inserted "+ itemIn + " - Retrieved: " + itemOut);
+
+		System.out.println();
+
+		//Reinsert key with diferent value
+		itemIn = new ArrayList<Integer>();
+		itemIn.add(1);
+		itemIn.add(2);
+		for(int i=0; i <200000; i++)
+			pimd.Insert("this is the key"+i,itemIn);
+		System.out.println();
+		//itemOut = (ArrayList)pimd.Retrieve("this is the key",1024*1024);
+		//System.out.println("Waiting");
+		//System.out.println("*** Inserted "+ itemIn + " - Retrieved: " + itemOut);
+
+		pimd.Wait();
+		System.out.println("Done");
+
+	}
+}
diff --git a/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/ValuesIterator.java b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/ValuesIterator.java
new file mode 100644
index 0000000..65ed60d
--- /dev/null
+++ b/pimd/pimd_wrapper/jni_java/java/src/com/ibm/pimd/ValuesIterator.java
@@ -0,0 +1,61 @@
+package com.ibm.pimd;
+
+import java.io.Serializable;
+import java.nio.ByteBuffer;
+import java.util.Iterator;
+import org.apache.log4j.Logger;
+
+
+public class ValuesIterator implements Serializable, Iterator<ByteBuffer>{
+	public static final long serialVersionUID = 3L;
+	
+	private ByteBuffer data;
+	private ByteBuffer iteratorView;
+	
+	static Logger logger = Logger.getLogger(ValuesIterator.class);
+	
+	public ValuesIterator(ByteBuffer data) {
+		this.data = data;
+		//System.out.println("ArrayValues.data: "+this.data);
+	}
+
+	public Iterator<ByteBuffer> iterator() {
+		iteratorView = data.duplicate();
+//		logger.debug("data.get(" + 3 + ") = " + data.get(3));
+//		logger.debug("iteratorView.get(" + 3 + ") = " + iteratorView.get(3));
+		iteratorView.flip();
+		//System.out.println("data: "+data+", iteratorView: "+iteratorView);
+		return this;
+	}
+	
+	//Iterator
+	public boolean hasNext() {
+		return (iteratorView.remaining()>0);
+	}
+		
+	public ByteBuffer next() {
+	
+		
+//		logger.debug("iteratorView.get(" + 3 + ") = " + iteratorView.get(3));
+		
+		int len = iteratorView.getInt();
+//		logger.debug("len = " + len);
+		ByteBuffer res = iteratorView.duplicate();
+//		logger.debug("iteratorView = " + iteratorView);
+//		logger.debug("res = " + res);
+		res.limit(res.position() + len);
+//		logger.debug("res = " + res);
+		res = res.slice();
+//		logger.debug("res = " + res);
+		
+		iteratorView.position(iteratorView.position() + len);
+//		logger.debug("iteratorView = " + iteratorView);
+		
+		return res;
+	}
+		
+	public void remove() {
+		
+	}
+}
+
diff --git a/pimd/run.sh b/pimd/run.sh
old mode 100644
new mode 100755
diff --git a/pimd/run_bench.sh b/pimd/run_bench.sh
old mode 100644
new mode 100755
diff --git a/pimd/run_bench_java_async.sh b/pimd/run_bench_java_async.sh
new file mode 100755
index 0000000..994a885
--- /dev/null
+++ b/pimd/run_bench_java_async.sh
@@ -0,0 +1,5 @@
+#!/bin/bash
+
+. ./env.sh
+
+make pimd; make run_bench_java_async; make clean-pimd
diff --git a/pimd/run_bench_java_sync.sh b/pimd/run_bench_java_sync.sh
new file mode 100755
index 0000000..c7e31ef
--- /dev/null
+++ b/pimd/run_bench_java_sync.sh
@@ -0,0 +1,5 @@
+#!/bin/bash
+
+. ./env.sh
+
+make pimd; make run_bench_java_sync; make clean-pimd
diff --git a/pimd/save_stats.sh b/pimd/save_stats.sh
new file mode 100755
index 0000000..28adf03
--- /dev/null
+++ b/pimd/save_stats.sh
@@ -0,0 +1,16 @@
+#!/bin/bash
+
+
+if [ $# -ne 2 ]
+then
+  echo "Usage: `basename $0` <test name>  <root folder containing stats and rawdata>"
+  exit
+fi
+
+name=backup_stats/`date +%d%m%y_%H%M%S_backup_`$1.tgz
+folder=$2
+
+
+
+tar czf $name $folder/stats $folder/rawdata
+
diff --git a/pimd/utils/Makefile b/pimd/utils/Makefile
index 3ebcb97..8b849c7 100644
--- a/pimd/utils/Makefile
+++ b/pimd/utils/Makefile
@@ -17,6 +17,7 @@ tester:
 	$(CPP) -I$(INCLUDE) -I$(MPIINC) -I$(PIMDINC) -L$(PIMDLIB) $(CFLAGS) pimd_tester1.cpp $(LIBS) -o pimdtester1
 	$(CPP) -I$(INCLUDE) -I$(MPIINC) -I$(PIMDINC) -L$(PIMDLIB) $(CFLAGS) pimd_tester2.cpp $(LIBS) -o pimdtester2
 	$(CPP) -I$(INCLUDE) -I$(MPIINC) -I$(PIMDINC) -L$(PIMDLIB) $(CFLAGS) pimd_tester3.cpp $(LIBS) -o pimdtester3
+	$(CPP) -I$(INCLUDE) -I$(MPIINC) -I$(PIMDINC) -L$(PIMDLIB) $(CFLAGS) pimd_tester4.cpp $(LIBS) -o pimdtester4
 
 
 run_tester1: tester
@@ -28,6 +29,9 @@ run_tester2: tester
 run_tester3: tester
 	./pimdtester3
 
+run_tester4: tester
+	./pimdtester4
+
 clean:
 	rm -f hadoop.class 
 	rm -f pimdkv_tester1.class 
diff --git a/pimd/utils/pimd_tester4.cpp b/pimd/utils/pimd_tester4.cpp
new file mode 100644
index 0000000..cf4c5f2
--- /dev/null
+++ b/pimd/utils/pimd_tester4.cpp
@@ -0,0 +1,164 @@
+#include <pimd_client.hpp>
+#include <stdlib.h>
+#include <unistd.h>
+
+int main(int argc, char**argv) {
+
+	pimd_pds_id_t   PDSId;
+	pimd_client_t Client;
+	pimd_client_cursor_ext_hdl_t current_cursor;
+
+	char *PDS="MyPDS";
+
+	int Key1 = 1;
+	int KeySize1 = sizeof(Key1);
+	int Key2 = 2;
+	int KeySize2 = sizeof(Key2);
+	int Key3 = 3;
+	int KeySize3 = sizeof(Key3);
+	int Key4 = 4;
+	int KeySize4 = sizeof(Key4);
+
+	int Value1[1]={1};
+	int Value2[2]={1,2};
+	int Value3[3]={1,2,3};
+	int Value4[4]={1,2,3,4};
+
+	int Value1Size=sizeof(Value1);
+	int Value2Size=sizeof(Value2);
+	int Value3Size=sizeof(Value3);
+	int Value4Size=sizeof(Value4);
+
+	int KeyRetrieve[1];
+	int KeyRetrieveSize = sizeof(KeyRetrieve);
+	int ValueRetrieve[4];
+	int ValueRetrieveSize = sizeof(ValueRetrieve);
+
+	printf("\nSize Key1: %d, size Val1: %d, size ValRetrieve: %d\n",KeySize1, Value1Size, ValueRetrieveSize);
+
+	printf("We insert a number of keys\n");
+
+	pimd_status_t status = Client.Init(0, 0);
+	printf("Init: %s\n",pimd_status_to_string(status));
+	
+	status = Client.Connect( "10.0.0.1", 0 );
+	printf("Connect: %s\n",pimd_status_to_string(status));
+
+	status = Client.Open( PDS,
+												(pimd_pds_priv_t)(PIMD_PDS_READ|PIMD_PDS_WRITE),
+												(pimd_cmd_open_flags_t)PIMD_COMMAND_OPEN_FLAGS_CREATE,
+												&PDSId);
+
+	printf("Open: %s\n",pimd_status_to_string(status));
+
+	status = Client.Insert( &PDSId,
+													(char *)&Key1,
+													KeySize1,
+													(char *)&Value1,
+													Value1Size,
+													0,
+													(pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
+
+	printf("Insert: %s - %d bytes\n",pimd_status_to_string(status),Value1Size);
+
+	status = Client.Insert( &PDSId,
+													(char *)&Key2,
+													KeySize2,
+													(char *)&Value2,
+													Value2Size,
+													0,
+													(pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
+
+	printf("Insert: %s - %d bytes\n",pimd_status_to_string(status),Value2Size);
+
+
+	status = Client.Insert( &PDSId,
+													(char *)&Key3,
+													KeySize3,
+													(char *)&Value3,
+													Value3Size,
+													0,
+													(pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
+
+	printf("Insert: %s - %d bytes\n",pimd_status_to_string(status),Value3Size);
+
+	status = Client.Insert( &PDSId,
+													(char *)&Key4,
+													KeySize4,
+													(char *)&Value4,
+													Value4Size,
+													0,
+													(pimd_cmd_RIU_flags_t)PIMD_COMMAND_RIU_UPDATE);
+
+	printf("Insert: %s - %d bytes\n",pimd_status_to_string(status),Value4Size);
+
+//  int len = 0;
+//  status = Client.Retrieve(	&PDSId,
+//                            (char *)&Key1,
+//                            KeySize1,
+//                            (char *)&ValueRetrieve,
+//                            ValueRetrieveSize,
+//                            &len,
+//                            0,
+//                            (pimd_cmd_RIU_flags_t)0);  //PIMD_COMMAND_RIU_FLAGS_NONE
+
+//  printf("Retrieve: %s\n",pimd_status_to_string(status));
+//  printf("len: %d\n\n",len);
+
+	//Start Iterator
+	int i;
+	status = Client.OpenCursor(	&PDSId,
+															&current_cursor);
+
+	status = Client.GetFirstElement( current_cursor,
+																		(char *)&KeyRetrieve,
+																		&KeyRetrieveSize,
+																		KeyRetrieveSize,
+																		(char *)&ValueRetrieve,
+																		&ValueRetrieveSize,
+																		ValueRetrieveSize,
+																		(pimd_cursor_flags_t)PIMD_CURSOR_NONE_FLAG);
+
+	printf("First Element: [%d, <%d>], status = %s\n", KeyRetrieve[0], ValueRetrieve[0], pimd_status_to_string(status));
+
+	while (status != PIMD_ERRNO_END_OF_RECORDS) {
+//    printf("Key = %d, status = %s\n", KeyRetrieve[0], pimd_status_to_string(status));
+//    printf("Value = < ");
+//    for (i=0; i<KeyRetrieve[0]; i++) {
+//      printf("%d ", ValueRetrieve[i]);
+//    }
+//    printf(">\n");
+
+		int ValueRetrieve[4];
+		int ValueRetrieveSize = sizeof(ValueRetrieve);
+		status = Client.GetNextElement( current_cursor,
+																		(char *)&KeyRetrieve,
+																		&KeyRetrieveSize,
+																		KeyRetrieveSize,
+																		(char *)&ValueRetrieve,
+																		&ValueRetrieveSize,
+																		ValueRetrieveSize,
+																		(pimd_cursor_flags_t)PIMD_CURSOR_NONE_FLAG);
+		if (status == PIMD_ERRNO_END_OF_RECORDS) break;
+
+		printf("Key = %d, status = %s\n", KeyRetrieve[0], pimd_status_to_string(status));
+		printf("Value = < ");
+		for (i=0; i<KeyRetrieve[0]; i++) {
+			printf("%d ", ValueRetrieve[i]);
+		}
+		printf(">\n");
+
+//    int alen = sizeof(ValueRetrieve)/sizeof(int);
+//    printf("alen = %d\n", alen);
+	}
+
+	status = Client.CloseCursor(current_cursor);
+
+
+
+
+
+
+
+
+}
diff --git a/src/core/org/apache/hadoop/fs/IntermediateKV.java b/src/core/org/apache/hadoop/fs/IntermediateKV.java
index 8bf318d..27b43ca 100644
--- a/src/core/org/apache/hadoop/fs/IntermediateKV.java
+++ b/src/core/org/apache/hadoop/fs/IntermediateKV.java
@@ -1529,7 +1529,7 @@ public abstract class IntermediateKV extends Configured implements Closeable {
    * @return a long
    * @throws IOException see specific implementation
    */
-  public abstract int numKeys() throws IOException;
+//  public abstract int numKeys() throws IOException;
   
   /**
    * Get the checksum of a file.
diff --git a/src/core/org/apache/hadoop/fs/KVDataInputStream.java b/src/core/org/apache/hadoop/fs/KVDataInputStream.java
index f61a0fa..3a919c7 100644
--- a/src/core/org/apache/hadoop/fs/KVDataInputStream.java
+++ b/src/core/org/apache/hadoop/fs/KVDataInputStream.java
@@ -50,9 +50,9 @@ public class KVDataInputStream extends DataInputStream
     }
   }
   
-  public int numKeys() throws IOException {
-  	return ((KVReadable)in).numKeys();
-  }
+//  public int numKeys() throws IOException {
+//  	return ((KVReadable)in).numKeys();
+//  }
   
   public int seekNextKey() throws IOException {
   	return ((KVReadable)in).seekNextKey();
diff --git a/src/core/org/apache/hadoop/fs/KVReadable.java b/src/core/org/apache/hadoop/fs/KVReadable.java
index fd52e42..6487791 100644
--- a/src/core/org/apache/hadoop/fs/KVReadable.java
+++ b/src/core/org/apache/hadoop/fs/KVReadable.java
@@ -29,7 +29,7 @@ public interface KVReadable {
 	/**
 	 * Return the current number of Keys
 	 */
-	int numKeys() throws IOException;
+//	int numKeys() throws IOException;
 	
 	/**
 	 * Position KV storage in the next key
diff --git a/src/core/org/apache/hadoop/fs/memcachedKV/MemCachedKV.java b/src/core/org/apache/hadoop/fs/memcachedKV/MemCachedKV.java
index cd85764..f681eb2 100644
--- a/src/core/org/apache/hadoop/fs/memcachedKV/MemCachedKV.java
+++ b/src/core/org/apache/hadoop/fs/memcachedKV/MemCachedKV.java
@@ -61,11 +61,6 @@ public class MemCachedKV extends IntermediateKV {
 	}
 	
 	@Override
-	public int numKeys() throws IOException {
-		return memKV.numKeys();
-	}
-
-	@Override
 	public KVDataOutputStream create() throws IOException {
 		return new KVDataOutputStream(new MeMCKVOutputStream(memKV, null), 
 				statistics);
diff --git a/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVImpl.java b/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVImpl.java
index d41c41f..25935d8 100644
--- a/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVImpl.java
+++ b/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVImpl.java
@@ -60,10 +60,10 @@ public class PIMDKVImpl extends IntermediateKV {
 		return new KVDataInputStream(new PIMDKVInputStream(pimdKV, null));
 	}
 	
-	@Override
-	public int numKeys() throws IOException {
-		return pimdKV.numKeys();
-	}
+//	@Override
+//	public int numKeys() throws IOException {
+//		return pimdKV.numKeys();
+//	}
 
 	@Override
 	public KVDataOutputStream create() throws IOException {
diff --git a/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVInputStream.java b/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVInputStream.java
index 1ad393a..b7fc3f8 100644
--- a/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVInputStream.java
+++ b/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVInputStream.java
@@ -25,21 +25,18 @@ import org.apache.hadoop.fs.FSInputStream;
 import com.ibm.pimdKV.PIMDKV;
 import org.apache.hadoop.fs.KVReadable;
 
+import com.ibm.pimdutils.TimeCounter;
+
 public class PIMDKVInputStream extends InputStream 
 		implements KVReadable {
 
     private FileSystem.Statistics statistics;
     private PIMDKV pimdKV;
     
-    private int numKeys, numValues;
-    private int keypos;
-    private int valuepos;
+//    private int numKeys; 
     private int currentKeyLength, currentValueLength;
     private boolean isNewKey;
-    private String key;
     
-		private int fid;
-		private long pos = 0;
 		private boolean closed;
 
     @Deprecated
@@ -50,36 +47,16 @@ public class PIMDKVInputStream extends InputStream
 	public PIMDKVInputStream(PIMDKV pimdKV, FileSystem.Statistics stats) throws IOException{
 		this.statistics = stats;
 		
-		//this.fid = fsstore.open(path);
 		this.pimdKV = pimdKV;
 		
-		numKeys = pimdKV.numKeys();
-		numValues = -1;
-		keypos = -1;
-		valuepos = -1;
 		isNewKey = true;
 		
-/*		if (this.fid < 0) {
-			this.fsize = 0;
-		} else {
-			this.fsize = fsstore.filesize(path);
-		}
-*/
 	}
 
-	public int numKeys() throws IOException {
-		return this.numKeys;
-	}
-
-	
 	public boolean hasNext() throws IOException {
 		return true;
 	}
 
-	public long getPos() throws IOException {
-		return pos;
-	}
-	
 	public long length() throws IOException {
 		return 0;
 	}
@@ -120,105 +97,59 @@ public class PIMDKVInputStream extends InputStream
 	}
 	
 	public int seekNextKey() throws IOException {
-		//There are values evaluate
-		if (valuepos < 0 || (valuepos + 1) >= numValues) {
-			//Another Key
-			if (++keypos >= numKeys) {
-				return -1;//No more keys
-			}
-			currentKeyLength = pimdKV.seekKey(keypos);
-			// Sanity check
-			if (currentKeyLength < 0) {
-				throw new IOException("keypos = " + keypos + " : Negative key-length: " + currentKeyLength);
-			}
-			numValues = pimdKV.numValues();
-			if (numValues < 0) {
-				throw new IOException("Negative numValues: " + numValues + " : At least one!!!");
-			}
-			valuepos = -1;
-			isNewKey = true;
+		
+			long startT = System.currentTimeMillis(); //[Time_Calc]
+			
+		if (pimdKV.hasNextValue()) {
+			isNewKey = false;
 			return currentKeyLength;
-			/*
-			 * Another way maintaining a key
-			 * key = pimdKV.readKey(keypos);
-			 * return key.length();
-			 */
 		}
-		isNewKey = false;
-		return currentKeyLength;
+		
+			long nstartT = System.currentTimeMillis(); //[Time_Calc]
+		currentKeyLength = pimdKV.seekKey();
+			long nstopT = System.currentTimeMillis(); //[Time_Calc]
+			TimeCounter.seekKey += (nstopT - nstartT); //[Time_Calc]
+			
+		isNewKey = true;
+		
+			long stopT = System.currentTimeMillis(); //[Time_Calc]
+			TimeCounter.seekNextKey += (stopT - startT); //[Time_Calc]
+		
+			return currentKeyLength;
+			
 	}
 	
 	public int seekNextValue() throws IOException {
-		//Another Value
-		if (++valuepos >= numValues) {
-			return -1;
-		}
-		currentValueLength = pimdKV.valueSize(valuepos);
-		if (currentValueLength < 0) {
-			throw new IOException("valuepos = " + valuepos + " : Negative value-length: " + currentValueLength);
-		}
-		//return pimdKV.keySize(keypos);
-		/*
-		 * Another way maintaining a key
-		 * key = pimdKV.readKey(keypos);
-		 * return key.length();
-		 */
+		
+		currentValueLength = pimdKV.valueSize();
+		
 		return currentValueLength;
 	}
 	
 	public int readKey(byte[] buf, int off, int len) throws IOException {
 		if (isNewKey) {
-			if (keypos < numKeys) {
-				int res = pimdKV.readKey(keypos, buf, off, len);
-				/*
-				 * Another way maintaining a key
-				 * System.arraycopy(key.getBytes(), off, buf, 0, len);
-				 */
+				int res = pimdKV.readKey(buf, off, len);
+				
 				if (statistics != null && res > 0) {
 					statistics.incrementBytesRead(res);
 				}
 				return res;
-			}
-			return -1;
+				
 		}
 		return -99;
   }
 
 	public int readValue(byte[] buf, int off, int len) throws IOException {
-		if (valuepos < numValues) {
-			int res = pimdKV.readValue(valuepos, buf, off, len);
-			/*
-			 * Another way maintaining a key
-			 * System.arraycopy(key.getBytes(), off, buf, 0, len);
-			 */
+		
+			int res = pimdKV.readValue(buf, off, len);
 			
 			if (statistics != null && res > 0) {
 				statistics.incrementBytesRead(res);
 			}
 			return res;
-		}
-		return -1;
+			
   }
 
-    public synchronized int read(byte b[], int off, int len) throws IOException {
-			if (closed) throw new IOException("Stream closed");
-
-			/*
-			if (pos < fsize) {
-				int res = 0; //fsstore.read(this.fid, pos, b, off, len);
-				if (res >= 0) {
-					pos += res;
-				}
-
-				if (statistics != null && res > 0) {
-					statistics.incrementBytesRead(res);
-				}
-				return res;
-			}
-			*/
-			return -1;
-    }
-
     public synchronized void close() throws IOException {
 			if (closed) return;
 
diff --git a/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVOutputStream.java b/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVOutputStream.java
index 13f10c1..f3d741b 100644
--- a/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVOutputStream.java
+++ b/src/core/org/apache/hadoop/fs/pimdKV/PIMDKVOutputStream.java
@@ -34,28 +34,21 @@ class PIMDKVOutputStream extends OutputStream {
     private Progressable progressReporter;
     private PIMDKV pimdKV;
 
-    private String path;
-		private int fid;
-		private long filePos = 0;
-		private int pos = 0;
+//		private long filePos = 0;
 		private boolean closed;
 		
-		private boolean writeKey;
 
     public PIMDKVOutputStream(PIMDKV pimdKV, Progressable prog) throws IOException{
 
 			this.pimdKV = pimdKV;
 
-			//this.fid = fsstore.create(path, replication);
-
 			this.progressReporter = prog;
 			
-			this.writeKey = true;
     }
 
-    public long getPos() throws IOException {
-			return filePos;
-    }
+//    public long getPos() throws IOException {
+//			return filePos;
+//    }
 
     public void write(int v) throws IOException {
 			if (closed) throw new IOException("Stream closed");
@@ -74,32 +67,21 @@ class PIMDKVOutputStream extends OutputStream {
 
 			//progressReporter.progress();
 			int bytesWrited = 0;
-			if (writeKey) {
-				bytesWrited = pimdKV.writeK(buf, 0, len);
-			} else {
-				pimdKV.writeV(buf, 0, len);
-			}
-			writeKey = !writeKey;
+			bytesWrited = pimdKV.write(buf, 0, len);
 			
-			//int res = pimdKV.write(this.fid, filePos, b, off, len);
 			if (bytesWrited < 0) throw new IOException("MeMCFSOutputStream write failed");
 
-			filePos += bytesWrited;
+//			filePos += bytesWrited;
     }
 
     public void flush() throws IOException {
 			if (closed) throw new IOException("Stream closed");
 
-			return;
+			// DC - ASYNC
+      if(pimdKV != null)
+        pimdKV.flush();
 
-			/*
-        if (kfsChannel == null) {
-            throw new IOException("File closed");
-        }
-        // touch the progress before going into KFS since the call can block
-        progressReporter.progress();
-        kfsChannel.sync();
-				*/
+			return;
     }
 
     public synchronized void close() throws IOException {
diff --git a/src/mapred/mapred-default.xml b/src/mapred/mapred-default.xml
index 72f59ea..c0d4fa9 100644
--- a/src/mapred/mapred-default.xml
+++ b/src/mapred/mapred-default.xml
@@ -322,7 +322,9 @@
 
 <property>
   <name>mapred.child.java.opts</name>
-  <value>-Xmx200m</value>
+	<value>-Xmx200m</value>
+<!--  <value>-Xmx200m -Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8012</value>-->
+<!--  <final>true</final>-->
   <description>Java opts for the task tracker child processes.  
   The following symbol, if present, will be interpolated: @taskid@ is replaced 
   by current TaskID. Any other occurrences of '@' will go unchanged.
diff --git a/src/mapred/org/apache/hadoop/mapred/IKV.java b/src/mapred/org/apache/hadoop/mapred/IKV.java
index b3be703..53d260c 100644
--- a/src/mapred/org/apache/hadoop/mapred/IKV.java
+++ b/src/mapred/org/apache/hadoop/mapred/IKV.java
@@ -138,6 +138,13 @@ public class IKV {
       }
     }
 
+// DC - ASYNC
+//
+    public void flush() throws IOException {
+      outKV.flush();
+    }
+
+
     public void append(K key, V value) throws IOException {
       if (key.getClass() != keyClass)
         throw new IOException("wrong key class: "+ key.getClass()
diff --git a/src/mapred/org/apache/hadoop/mapred/IntermediateKVIter.java b/src/mapred/org/apache/hadoop/mapred/IntermediateKVIter.java
index 0a81165..00a89bd 100644
--- a/src/mapred/org/apache/hadoop/mapred/IntermediateKVIter.java
+++ b/src/mapred/org/apache/hadoop/mapred/IntermediateKVIter.java
@@ -82,7 +82,6 @@ implements RawKeyValueIterator {
     this.ikv= ikv;
     KVDataInputStream inKV = ikv.open();
     //this.numKeys = ikv.numKeys(); //ikv -> MemCachedKV
-    this.numKeys = inKV.numKeys(); //inKV -> MeMCKVInputStream
     
     reader = new ReaderKV<K,V>(conf, inKV, codec, readsCounter);
     
@@ -125,15 +124,11 @@ implements RawKeyValueIterator {
   }
 
   public boolean next() throws IOException {
-//  	if (first == false) {
-  		boolean hasNext = nextRawKey();
+  		boolean hasNext = reader.nextRawKey(key); //intentando eliminar nextRawKey()
   		if (!hasNext) {
   			return false;
   		}
-//  	}
-  	
-//  	first = false;
-  	nextRawValue();
+  	reader.nextRawValue(value); //nextRawValue();
   	
     //key = minSegment.getKey();
     //minSegment.getValue(value);
diff --git a/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java b/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
index 4372934..b5ddfa1 100644
--- a/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
+++ b/src/mapred/org/apache/hadoop/mapred/LocalJobRunner.java
@@ -196,7 +196,6 @@ class LocalJobRunner implements JobSubmissionProtocol {
 				
 					long stopAllMapTask = System.currentTimeMillis(); //[Time_Calc]
 					LOG.info("[TIME]: [LocalJobRunner] All MapTask: "+ (stopAllMapTask-startAllMapTask) + " ms"); //[Time_Calc]
-					LOG.debug("PROBANNNNNNDO");
 
         TaskAttemptID reduceId = 
           new TaskAttemptID(new TaskID(jobId, false, 0), 0);
diff --git a/src/mapred/org/apache/hadoop/mapred/MapTask.java b/src/mapred/org/apache/hadoop/mapred/MapTask.java
index 7abc0ad..116dcdc 100644
--- a/src/mapred/org/apache/hadoop/mapred/MapTask.java
+++ b/src/mapred/org/apache/hadoop/mapred/MapTask.java
@@ -69,6 +69,8 @@ import org.apache.hadoop.util.QuickSort;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
+import com.ibm.pimdutils.TimeCounter;
+
 /** A Map task. */
 class MapTask extends Task {
   /**
@@ -283,6 +285,10 @@ class MapTask extends Task {
   public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)
     throws IOException, ClassNotFoundException, InterruptedException {
     this.umbilical = umbilical;
+    System.out.println("************************");
+    System.out.println("*** RUNNING MAPTASK!!!! ***");
+    System.out.println("************************");
+    
 
     // start thread that will handle communication with parent
     TaskReporter reporter = new TaskReporter(getProgress(), umbilical);
@@ -1145,6 +1151,10 @@ class MapTask extends Task {
           sortAndSpill();
 						long stopFinalSortAndSpill= System.currentTimeMillis(); //[Time_Calc]
 						LOG.info("[TIME]:   [MapTask] Final sortAndSpill: "+ (stopFinalSortAndSpill-startFinalSortAndSpill) + " ms"); //[Time_Calc]
+						LOG.info("[TIME]:   [MapTask]-[PIMDKV.java] All Appends: " + TimeCounter.All_PIMDKV_Appends + "ms"); //[Time_Calc]
+						LOG.info("[TIME]:   [MapTask]-[PIMD.java]      Allocate key value ByteBuffers: " + TimeCounter.PIMD_Append_allocate_key_value+ "ms"); //[Time_Calc]
+						LOG.info("[TIME]:   [MapTask]-[PIMD.java]      Append PIMDWrapper Cost: " + TimeCounter.PIMDWrapper_Append + "ms"); //[Time_Calc]
+//            LOG.info("[TIME]:   [MapTask]       Append WrapperFake Operative: " + TimeCounter.Append_op_timer + "ms"); //[Time_Calc]
         }
       } catch (InterruptedException e) {
         throw (IOException)new IOException(
@@ -1234,14 +1244,14 @@ class MapTask extends Task {
           : (bufvoid - bufend) + bufstart) +
                   partitions * APPROX_HEADER_LENGTH; */
 //      FSDataOutputStream out = null;
-			KVDataOutputStream outKV = null;
+    	KVDataOutputStream outKV = null;
       try {
         // create spill file
 //        final SpillRecord spillRec = new SpillRecord(partitions);
 /*        final Path filename = mapOutputFile.getSpillFileForWrite(getTaskID(),
 						numSpills, size); */
 //        out = rfs.create(filename);
-				outKV = rkv.create();
+    	  outKV = rkv.create();
 
         final int endPosition = (kvend > kvstart)
           ? kvend
@@ -1271,7 +1281,7 @@ class MapTask extends Task {
                           (kvindices[kvoff + VALSTART] - 
                            kvindices[kvoff + KEYSTART]));
 //                writer.append(key, value);
-								writerKV.append(key, value);
+                writerKV.append(key, value);
                 ++spindex;
               }
             } else {
@@ -1303,6 +1313,8 @@ class MapTask extends Task {
             writer = null; */
           } finally {
 //            if (null != writer) writer.close();
+						// DC - ASYNC
+						if (null != writerKV) writerKV.flush();
           }
         }
 
diff --git a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
index 85c2d9c..92f0195 100644
--- a/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
+++ b/src/mapred/org/apache/hadoop/mapred/ReduceTask.java
@@ -86,6 +86,8 @@ import org.apache.hadoop.util.Progressable;
 import org.apache.hadoop.util.ReflectionUtils;
 import org.apache.hadoop.util.StringUtils;
 
+import com.ibm.pimdutils.TimeCounter;
+
 /** A Reduce task. */
 class ReduceTask extends Task {
 
@@ -372,16 +374,17 @@ class ReduceTask extends Task {
     codec = initCodec();
 
     boolean isLocal = "local".equals(job.get("mapred.job.tracker", "local"));
-    if (!isLocal) {
-      reduceCopier = new ReduceCopier(umbilical, job, reporter);
-      if (!reduceCopier.fetchOutputs()) {
-        if(reduceCopier.mergeThrowable instanceof FSError) {
-          throw (FSError)reduceCopier.mergeThrowable;
-        }
-        throw new IOException("Task: " + getTaskID() + 
-            " - The reduce copier failed", reduceCopier.mergeThrowable);
-      }
-    }
+//    In IntermediateKV is not necessary to copy Reduce Input [JLUPOX]
+//    if (!isLocal) {
+//      reduceCopier = new ReduceCopier(umbilical, job, reporter);
+//      if (!reduceCopier.fetchOutputs()) {
+//        if(reduceCopier.mergeThrowable instanceof FSError) {
+//          throw (FSError)reduceCopier.mergeThrowable;
+//        }
+//        throw new IOException("Task: " + getTaskID() + 
+//            " - The reduce copier failed", reduceCopier.mergeThrowable);
+//      }
+//    }
     copyPhase.complete();                         // copy is already complete
     setPhase(TaskStatus.Phase.SORT);
     statusUpdate(umbilical);
@@ -590,12 +593,19 @@ class ReduceTask extends Task {
     job.setBoolean("mapred.skip.on", isSkipping());
     org.apache.hadoop.mapreduce.Reducer.Context 
          reducerContext = createReduceContext(reducer, job, getTaskID(),
-																							 rIterKV, reduceInputKeyCounter,
+        		 							   rIterKV, reduceInputKeyCounter,
                                                reduceInputValueCounter, 
                                                trackedRW, committer,
                                                reporter, comparator, keyClass,
                                                valueClass);
     reducer.run(reducerContext);
+		LOG.info("[TIME]:   [ReduceTask]-[PIMDKV.java] #New Keys : " + TimeCounter.Num_new_keys ); //[Time_Calc]
+		LOG.info("[TIME]:   [ReduceTask]-[PIMDKVIS.java]      NewItem seekNextKey Cost: " + TimeCounter.seekNextKey + "ms"); //[Time_Calc]
+		LOG.info("[TIME]:   [ReduceTask]-[PIMDKVIS.java]      NewItem seekKey Cost: " + TimeCounter.seekKey + "ms"); //[Time_Calc]
+		LOG.info("[TIME]:   [ReduceTask]-[PIMD.java]             Allocate key value ByteBuffers: " + TimeCounter.PIMD_Allocate_key_value + "ms"); //[Time_Calc]
+		LOG.info("[TIME]:   [ReduceTask]-[PIMD.java]             PIMDWrapper Iterator Next Cost: " + TimeCounter.PIMDWrapper_GlobalIteratorNext + "ms"); //[Time_Calc]
+		LOG.info("[TIME]:   [ReduceTask]-[PIMD.java]             sets value correctly: " + TimeCounter.PIMD_sets_value + "ms"); //[Time_Calc]
+		LOG.info("[TIME]:   [ReduceTask]-[PIMD.java]             close iterator: " + TimeCounter.PIMD_iterrator_close + "ms"); //[Time_Calc]
     output.close(reducerContext);
   }
 
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskRunner.java b/src/mapred/org/apache/hadoop/mapred/TaskRunner.java
index eb7cc0d..503337b 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskRunner.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskRunner.java
@@ -105,7 +105,8 @@ abstract class TaskRunner extends Thread {
       
       //before preparing the job localize 
       //all the archives
-      TaskAttemptID taskid = t.getTaskID();
+      @SuppressWarnings("deprecation")
+	TaskAttemptID taskid = t.getTaskID();
       LocalDirAllocator lDirAlloc = new LocalDirAllocator("mapred.local.dir");
       File jobCacheDir = null;
       if (conf.getJar() != null) {
@@ -338,6 +339,9 @@ abstract class TaskRunner extends Thread {
         throw new IOException("Mkdirs failed to create " + tmpDir.toString());
       }
       vargs.add("-Djava.io.tmpdir=" + tmpDir.toString());
+      
+      //To debug the jvm [JLUPOX]
+//      vargs.add("-Xdebug -Xrunjdwp:transport=dt_socket,address=8015,server=y,suspend=y");
 
       // Add classpath.
       vargs.add("-classpath");
diff --git a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
index 69d8645..e15b7b7 100644
--- a/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
+++ b/src/mapred/org/apache/hadoop/mapred/TaskTracker.java
@@ -1012,18 +1012,19 @@ public class TaskTracker
         // 2. Get the system directory & filesystem
         if(justInited) {
           String jobTrackerBV = jobClient.getBuildVersion();
-          if(!VersionInfo.getBuildVersion().equals(jobTrackerBV)) {
-            String msg = "Shutting down. Incompatible buildVersion." +
-            "\nJobTracker's: " + jobTrackerBV + 
-            "\nTaskTracker's: "+ VersionInfo.getBuildVersion();
-            LOG.error(msg);
-            try {
-              jobClient.reportTaskTrackerError(taskTrackerName, null, msg);
-            } catch(Exception e ) {
-              LOG.info("Problem reporting to jobtracker: " + e);
-            }
-            return State.DENIED;
-          }
+// *** Anulate version checking *** [JLUPOX]
+//          if(!VersionInfo.getBuildVersion().equals(jobTrackerBV)) {
+//            String msg = "Shutting down. Incompatible buildVersion." +
+//            "\nJobTracker's: " + jobTrackerBV + 
+//            "\nTaskTracker's: "+ VersionInfo.getBuildVersion();
+//            LOG.error(msg);
+//            try {
+//              jobClient.reportTaskTrackerError(taskTrackerName, null, msg);
+//            } catch(Exception e ) {
+//              LOG.info("Problem reporting to jobtracker: " + e);
+//            }
+//            return State.DENIED;
+//          }
           
           String dir = jobClient.getSystemDir();
           if (dir == null) {
diff --git a/tests/conf/hadoop-local.xml b/tests/conf/hadoop-local.xml
new file mode 100644
index 0000000..aea0921
--- /dev/null
+++ b/tests/conf/hadoop-local.xml
@@ -0,0 +1,14 @@
+<?xml version="1.0"?>
+<configuration>
+
+  <property>
+    <name>fs.default.name</name>
+    <value>file:///</value>
+  </property>
+  
+  <property>
+    <name>mapred.job.tracker</name>
+    <value>local</value>
+  </property>
+  
+</configuration>
diff --git a/tests/terasort/makefile b/tests/terasort/makefile
new file mode 100644
index 0000000..e2f4095
--- /dev/null
+++ b/tests/terasort/makefile
@@ -0,0 +1,169 @@
+export HADOOP_VERSION := 0.20.2-SNAPSHOT
+export HADOOP_HOME := $(PWD)/../..
+export HADOOP_COMMON_HOME := ${HADOOP_HOME}
+export HADOOP_INSTALL := ${HADOOP_HOME}
+
+export ANT_HOME := /home/jlperez/hadoop-pimd/apache-ant-1.8.2
+export JAVA_HOME := /home/jlperez/hadoop-pimd/ibm-java-ppc64-60
+export PATH := ${JAVA_HOME}/bin:${PATH}
+
+export LD_LIBRARY_PATH := ${HADOOP_INSTALL}/pimd/lib
+export CLASSPATH := ${HADOOP_INSTALL}/hadoop-${HADOOP_VERSION}-core.jar:${UNIT_TEST}
+
+#LD_LIBRARY_PATH=~/Software/kfs-0.3/build/lib/
+#CLASSPATH :=/home/jlperez/Software/kfs-0.5/build/lib/kfs-0.5.jar:.:..:${HADOOP_INSTALL}/hadoop-hdfs-0.21.0.jar:${HADOOP_INSTALL}/hadoop-mapred-0.21.0.jar:${HADOOP_INSTALL}/hadoop-common-0.21.0.jar
+
+# export PATH := ${HADOOP_INSTALL}/bin:${PATH}
+export PATH := ${JAVA_HOME}/bin:${HADOOP_INSTALL}/bin:${PATH}
+
+echo:
+	@echo HADOOP_HOME = ${HADOOP_HOME}
+	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
+	@echo HADOOP_INSTALL = ${HADOOP_INSTALL}
+	@echo LD_LIBRARY_PATH = ${LD_LIBRARY_PATH}
+	@echo CLASSPATH = ${CLASSPATH}
+	@echo JAVA_HOME = ${JAVA_HOME}
+	@echo PATH = ${PATH}
+
+format:
+	hadoop namenode -format
+
+start-all:
+	../../bin/start-dfs.sh
+	../../bin/start-mapred.sh
+
+start-dfs:
+	@echo HADOOP_COMMON_HOME = ${HADOOP_COMMON_HOME}
+	../../bin/start-dfs.sh
+
+start-mapred:
+	../../bin/start-mapred.sh
+	
+start-mapred-debug:
+	  ../../bin/start-mapred-debug.sh
+
+start-jobtracker:
+	$(HADOOP_HOME)/bin/hadoop-daemon.sh --config $(HADOOP_HOME)/bin/../conf start jobtracker
+
+start-jobtracker-debug:
+	$(HADOOP_HOME)/bin/hadoop-daemon-debug.sh --config $(HADOOP_HOME)/bin/../conf start jobtracker 
+
+start-tasktracker:
+	$(HADOOP_HOME)/bin/hadoop-daemons.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+#   $(HADOOP_HOME)/bin/hadoop-daemon.sh \
+#   --config $(HADOOP_HOME)/bin/../conf \
+#   --script $(HADOOP_HOME)/bin/mapred start tasktracker
+
+start-tasktracker-debug:
+	$(HADOOP_HOME)/bin/hadoop-daemons-debug.sh --config $(HADOOP_HOME)/bin/../conf start tasktracker
+
+
+stop-all:
+	../../bin/stop-dfs.sh
+	../../bin/stop-mapred.sh
+
+stop-dfs:
+	../../bin/stop-dfs.sh
+
+stop-mapred:
+	../../bin/stop-mapred.sh
+
+jps:
+	@ps aux | grep java | grep -v grep | awk '{print $$NF}'
+
+cpinput:
+	hadoop fs -mkdir input
+	hadoop fs -put input/* input/
+	hadoop fs -ls input
+
+view-output:
+	hadoop fs -ls output
+	hadoop fs -cat output/*
+
+view-input:
+	hadoop fs -ls input
+	hadoop fs -cat input/*
+
+view-validate:
+	hadoop fs -ls validate
+	hadoop fs -cat validate/part-00000
+
+ls-input:
+	hadoop fs -ls input
+
+rm-output:
+	hadoop fs -rmr output
+
+rm-input:
+	hadoop fs -rmr input
+
+delnamenode:
+	rm -r /tmp/hadoop-y99yse83*
+
+kfs-ls:
+	hadoop fs -fs kfs://localhost:40000 -ls /
+
+local-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
+
+local-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-local.xml input/ output
+
+hdfs-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount input/ output
+
+hdfs-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount input/ output
+
+kfs-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
+
+kfs-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-kfs.xml input/ output
+
+memcachedfs-hadoop:
+#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
+	hadoop jar wordcount.jar org.myorg.WordCount input/ output
+
+memcachedfs-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedfs.xml input/ output
+
+memcachedkv-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedkv.xml input/ output
+
+memcachedkv-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-memcachedkv.xml input/ output
+
+prueba:
+	hadoop classpath
+
+pimd-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
+
+pimd-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimd.xml input/ output
+
+pimdkv-debug:
+	hdebug jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+
+pimdkv-hadoop:
+	hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+#   hadoop jar wordcount.jar org.myorg.WordCount -conf ../conf/hadoop-pimdKV.xml input/ output
+
+WordCount:
+	@javac -Xlint ./org/myorg/WordCount.java -d ./classes
+	@cp ./org/myorg/WordCount.java ./classes
+	@jar -cvf wordcount.jar classes/WordCount.java -C classes org/myorg
+	@rm -r classes/*
+
+TeraGen:
+	hadoop jar ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-examples.jar teragen 1000 input
+
+TeraSort:
+	hadoop jar ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-examples.jar terasort input output
+
+TeraValidate:
+	hadoop jar ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-examples.jar teravalidate output validate
+
+hadoop:
+	hadoop 
diff --git a/use_devel.sh b/use_devel.sh
new file mode 100644
index 0000000..5030769
--- /dev/null
+++ b/use_devel.sh
@@ -0,0 +1 @@
+cp vanilla_jars/devel/* .
diff --git a/use_original.sh b/use_original.sh
new file mode 100644
index 0000000..a62dbdd
--- /dev/null
+++ b/use_original.sh
@@ -0,0 +1 @@
+cp vanilla_jars/vanilla/* .
diff --git a/vanilla_jars/vanilla/hadoop-0.20.2-SNAPSHOT-core.jar b/vanilla_jars/vanilla/hadoop-0.20.2-SNAPSHOT-core.jar
new file mode 100644
index 0000000..d84749a
Binary files /dev/null and b/vanilla_jars/vanilla/hadoop-0.20.2-SNAPSHOT-core.jar differ
